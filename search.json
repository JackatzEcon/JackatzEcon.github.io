[
  {
    "objectID": "Stock_Volatility.html",
    "href": "Stock_Volatility.html",
    "title": "Stock Volatility",
    "section": "",
    "text": "#loading packages\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidyselect)\nlibrary(skimr)\nlibrary(ggthemes)\n# loading data\nStock_data &lt;- read.csv(\"/Users/jackkatz/Downloads/data.csv\")"
  },
  {
    "objectID": "Stock_Volatility.html#calculating-volatility",
    "href": "Stock_Volatility.html#calculating-volatility",
    "title": "Stock Volatility",
    "section": "Calculating Volatility",
    "text": "Calculating Volatility\nBy separating the month and day from the year and creating a new variable, we can group the data set by Year and Company and find the standard deviations from Close.Last.Numeric.\n\nVolatility_analysis &lt;- Stock_data_clean %&gt;% \n  separate(Date, into = c(\"Month/Day\",\"Year\"), sep = 6 )\n\nhead(Volatility_analysis, n = 10)\n\n   Company Month/Day Year   Volume Close.Last.Numeric Open.Numeric High.Numeric\n1     AAPL    07/17/ 2023 50520160             193.99       191.90       194.32\n2     AAPL    07/14/ 2023 41616240             190.69       190.23     191.1799\n3     AAPL    07/13/ 2023 41342340             190.54       190.50       191.19\n4     AAPL    07-12- 2023 60750250             189.77       189.68       191.70\n5     AAPL    07-11- 2023 46638120             188.08       189.16       189.30\n6     AAPL    07-10- 2023 59922160             188.61       189.26       189.99\n7     AAPL    07-07- 2023 46815000             190.68       191.41       192.67\n8     AAPL    07-06- 2023 45156010             191.81       189.84       192.02\n9     AAPL    07-05- 2023 46920260             191.33      191.565       192.98\n10    AAPL    07-03- 2023 31346600             192.46       193.78       193.88\n   Low.Numeric\n1       191.81\n2       189.63\n3       189.78\n4       188.47\n5       186.60\n6      187.035\n7       190.24\n8       189.20\n9       190.62\n10      191.76\n\n\n\nHistorical_Volatility &lt;- Volatility_analysis %&gt;% \n  group_by(Company, Year) %&gt;% \n  summarise(Volatility = sd(Close.Last.Numeric))\n\n`summarise()` has grouped output by 'Company'. You can override using the\n`.groups` argument.\n\nhead(Historical_Volatility, n = 10)\n\n# A tibble: 10 × 3\n# Groups:   Company [1]\n   Company Year  Volatility\n   &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt;\n 1 AAPL    2013        1.33\n 2 AAPL    2014        3.34\n 3 AAPL    2015        1.92\n 4 AAPL    2016        1.91\n 5 AAPL    2017        3.66\n 6 AAPL    2018        5.15\n 7 AAPL    2019        8.63\n 8 AAPL    2020       21.8 \n 9 AAPL    2021       14.7 \n10 AAPL    2022       13.1 \n\n\nFrom this dataset we then used ggplot to show each companies volatility from 2013-2023\n\nggplot(Historical_Volatility,\n       mapping = aes(x = Year,\n                     y = Volatility),\n       grouping = 1)+\n  geom_point()+\n  facet_wrap( . ~Company)+\n  theme_economist_white()\n\n\n\n\nFrom here we can now see how the volatility changes. Other than CSCO and SBUX the collection of stocks has an increase in volatility over time. The increase happens for most companies around 2020. According to the graphs the most volaitle companies are NFLX, TSLA, and META."
  },
  {
    "objectID": "danl-210-quarto-reticulate.html",
    "href": "danl-210-quarto-reticulate.html",
    "title": "DANL 210: Data Preparation and Management",
    "section": "",
    "text": "In Python, variables can store data of different types without explicitly declaring the type.\nFor example:\n\ninteger_variable = 10\nstring_variable = 'Hello'\nfloat_variable = 10.5\n\nfloat_variable\n\n10.5"
  },
  {
    "objectID": "danl-210-quarto-reticulate.html#variables-and-data-types",
    "href": "danl-210-quarto-reticulate.html#variables-and-data-types",
    "title": "DANL 210: Data Preparation and Management",
    "section": "",
    "text": "In Python, variables can store data of different types without explicitly declaring the type.\nFor example:\n\ninteger_variable = 10\nstring_variable = 'Hello'\nfloat_variable = 10.5\n\nfloat_variable\n\n10.5"
  },
  {
    "objectID": "danl-210-quarto-reticulate.html#control-structures",
    "href": "danl-210-quarto-reticulate.html#control-structures",
    "title": "DANL 210: Data Preparation and Management",
    "section": "0.2 Control Structures",
    "text": "0.2 Control Structures\nPython supports the usual logical conditions from mathematics:\n\n# Equals: a == b\n# Not Equals: a != b\n# Less than: a &lt; b\n# Less than or equal to: a &lt;= b\n# Greater than: a &gt; b\n# Greater than or equal to: a &gt;= b\n\nThese conditions can be used in several ways, most commonly in ‘if statements’ and loops.\n\n# if statement:\nif 5 &gt; 2:\n    print('Five is greater than two!')\n\nFive is greater than two!"
  },
  {
    "objectID": "danl-210-quarto-reticulate.html#functions",
    "href": "danl-210-quarto-reticulate.html#functions",
    "title": "DANL 210: Data Preparation and Management",
    "section": "0.3 Functions",
    "text": "0.3 Functions\nA function is a block of code which only runs when it is called.\nYou can pass data, known as parameters, into a function.\nA function can return data as a result.\n\n# Defining a function:\ndef my_function():\n    print('Hello from a function')\n\n# Calling a function:\nmy_function()\n\nHello from a function"
  },
  {
    "objectID": "danl-210-quarto-reticulate.html#lists-and-dictionaries",
    "href": "danl-210-quarto-reticulate.html#lists-and-dictionaries",
    "title": "DANL 210: Data Preparation and Management",
    "section": "0.4 Lists and Dictionaries",
    "text": "0.4 Lists and Dictionaries\nA list is a collection which is ordered and changeable.\nA dictionary is a collection which is unordered, changeable and indexed.\n\n# List example:\nmy_list = ['apple', 'banana', 'cherry']\n\n# Dictionary example:\nmy_dict = {'name': 'John', 'age': 36}"
  },
  {
    "objectID": "danl-210-quarto-reticulate.html#import-python-libraries",
    "href": "danl-210-quarto-reticulate.html#import-python-libraries",
    "title": "DANL 210: Data Preparation and Management",
    "section": "1.1 Import Python libraries",
    "text": "1.1 Import Python libraries\n\nimport pandas as pd\n\n\noj = pd.read_csv('https://bcdanl.github.io/data/dominick_oj.csv')\n\n\n\nCode!\noj\n\n\n         sales  price      brand  ad\n0       8256.0   3.87  tropicana   0\n1       6144.0   3.87  tropicana   0\n2       3840.0   3.87  tropicana   0\n3       8000.0   3.87  tropicana   0\n4       8896.0   3.87  tropicana   0\n...        ...    ...        ...  ..\n28942   2944.0   2.00  dominicks   0\n28943   4928.0   1.94  dominicks   0\n28944  13440.0   1.59  dominicks   0\n28945  55680.0   1.49  dominicks   0\n28946   7040.0   1.75  dominicks   0\n\n[28947 rows x 4 columns]\n\n\n\noj.describe()\n\n               sales         price            ad\ncount   28947.000000  28947.000000  28947.000000\nmean    17312.213356      2.282488      0.237261\nstd     27477.660437      0.648001      0.425411\nmin        64.000000      0.520000      0.000000\n25%      4864.000000      1.790000      0.000000\n50%      8384.000000      2.170000      0.000000\n75%     17408.000000      2.730000      0.000000\nmax    716416.000100      3.870000      1.000000"
  },
  {
    "objectID": "danl-210-quarto-reticulate.html#python-r-interaction",
    "href": "danl-210-quarto-reticulate.html#python-r-interaction",
    "title": "DANL 210: Data Preparation and Management",
    "section": "1.2 Python-R Interaction",
    "text": "1.2 Python-R Interaction\nBelow is using Python’s DataFrame oj to visualize using R’s ggplot\n\nlibrary(tidyverse)\n\n# Access the Python pandas DataFrame\noj &lt;- py$oj\n\n# Plot using ggplot2\nggplot(oj, aes(x = log(sales), y = log(price), \n               color = brand)) +\n  geom_point(alpha = .25) +\n  geom_smooth(method = lm) +\n  theme_minimal()\n\n\n\n\n\n1.2.1 Interactive DataFrame with R’s DT Package\n\n\n\n\n\n\n\nIn *.ipynb on Google Colab, we can use itables or just Google Colab’s default to print DataFrame.\n\n# !pip install itables\nfrom itables import init_notebook_mode, show\ninit_notebook_mode(all_interactive=False)\n\noj = pd.read_csv('https://bcdanl.github.io/data/dominick_oj.csv')\nshow(oj)"
  },
  {
    "objectID": "Class_Notes_12-8.html",
    "href": "Class_Notes_12-8.html",
    "title": "Class-Notes_12-8",
    "section": "",
    "text": "df1 &lt;- data.frame(id = 1:3, \n                  name = c(\"Alice\", \"Bob\", \"Charlie\")\n                  )\ndf2 &lt;- data.frame(id = 4:6, \n                  name = c(\"Dave\", \"Eve\", \"Frank\")\n                  )\ndf_rbind &lt;- rbind(df1, df2)\ndf_cbind &lt;- cbind(df1, df2) # better to rename id before cbind()"
  },
  {
    "objectID": "blog-listing.html",
    "href": "blog-listing.html",
    "title": "DANL200 Homework",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nThe Fab Four: An Exploration of the Beatles Solo Careers Through Data\n\n\n\n\n\n\n\n\nJack Katz\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nSpotify Artist Dataset\n\n\n\n\n\n\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nRestaurant Inspection Data\n\n\n\n\n\n\n\n\n2 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Homework2_210_Title.html",
    "href": "Homework2_210_Title.html",
    "title": "The Fab Four: An Exploration of the Beatles Solo Careers Through Data",
    "section": "",
    "text": "#— #title: Homework 2 #author: Jack Katz #—\nimport pandas as pd\nspotify = pd.read_csv('https://bcdanl.github.io/data/spotify_all.csv')\nThe Beatles were a revolutionary Rock n’ Roll group that were active throughout the 1960’s and 70’s. In their early years they brought together blues and rock n’ roll style with the modern form of the four man rock band. Over the groups career they each developed their own styles and sounds as musicians which then influenced there individual solo careers. Although each of them are talented in their own way they all received varying levels of commercial and critical success. This data analysis takes a look at how each members popularity stands today among modern listeners."
  },
  {
    "objectID": "Homework2_210_Title.html#the-data",
    "href": "Homework2_210_Title.html#the-data",
    "title": "The Fab Four: An Exploration of the Beatles Solo Careers Through Data",
    "section": "The Data",
    "text": "The Data\nThe code below uses filtering methods and the isin function to narrow down the data set to just the individual music of each artist\n\nArtists = [\"Paul McCartney\", \"John Lennon\" , \"Ringo Starr\", \"George Harrison\" ]\n\nBeatles = spotify[\"artist_name\"].isin(Artists)\n\nIndividual_Beatles = spotify[Beatles]\n\nIndividual_Beatles.nunique()\n\n\n\n\n\npid              59\nplaylist_name    58\npos              75\nartist_name       4\ntrack_name       50\nduration_ms      50\nalbum_name       31\ndtype: int64"
  },
  {
    "objectID": "Homework2_210_Title.html#data-manipulation-and-approach",
    "href": "Homework2_210_Title.html#data-manipulation-and-approach",
    "title": "The Fab Four: An Exploration of the Beatles Solo Careers Through Data",
    "section": "Data manipulation and Approach",
    "text": "Data manipulation and Approach\nMy technique involves filtering the data for each Beatle to then find statistics on, amount of minutes each is listened to, and their individual representation within the dataset. The metric I will be using to determine popularity is amount of playlists they appear in and how many songs they have in the entire dataset.\n\nPaul\nPaul was the bassist for the Beatles and one of the main singers and songwriters. He is known for his signature lefty bass playing.\n\nPaul = (\n    Individual_Beatles\n        .set_index(\"artist_name\")\n        .loc[\"Paul McCartney\"]\n)\n\nPaul.count()\n\npid              44\nplaylist_name    44\npos              44\ntrack_name       44\nduration_ms      44\nalbum_name       44\ndtype: int64\n\n\n\nPaul[\"pid\"].nunique()\n\n30\n\n\n\n\nGeorge\nGeorge was the lead Guitarist of the group and was known for his simple yet elegant solos and fills.\n\nGeorge = (\n    Individual_Beatles\n        .set_index(\"artist_name\")\n        .loc[\"George Harrison\"]\n)\n\nGeorge.count()\n\npid              27\nplaylist_name    27\npos              27\ntrack_name       27\nduration_ms      27\nalbum_name       27\ndtype: int64\n\n\n\nGeorge[\"pid\"].nunique()\n\n18\n\n\n\n\nRingo\nRingo was the groups drummer. He was known for his quirky song writing and goofy personality.\n\nRingo = (\n    Individual_Beatles\n        .set_index(\"artist_name\")\n        .loc[\"Ringo Starr\"]\n)\n\nRingo.count()\n\npid              5\nplaylist_name    5\npos              5\ntrack_name       5\nduration_ms      5\nalbum_name       5\ndtype: int64\n\n\n\nRingo[\"pid\"].nunique()\n\n2\n\n\n\n\nJohn\nJohn was the rythm guitarist and wrote many of the groups most popular songs.\n\nJohn = (\n    Individual_Beatles\n        .set_index(\"artist_name\")\n        .loc[\"John Lennon\"]\n)\n\nJohn.count()\n\npid              32\nplaylist_name    32\npos              32\ntrack_name       32\nduration_ms      32\nalbum_name       32\ndtype: int64\n\n\n\nJohn[\"pid\"].nunique()\n\n26"
  },
  {
    "objectID": "Homework2_210_Title.html#conclusion",
    "href": "Homework2_210_Title.html#conclusion",
    "title": "The Fab Four: An Exploration of the Beatles Solo Careers Through Data",
    "section": "Conclusion",
    "text": "Conclusion\nPaul has the highest popularity with 44 songs on 30 datasets. Ringo has the lowest with 2 songs spread over 5 playlists. John was the second most popular with 32 songs in 26 playlists and George was the third with 27 songs over 18 playlists. These results can be attributed to a number of factors. For exampl, Paul is still alive releasing new music while George and John passed away while they were still making new albums. Another reason is due to overall style. George, John, and Ringo produced eclectic music that appealed to more niche groups of people while Paul released more widely accepted music."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jack Katz",
    "section": "",
    "text": "Jack Katz majors in Economics at SUNY Geneseo. When not doing school work, Jack enjoys watching movies, going hiking, and making soups."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Jack Katz",
    "section": "Education",
    "text": "Education\nState University of New York at Geneseo | Geneseo, NY  B.A. in Economics | Aug 2020 - May 2024  Minor in English Literature"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Jack Katz",
    "section": "Experience",
    "text": "Experience\nSUNY Geneseo Federal Reserve Challenge Team Member | June 2023 - Current\nUndergraduate economic research | September 2023 - Current"
  },
  {
    "objectID": "BlogPosts/Restaurant_Inspection_files/Restaurant_Inspection.html",
    "href": "BlogPosts/Restaurant_Inspection_files/Restaurant_Inspection.html",
    "title": "Restaurant Inspection Data",
    "section": "",
    "text": "restaurant &lt;- read_csv(\n  'https://bcdanl.github.io/data/DOHMH_NYC_Restaurant_Inspection.csv'\n)\n\nRows: 17633 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (10): DBA, BORO, STREET, CUISINE DESCRIPTION, INSPECTION DATE, ACTION, V...\ndbl  (2): CAMIS, SCORE\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThis dataset has 17633 observations and 12 variables. It contains information on health and hygiene rankings. Each restaurant has information about location, type of cuisine and performance on health inspections."
  },
  {
    "objectID": "BlogPosts/Restaurant_Inspection_files/Restaurant_Inspection.html#loading-in-data",
    "href": "BlogPosts/Restaurant_Inspection_files/Restaurant_Inspection.html#loading-in-data",
    "title": "Restaurant Inspection Data",
    "section": "",
    "text": "restaurant &lt;- read_csv(\n  'https://bcdanl.github.io/data/DOHMH_NYC_Restaurant_Inspection.csv'\n)\n\nRows: 17633 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (10): DBA, BORO, STREET, CUISINE DESCRIPTION, INSPECTION DATE, ACTION, V...\ndbl  (2): CAMIS, SCORE\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThis dataset has 17633 observations and 12 variables. It contains information on health and hygiene rankings. Each restaurant has information about location, type of cuisine and performance on health inspections."
  },
  {
    "objectID": "BlogPosts/Restaurant_Inspection_files/Restaurant_Inspection.html#creating-descriptive-statistics",
    "href": "BlogPosts/Restaurant_Inspection_files/Restaurant_Inspection.html#creating-descriptive-statistics",
    "title": "Restaurant Inspection Data",
    "section": "Creating Descriptive Statistics",
    "text": "Creating Descriptive Statistics\n\nQ1a &lt;- restaurant %&gt;% \n  group_by(GRADE) %&gt;% \n  summarise(SD_value = sd(SCORE),\n            first_quartile = quantile(SCORE, 0.25),\n            median_value = median(SCORE),\n            third_quartile = quantile(SCORE, .75),\n            MAX_value = max(SCORE))\n\n\nRelationship between Grade and Score\nBox plots can show us the relationship between a character variable and a quantitative variable.\n\nggplot(data = restaurant)+\n  geom_boxplot(mapping = aes(x = GRADE, y = SCORE))\n\n\n\n\n###Relationship between grade score and critical flag\n\nggplot(data = restaurant)+\n  geom_boxplot(mapping = aes(x = `CRITICAL FLAG`, y = SCORE))\n\n\n\n\n\n\nIn this ggplot graph we can see which Cuisine Description has the highest proportion of Grade A rankings\n\nQ1e &lt;- restaurant %&gt;% \n  group_by(`CUISINE DESCRIPTION`) %&gt;% \n  summarise(count = n()) %&gt;% \n  arrange(-count) %&gt;% \n  head(n = 10)\n\nQ1e\n\n# A tibble: 10 × 2\n   `CUISINE DESCRIPTION`    count\n   &lt;chr&gt;                    &lt;int&gt;\n 1 American                  3678\n 2 Coffee/Tea                1414\n 3 Chinese                   1372\n 4 Pizza                     1050\n 5 Italian                    689\n 6 Bakery Products/Desserts   615\n 7 Japanese                   584\n 8 Mexican                    582\n 9 Latin American             544\n10 Donuts                     505\n\n\n\nGrades &lt;- restaurant %&gt;% \n  filter(`CUISINE DESCRIPTION` %in% c(Q1e$`CUISINE DESCRIPTION`))\nGrades\n\n# A tibble: 11,033 × 12\n      CAMIS DBA      BORO  STREET `CUISINE DESCRIPTION` `INSPECTION DATE` ACTION\n      &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;                 &lt;chr&gt;             &lt;chr&gt; \n 1 40356018 riviera… Broo… STILL… American              02/01/2022        Viola…\n 2 40357217 wild as… Bronx SOUTH… American              07/28/2021        Viola…\n 3 40359480 1 east … Manh… EAST … American              05/03/2022        Viola…\n 4 40362264 p & s d… Manh… COLUM… American              02/07/2023        Viola…\n 5 40362274 angelik… Manh… WEST … American              02/14/2022        Viola…\n 6 40362432 ho mei … Quee… 37 AV… Chinese               10/25/2022        Viola…\n 7 40363298 cafe me… Manh… 8 AVE… American              09/02/2022        Viola…\n 8 40363920 new gol… Broo… RUTLA… Chinese               02/16/2022        Viola…\n 9 40364305 philade… Broo… 4 AVE… Pizza                 01/23/2023        Viola…\n10 40364347 metropo… Manh… EAST … American              04/12/2022        Viola…\n# ℹ 11,023 more rows\n# ℹ 5 more variables: `VIOLATION CODE` &lt;chr&gt;, `VIOLATION DESCRIPTION` &lt;chr&gt;,\n#   `CRITICAL FLAG` &lt;chr&gt;, SCORE &lt;dbl&gt;, GRADE &lt;chr&gt;\n\n\n\nggplot(data = Grades)+\n  geom_bar(mapping = aes(x = `CUISINE DESCRIPTION`, \n                         y = after_stat(prop),\n                         group = GRADE,\n                         fill = GRADE))\n\n\n\n\nFrom this ggplot we can see that donuts have the highest proportion of A grades for health.\n\n\nMost popular restaurant type\n\nQ1f &lt;- restaurant %&gt;% \n  group_by(`DBA`) %&gt;% \n  summarise(count = n()) %&gt;%   \n  arrange(desc(count))\n\nQ1f\n\n# A tibble: 13,914 × 2\n   DBA                     count\n   &lt;chr&gt;                   &lt;int&gt;\n 1 dunkin                    402\n 2 starbucks                 286\n 3 mcdonald's                170\n 4 subway                    166\n 5 dunkin', baskin robbins   107\n 6 popeyes                    98\n 7 chipotle mexican grill     91\n 8 burger king                63\n 9 domino's                   60\n10 kennedy fried chicken      52\n# ℹ 13,904 more rows"
  },
  {
    "objectID": "BlogPosts/Beatles_Data_Updated.html",
    "href": "BlogPosts/Beatles_Data_Updated.html",
    "title": "The Fab Four: An Exploration of the Beatles Solo Careers Through Data",
    "section": "",
    "text": "import pandas as pd\nspotify = pd.read_csv('https://bcdanl.github.io/data/spotify_all.csv')"
  },
  {
    "objectID": "BlogPosts/Beatles_Data_Updated.html#the-data",
    "href": "BlogPosts/Beatles_Data_Updated.html#the-data",
    "title": "The Fab Four: An Exploration of the Beatles Solo Careers Through Data",
    "section": "The Data",
    "text": "The Data\nThe code below uses filtering methods and the isin function to narrow down the data set to just the individual music of each artist\n\nArtists = [\"Paul McCartney\", \"John Lennon\" , \"Ringo Starr\", \"George Harrison\" ]\n\nBeatles = spotify[\"artist_name\"].isin(Artists)\n\nIndividual_Beatles = spotify[Beatles]\n\nIndividual_Beatles.nunique()\n\n\n\n\n\npid              59\nplaylist_name    58\npos              75\nartist_name       4\ntrack_name       50\nduration_ms      50\nalbum_name       31\ndtype: int64"
  },
  {
    "objectID": "BlogPosts/Beatles_Data_Updated.html#data-manipulation-and-approach",
    "href": "BlogPosts/Beatles_Data_Updated.html#data-manipulation-and-approach",
    "title": "The Fab Four: An Exploration of the Beatles Solo Careers Through Data",
    "section": "Data manipulation and Approach",
    "text": "Data manipulation and Approach\nMy technique involves filtering the data for each Beatle to then find statistics on, amount of minutes each is listened to, and their individual representation within the dataset. The metric I will be using to determine popularity is amount of playlists they appear in and how many songs they have in the entire dataset.\n\nPaul\nPaul was the bassist for the Beatles and one of the main singers and songwriters. He is known for his signature lefty bass playing.\n\nPaul = (\n    Individual_Beatles\n        .set_index(\"artist_name\")\n        .loc[\"Paul McCartney\"]\n)\n\nPaul.count()\n\npid              44\nplaylist_name    44\npos              44\ntrack_name       44\nduration_ms      44\nalbum_name       44\ndtype: int64\n\n\n\nPaul[\"pid\"].nunique()\n\n30\n\n\n\n\nGeorge\nGeorge was the lead Guitarist of the group and was known for his simple yet elegant solos and fills.\n\nGeorge = (\n    Individual_Beatles\n        .set_index(\"artist_name\")\n        .loc[\"George Harrison\"]\n)\n\nGeorge.count()\n\npid              27\nplaylist_name    27\npos              27\ntrack_name       27\nduration_ms      27\nalbum_name       27\ndtype: int64\n\n\n\nGeorge[\"pid\"].nunique()\n\n18\n\n\n\n\nRingo\nRingo was the groups drummer. He was known for his quirky song writing and goofy personality.\n\nRingo = (\n    Individual_Beatles\n        .set_index(\"artist_name\")\n        .loc[\"Ringo Starr\"]\n)\n\nRingo.count()\n\npid              5\nplaylist_name    5\npos              5\ntrack_name       5\nduration_ms      5\nalbum_name       5\ndtype: int64\n\n\n\nRingo[\"pid\"].nunique()\n\n2\n\n\n\n\nJohn\nJohn was the rythm guitarist and wrote many of the groups most popular songs.\n\nJohn = (\n    Individual_Beatles\n        .set_index(\"artist_name\")\n        .loc[\"John Lennon\"]\n)\n\nJohn.count()\n\npid              32\nplaylist_name    32\npos              32\ntrack_name       32\nduration_ms      32\nalbum_name       32\ndtype: int64\n\n\n\nJohn[\"pid\"].nunique()\n\n26"
  },
  {
    "objectID": "BlogPosts/Beatles_Data_Updated.html#conclusion",
    "href": "BlogPosts/Beatles_Data_Updated.html#conclusion",
    "title": "The Fab Four: An Exploration of the Beatles Solo Careers Through Data",
    "section": "Conclusion",
    "text": "Conclusion\nPaul has the highest popularity with 44 songs on 30 datasets. Ringo has the lowest with 2 songs spread over 5 playlists. John was the second most popular with 32 songs in 26 playlists and George was the third with 27 songs over 18 playlists. These results can be attributed to a number of factors. For exampl, Paul is still alive releasing new music while George and John passed away while they were still making new albums. Another reason is due to overall style. George, John, and Ringo produced eclectic music that appealed to more niche groups of people while Paul released more widely accepted music."
  },
  {
    "objectID": "posts/starwars/starwars_df.html",
    "href": "posts/starwars/starwars_df.html",
    "title": "Starwars",
    "section": "",
    "text": "Let’s analyze the starwars data:\nstarwars &lt;- read_csv(\"https://bcdanl.github.io/data/starwars.csv\")"
  },
  {
    "objectID": "posts/starwars/starwars_df.html#variable-description-for-starwars-data.frame",
    "href": "posts/starwars/starwars_df.html#variable-description-for-starwars-data.frame",
    "title": "Starwars",
    "section": "Variable Description for starwars data.frame",
    "text": "Variable Description for starwars data.frame\nThe following describes the variables in the starwars data.frame.\n\nfilms List of films the character appeared in\nname Name of the character\nspecies Name of species\nheight Height (cm)\nmass Weight (kg)\nhair_color, skin_color, eye_color Hair, skin, and eye colors\nbirth_year Year born (BBY = Before Battle of Yavin)\nsex The biological sex of the character, namely male, female, hermaphroditic, or none (as in the case for Droids).\ngender The gender role or gender identity of the character as determined by their personality or the way they were programmed (as in the case for Droids).\nhomeworld Name of homeworld"
  },
  {
    "objectID": "posts/starwars/starwars_df.html#human-vs.-droid",
    "href": "posts/starwars/starwars_df.html#human-vs.-droid",
    "title": "Starwars",
    "section": "Human vs. Droid",
    "text": "Human vs. Droid\n\nggplot(data = \n         starwars %&gt;% \n         filter(species %in% c(\"Human\", \"Droid\"))) +\n  geom_boxplot(aes(x = species, y = mass, \n                   fill = species),\n               show.legend = FALSE)"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Classwork-11:29.html",
    "href": "Classwork-11:29.html",
    "title": "DANL-200-1129",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ntable1\n\n# A tibble: 6 × 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\ntable2\n\n# A tibble: 12 × 4\n   country      year type            count\n   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1 Afghanistan  1999 cases             745\n 2 Afghanistan  1999 population   19987071\n 3 Afghanistan  2000 cases            2666\n 4 Afghanistan  2000 population   20595360\n 5 Brazil       1999 cases           37737\n 6 Brazil       1999 population  172006362\n 7 Brazil       2000 cases           80488\n 8 Brazil       2000 population  174504898\n 9 China        1999 cases          212258\n10 China        1999 population 1272915272\n11 China        2000 cases          213766\n12 China        2000 population 1280428583\n\ntable3\n\n# A tibble: 6 × 3\n  country      year rate             \n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;            \n1 Afghanistan  1999 745/19987071     \n2 Afghanistan  2000 2666/20595360    \n3 Brazil       1999 37737/172006362  \n4 Brazil       2000 80488/174504898  \n5 China        1999 212258/1272915272\n6 China        2000 213766/1280428583\n\n# Spread across two tibbles\ntable4a  # cases\n\n# A tibble: 3 × 3\n  country     `1999` `2000`\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n1 Afghanistan    745   2666\n2 Brazil       37737  80488\n3 China       212258 213766\n\ntable4b\n\n# A tibble: 3 × 3\n  country         `1999`     `2000`\n  &lt;chr&gt;            &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan   19987071   20595360\n2 Brazil       172006362  174504898\n3 China       1272915272 1280428583\n\ntable4a %&gt;% \n  pivot_longer(cols = c(`1999`, `2000`),\n               names_to = \"year\",\n               values_to = \"cases\")\n\n# A tibble: 6 × 3\n  country     year   cases\n  &lt;chr&gt;       &lt;chr&gt;  &lt;dbl&gt;\n1 Afghanistan 1999     745\n2 Afghanistan 2000    2666\n3 Brazil      1999   37737\n4 Brazil      2000   80488\n5 China       1999  212258\n6 China       2000  213766"
  },
  {
    "objectID": "Homework2_210.html",
    "href": "Homework2_210.html",
    "title": "The Fab Four: An Exploration of the Beatles Solo Careers Through Data",
    "section": "",
    "text": "import pandas as pd\nspotify = pd.read_csv('https://bcdanl.github.io/data/spotify_all.csv')\nThe Beatles were a revolutionary Rock n’ Roll group that were active throughout the 1960’s and 70’s. In their early years they brought together blues and rock n’ roll style with the modern form of the four man rock band. Over the groups career they each developed their own styles and sounds as musicians which then influenced there individual solo careers. Although each of them are talented in their own way they all received varying levels of commercial and critical success. This data analysis takes a look at how each members popularity stands today among modern listeners."
  },
  {
    "objectID": "Homework2_210.html#the-data",
    "href": "Homework2_210.html#the-data",
    "title": "The Fab Four: An Exploration of the Beatles Solo Careers Through Data",
    "section": "The Data",
    "text": "The Data\nThe code below uses filtering methods and the isin function to narrow down the data set to just the individual music of each artist\n\nArtists = [\"Paul McCartney\", \"John Lennon\" , \"Ringo Starr\", \"George Harrison\" ]\n\nBeatles = spotify[\"artist_name\"].isin(Artists)\n\nIndividual_Beatles = spotify[Beatles]\n\nIndividual_Beatles.nunique()\n\n\n\n\n\npid              59\nplaylist_name    58\npos              75\nartist_name       4\ntrack_name       50\nduration_ms      50\nalbum_name       31\ndtype: int64"
  },
  {
    "objectID": "Homework2_210.html#data-manipulation-and-approach",
    "href": "Homework2_210.html#data-manipulation-and-approach",
    "title": "The Fab Four: An Exploration of the Beatles Solo Careers Through Data",
    "section": "Data manipulation and Approach",
    "text": "Data manipulation and Approach\nMy technique involves filtering the data for each Beatle to then find statistics on, amount of minutes each is listened to, and their individual representation within the dataset. The metric I will be using to determine popularity is amount of playlists they appear in and how many songs they have in the entire dataset.\n\nPaul\nPaul was the bassist for the Beatles and one of the main singers and songwriters. He is known for his signature lefty bass playing.\n\nPaul = (\n    Individual_Beatles\n        .set_index(\"artist_name\")\n        .loc[\"Paul McCartney\"]\n)\n\nPaul.count()\n\npid              44\nplaylist_name    44\npos              44\ntrack_name       44\nduration_ms      44\nalbum_name       44\ndtype: int64\n\n\n\nPaul[\"pid\"].nunique()\n\n30\n\n\n\n\nGeorge\nGeorge was the lead Guitarist of the group and was known for his simple yet elegant solos and fills.\n\nGeorge = (\n    Individual_Beatles\n        .set_index(\"artist_name\")\n        .loc[\"George Harrison\"]\n)\n\nGeorge.count()\n\npid              27\nplaylist_name    27\npos              27\ntrack_name       27\nduration_ms      27\nalbum_name       27\ndtype: int64\n\n\n\nGeorge[\"pid\"].nunique()\n\n18\n\n\n\n\nRingo\nRingo was the groups drummer. He was known for his quirky song writing and goofy personality.\n\nRingo = (\n    Individual_Beatles\n        .set_index(\"artist_name\")\n        .loc[\"Ringo Starr\"]\n)\n\nRingo.count()\n\npid              5\nplaylist_name    5\npos              5\ntrack_name       5\nduration_ms      5\nalbum_name       5\ndtype: int64\n\n\n\nRingo[\"pid\"].nunique()\n\n2\n\n\n\n\nJohn\nJohn was the rythm guitarist and wrote many of the groups most popular songs.\n\nJohn = (\n    Individual_Beatles\n        .set_index(\"artist_name\")\n        .loc[\"John Lennon\"]\n)\n\nJohn.count()\n\npid              32\nplaylist_name    32\npos              32\ntrack_name       32\nduration_ms      32\nalbum_name       32\ndtype: int64\n\n\n\nJohn[\"pid\"].nunique()\n\n26"
  },
  {
    "objectID": "Homework2_210.html#conclusion",
    "href": "Homework2_210.html#conclusion",
    "title": "The Fab Four: An Exploration of the Beatles Solo Careers Through Data",
    "section": "Conclusion",
    "text": "Conclusion\nPaul has the highest popularity with 44 songs on 30 datasets. Ringo has the lowest with 2 songs spread over 5 playlists. John was the second most popular with 32 songs in 26 playlists and George was the third with 27 songs over 18 playlists. These results can be attributed to a number of factors. For exampl, Paul is still alive releasing new music while George and John passed away while they were still making new albums. Another reason is due to overall style. George, John, and Ringo produced eclectic music that appealed to more niche groups of people while Paul released more widely accepted music."
  },
  {
    "objectID": "basic_python_intro_(2).html",
    "href": "basic_python_intro_(2).html",
    "title": "Introduction to Python",
    "section": "",
    "text": "Python is a high-level, interpreted programming language. This is a simple Python code:\n\nprint('Hello, World!')\n\nHello, World!"
  },
  {
    "objectID": "basic_python_intro_(2).html#what-is-python",
    "href": "basic_python_intro_(2).html#what-is-python",
    "title": "Introduction to Python",
    "section": "",
    "text": "Python is a high-level, interpreted programming language. This is a simple Python code:\n\nprint('Hello, World!')\n\nHello, World!"
  },
  {
    "objectID": "basic_python_intro_(2).html#variables-and-data-types",
    "href": "basic_python_intro_(2).html#variables-and-data-types",
    "title": "Introduction to Python",
    "section": "Variables and Data Types",
    "text": "Variables and Data Types\nIn Python, variables can store data of different types without explicitly declaring the type.\nFor example:\n\ninteger_variable = 10\nstring_variable = 'Hello'\nfloat_variable = 10.5\n\nfloat_variable\n\n10.5"
  },
  {
    "objectID": "basic_python_intro_(2).html#control-structures",
    "href": "basic_python_intro_(2).html#control-structures",
    "title": "Introduction to Python",
    "section": "Control Structures",
    "text": "Control Structures\nPython supports the usual logical conditions from mathematics:\n\n# Equals: a == b\n# Not Equals: a != b\n# Less than: a &lt; b\n# Less than or equal to: a &lt;= b\n# Greater than: a &gt; b\n# Greater than or equal to: a &gt;= b\n\nThese conditions can be used in several ways, most commonly in ‘if statements’ and loops.\n\n# if statement:\nif 5 &gt; 2:\n    print('Five is greater than two!')"
  },
  {
    "objectID": "basic_python_intro_(2).html#functions",
    "href": "basic_python_intro_(2).html#functions",
    "title": "Introduction to Python",
    "section": "Functions",
    "text": "Functions\nA function is a block of code which only runs when it is called.\nYou can pass data, known as parameters, into a function.\nA function can return data as a result.\n\n# Defining a function:\ndef my_function():\n    print('Hello from a function')\n\n# Calling a function:\nmy_function()"
  },
  {
    "objectID": "basic_python_intro_(2).html#lists-and-dictionaries",
    "href": "basic_python_intro_(2).html#lists-and-dictionaries",
    "title": "Introduction to Python",
    "section": "Lists and Dictionaries",
    "text": "Lists and Dictionaries\nA list is a collection which is ordered and changeable.\nA dictionary is a collection which is unordered, changeable and indexed.\n\n# List example:\nmy_list = ['apple', 'banana', 'cherry']\n\n# Dictionary example:\nmy_dict = {'name': 'John', 'age': 36}"
  },
  {
    "objectID": "Directed_Study.html",
    "href": "Directed_Study.html",
    "title": "Directed Study",
    "section": "",
    "text": "Loading in Packages\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ipumsr)\n\n\n\nLoading in data\nThis data set is from the Integrated Public Use Micro data website which provides micro data from United States surveys for research purposes. Below is is code I wrote to clean and analyze the data for undergraduate research I am currently working on.\n\nddi &lt;- read_ipums_ddi(\"usa_00007.xml\")\ndata &lt;- read_ipums_micro(ddi)\n\nUse of data from IPUMS USA is subject to conditions including that users should cite the data appropriately. Use command `ipums_conditions()` for more details.\n\n\n\n\nCleaning Data\n\n\nCreating New Variables\n\n\nDescriptive Statistics"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/beer-markets/beer-markets.html",
    "href": "posts/beer-markets/beer-markets.html",
    "title": "Beer Markets",
    "section": "",
    "text": "Let’s analyze the beer_data data:\nbeer_data &lt;- read_csv(\"https://bcdanl.github.io/data/beer_markets.csv\")"
  },
  {
    "objectID": "posts/beer-markets/beer-markets.html#variable-description-for-beer_data-data.frame",
    "href": "posts/beer-markets/beer-markets.html#variable-description-for-beer_data-data.frame",
    "title": "Beer Markets",
    "section": "Variable Description for beer_data data.frame",
    "text": "Variable Description for beer_data data.frame\nThe following describes the variables in the beer_data data.frame.\n\nhh: Household identifier\n_purchase_desc: Description of the purchase\nquantity: The quantity of beer purchased\nbrand: The brand of beer\ndollar_spent: The amount spent\nbeer_floz: Fluid ounces of beer\nprice_per_floz: Price per fluid ounce\ncontainer: Type of container\npromo: Whether the purchase was on promotion\nmarket: The market where the purchase was made\nDemographics: age, employment status, degree, class of worker (cow), race, and household information like microwave, dishwasher, tvcable, singlefamilyhome, and npeople (number of people in the household)"
  },
  {
    "objectID": "posts/beer-markets/beer-markets.html#purchase-patterns",
    "href": "posts/beer-markets/beer-markets.html#purchase-patterns",
    "title": "Beer Markets",
    "section": "Purchase Patterns",
    "text": "Purchase Patterns\nWe’ll explore the purchase patterns in the dataset. This includes understanding the most popular brands, the average quantity purchased, and spending habits across different markets. Here are some specific analyses we can perform:\n\nCalculate the total quantity and spending for each brand.\nFind the average quantity purchased and average spending per purchase.\nCompare the total spending across different markets.\n\nI’ll begin with these analyses and create visualizations to help us understand the data better. Let’s start by calculating the total quantity and spending for each brand.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Reading the CSV file\nbeer_data = pd.read_csv(\"https://bcdanl.github.io/data/beer_markets.csv\")\n\n# Setting up the visualisation settings\nsns.set(style=\"whitegrid\")\n\n# Calculate total quantity and spending for each brand\nbrand_summary = beer_data.groupby('brand').agg({'quantity':'sum', 'dollar_spent':'sum'}).reset_index()\n\n# Sort by total quantity and spending\nbrand_summary_sorted_quantity = brand_summary.sort_values('quantity', ascending=False)\nbrand_summary_sorted_spent = brand_summary.sort_values('dollar_spent', ascending=False)\n\n\n# Plotting total quantity for each brand\nplt.figure(figsize=(10, 8))\nsns.barplot(x='quantity', y='brand', data=brand_summary_sorted_quantity, palette='viridis')\nplt.title('Total Quantity of Beer Purchased by Brand')\nplt.xlabel('Total Quantity')\nplt.ylabel('Brand')\nplt.show()\n\n\n\n# Plotting total spending for each brand\nplt.figure(figsize=(10, 8))\nsns.barplot(x='dollar_spent', y='brand', data=brand_summary_sorted_spent, palette='viridis')\nplt.title('Total Spending on Beer by Brand')\nplt.xlabel('Total Spending')\nplt.ylabel('Brand')\nplt.show()\n\n\n\n\nThe bar charts above display the total quantity of beer purchased and the total spending by brand. From the looks of it, certain brands dominate in terms of quantity sold and total spending, indicating their popularity.\nNow, let’s calculate the average quantity purchased and average spending per purchase. For this, we’ll consider each row in the dataset as a separate purchase and compute the averages accordingly.\n\n# Calculate average quantity purchased and average spending per purchase\naverage_purchase = beer_data.groupby('brand').agg({\n    'quantity': 'mean', \n    'dollar_spent': 'mean'\n}).reset_index()\n\n# Sort by average quantity and average spending\naverage_purchase_sorted_quantity = average_purchase.sort_values('quantity', ascending=False)\naverage_purchase_sorted_spent = average_purchase.sort_values('dollar_spent', ascending=False)\n\n# Plotting average quantity for each brand\nplt.figure(figsize=(10, 8))\nsns.barplot(x='quantity', y='brand', data=average_purchase_sorted_quantity, palette='viridis')\nplt.title('Average Quantity of Beer Purchased by Brand')\nplt.xlabel('Average Quantity')\nplt.ylabel('Brand')\nplt.show()\n\n\n\n# Plotting average spending for each brand\nplt.figure(figsize=(10, 8))\nsns.barplot(x='dollar_spent', y='brand', data=average_purchase_sorted_spent, palette='viridis')\nplt.title('Average Spending on Beer by Brand')\nplt.xlabel('Average Spending')\nplt.ylabel('Brand')\nplt.show()\n\n\n\n\nThe visualizations above depict the average quantity of beer purchased per brand and the average spending per brand. This shows which brands tend to be bought in larger quantities on average and which brands tend to have higher spending per purchase, which could be indicative of their price point or the purchase of premium products.\nNext, we’ll look at the total spending across different markets to see if there are any notable differences in spending habits geographically. To do this, we’ll sum up the spending in each market and visualize it.\n\n# Calculate total spending in each market\nmarket_spending_summary = beer_data.groupby('market').agg({'dollar_spent':'sum'}).reset_index()\n\n# Sort by total spending\nmarket_spending_summary_sorted = market_spending_summary.sort_values('dollar_spent', ascending=False)\n\n# Plotting total spending in each market\nplt.figure(figsize=(12, 10))\nsns.barplot(x='dollar_spent', y='market', data=market_spending_summary_sorted, palette='viridis')\nplt.title('Total Spending on Beer by Market')\nplt.xlabel('Total Spending')\nplt.ylabel('Market')\nplt.show()\n\n\n\n\nThe bar chart illustrates the total spending on beer by market, showcasing the differences in spending habits across various regions. Some markets have significantly higher spending, which could be due to a variety of factors including market size, consumer preferences, or economic factors.\nNow, let’s move on to the second analysis:"
  },
  {
    "objectID": "posts/beer-markets/beer-markets.html#demographic-analysis",
    "href": "posts/beer-markets/beer-markets.html#demographic-analysis",
    "title": "Beer Markets",
    "section": "Demographic Analysis",
    "text": "Demographic Analysis\nWe will examine which demographics are buying what kind of beer and whether spending habits vary by demographics such as age, employment, and race. For this, we could look at:\n\nSpending by age group\nSpending by employment status\nSpending by race\n\nI’ll start by analyzing spending by age group.\n\n# Calculate total spending by age group\nage_group_spending = beer_data.groupby('age').agg({'dollar_spent':'sum'}).reset_index()\n\n# Sort by total spending\nage_group_spending_sorted = age_group_spending.sort_values('dollar_spent', ascending=False)\n\n# Plotting total spending by age group\nplt.figure(figsize=(10, 6))\nsns.barplot(x='dollar_spent', y='age', data=age_group_spending_sorted, palette='viridis')\nplt.title('Total Spending on Beer by Age Group')\nplt.xlabel('Total Spending')\nplt.ylabel('Age Group')\nplt.show()\n\n\n\n\nThe bar chart demonstrates the total spending on beer segmented by age group, highlighting which age groups spend the most on beer. It appears that certain age groups are more dominant in beer spending, which may align with the purchasing power or preferences of those groups.\nNext, we will examine spending by employment status.\n\n# Calculate total spending by employment status\nemployment_spending = beer_data.groupby('employment').agg({'dollar_spent':'sum'}).reset_index()\n\n# Sort by total spending\nemployment_spending_sorted = employment_spending.sort_values('dollar_spent', ascending=False)\n\n# Plotting total spending by employment status\nplt.figure(figsize=(10, 6))\nsns.barplot(x='dollar_spent', y='employment', data=employment_spending_sorted, palette='viridis')\nplt.title('Total Spending on Beer by Employment Status')\nplt.xlabel('Total Spending')\nplt.ylabel('Employment Status')\nplt.show()\n\n\n\n\nThe visualization shows the total spending on beer by employment status. We can see that certain employment groups, such as full-time workers, are spending more on beer, which might be related to their disposable income.\nFinally, let’s look at spending by race to complete the demographic analysis.\n\n# Calculate total spending by race\nrace_spending = beer_data.groupby('race').agg({'dollar_spent':'sum'}).reset_index()\n\n# Sort by total spending\nrace_spending_sorted = race_spending.sort_values('dollar_spent', ascending=False)\n\n# Plotting total spending by race\nplt.figure(figsize=(10, 6))\nsns.barplot(x='dollar_spent', y='race', data=race_spending_sorted, palette='viridis')\nplt.title('Total Spending on Beer by Race')\nplt.xlabel('Total Spending')\nplt.ylabel('Race')\nplt.show()\n\n\n\n\nThe bar chart above indicates the total spending on beer broken down by race, highlighting which racial groups account for the most beer spending within the dataset. This could reflect both the demographics of the regions where the data was collected and cultural preferences regarding beer.\nNow, let’s proceed to the third analysis:"
  },
  {
    "objectID": "posts/beer-markets/beer-markets.html#price-sensitivity",
    "href": "posts/beer-markets/beer-markets.html#price-sensitivity",
    "title": "Beer Markets",
    "section": "Price Sensitivity",
    "text": "Price Sensitivity\nWe’ll look at the price per fluid ounce and see if there are any trends or correlations with the quantity purchased or the brand popularity. To do this, we’ll calculate the average price per fluid ounce for each brand and then visualize how this relates to the average quantity purchased and the total quantity purchased by brand.\nFirst, let’s calculate the average price per fluid ounce for each brand.\n\n# Calculate average price per fluid ounce for each brand\nbrand_price_sensitivity = beer_data.groupby('brand').agg({\n    'price_per_floz': 'mean', \n    'quantity': 'sum'\n}).reset_index()\n\n# Sort by price per fluid ounce\nbrand_price_sensitivity_sorted = brand_price_sensitivity.sort_values('price_per_floz', ascending=True)\n\n# Plotting average price per fluid ounce for each brand and the total quantity purchased\nfig, ax1 = plt.subplots(figsize=(12, 10))\n\ncolor = 'tab:red'\nax1.set_xlabel('Brand')\nax1.set_ylabel('Average Price per Fluid Ounce', color=color)\nax1.bar(brand_price_sensitivity_sorted['brand'], brand_price_sensitivity_sorted['price_per_floz'], color=color)\nax1.tick_params(axis='y', labelcolor=color)\nax1.set_xticklabels(brand_price_sensitivity_sorted['brand'], rotation=90)\n\nax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\ncolor = 'tab:blue'\nax2.set_ylabel('Total Quantity Purchased', color=color)  # we already handled the x-label with ax1\nax2.plot(brand_price_sensitivity_sorted['brand'], brand_price_sensitivity_sorted['quantity'], color=color)\nax2.tick_params(axis='y', labelcolor=color)\n\nfig.tight_layout()  # otherwise the right y-label is slightly clipped\nplt.title('Average Price per Fluid Ounce & Total Quantity Purchased by Brand')\nplt.show()\n\n\n\n\nIn the visualization, we have a bar graph showing the average price per fluid ounce for each brand (in red) and a line graph showing the total quantity purchased for each brand (in blue). This gives us a sense of whether there’s a relationship between the price and the quantity purchased. The x-axis labels are quite compressed due to the number of brands, but we can still observe trends such as whether lower-priced beers tend to be purchased in larger quantities.\nLastly, let’s move to the fourth analysis:"
  },
  {
    "objectID": "posts/beer-markets/beer-markets.html#promotional-impact",
    "href": "posts/beer-markets/beer-markets.html#promotional-impact",
    "title": "Beer Markets",
    "section": "Promotional Impact",
    "text": "Promotional Impact\nWe’ll assess the impact of promotions on the quantity of beer purchased. For this analysis, we can calculate the average quantity purchased with and without promotions and visualize the difference. We’ll do this for each brand to see which brands are most affected by promotions.\nLet’s begin this analysis by looking at the average quantity purchased with and without promotions for each brand.\n\n# Calculate average quantity purchased with and without promotions for each brand\npromo_impact = beer_data.groupby(['brand', 'promo']).agg({'quantity':'mean'}).reset_index()\n\n# Pivot the data to have promo and non-promo side by side for each brand\npromo_impact_pivot = promo_impact.pivot(index='brand', columns='promo', values='quantity').reset_index()\npromo_impact_pivot.columns = ['brand', 'non_promo', 'promo']\n\n# Calculate the difference in average quantity purchased between promo and non-promo\npromo_impact_pivot['promo_impact'] = promo_impact_pivot['promo'] - promo_impact_pivot['non_promo']\n\n# Sort by the impact of promo\npromo_impact_pivot_sorted = promo_impact_pivot.sort_values('promo_impact', ascending=False)\n\n# Plotting the difference in average quantity purchased between promo and non-promo for each brand\nplt.figure(figsize=(12, 10))\nsns.barplot(x='promo_impact', y='brand', data=promo_impact_pivot_sorted, palette='viridis')\nplt.title('Impact of Promotions on Average Quantity Purchased by Brand')\nplt.xlabel('Difference in Average Quantity Purchased (Promo - Non-Promo)')\nplt.ylabel('Brand')\nplt.show()\n\n\n\n\nThe bar chart illustrates the impact of promotions on the average quantity of beer purchased by brand. A positive value indicates that, on average, more beer is purchased when there is a promotion compared to when there isn’t. Some brands appear to be significantly more influenced by promotions, with customers buying more when the products are on sale or promotion.\nThis comprehensive analysis has provided insights into purchase patterns, demographic preferences, price sensitivity, and the impact of promotions on beer purchases."
  },
  {
    "objectID": "BlogPosts/Spotify_Blog_files/Spotify_Blog.html",
    "href": "BlogPosts/Spotify_Blog_files/Spotify_Blog.html",
    "title": "Spotify Artist Dataset",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(skimr)\nlibrary(ggthemes)"
  },
  {
    "objectID": "BlogPosts/Spotify_Blog_files/Spotify_Blog.html#loading-in-data-set",
    "href": "BlogPosts/Spotify_Blog_files/Spotify_Blog.html#loading-in-data-set",
    "title": "Spotify Artist Dataset",
    "section": "Loading in Data set",
    "text": "Loading in Data set\n\nspotify_all &lt;- read_csv('https://bcdanl.github.io/data/spotify_all.csv')\n\nRows: 198005 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): playlist_name, artist_name, track_name, album_name\ndbl (3): pid, pos, duration_ms\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nspotify_all\n\n# A tibble: 198,005 × 7\n     pid playlist_name   pos artist_name       track_name duration_ms album_name\n   &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;             &lt;chr&gt;            &lt;dbl&gt; &lt;chr&gt;     \n 1     0 Throwbacks        0 Missy Elliott     Lose Cont…      226863 The Cookb…\n 2     0 Throwbacks        1 Britney Spears    Toxic           198800 In The Zo…\n 3     0 Throwbacks        2 Beyoncé           Crazy In …      235933 Dangerous…\n 4     0 Throwbacks        3 Justin Timberlake Rock Your…      267266 Justified \n 5     0 Throwbacks        4 Shaggy            It Wasn't…      227600 Hot Shot  \n 6     0 Throwbacks        5 Usher             Yeah!           250373 Confessio…\n 7     0 Throwbacks        6 Usher             My Boo          223440 Confessio…\n 8     0 Throwbacks        7 The Pussycat Dol… Buttons         225560 PCD       \n 9     0 Throwbacks        8 Destiny's Child   Say My Na…      271333 The Writi…\n10     0 Throwbacks        9 OutKast           Hey Ya! -…      235213 Speakerbo…\n# ℹ 197,995 more rows"
  },
  {
    "objectID": "BlogPosts/Spotify_Blog_files/Spotify_Blog.html#the-artists-for-the-ten-most-popular-songs",
    "href": "BlogPosts/Spotify_Blog_files/Spotify_Blog.html#the-artists-for-the-ten-most-popular-songs",
    "title": "Spotify Artist Dataset",
    "section": "The Artists for the Ten Most Popular Songs",
    "text": "The Artists for the Ten Most Popular Songs\n\nQ1a &lt;- spotify_all %&gt;% \n  arrange(-`duration_ms`) %&gt;% \n  filter(playlist_name != \"Audiobooks\") %&gt;% \n  slice_head(n = 10) %&gt;% \n  select(`artist_name`)\n\nQ1a\n\n# A tibble: 10 × 1\n   artist_name          \n   &lt;chr&gt;                \n 1 Sound Dreamer        \n 2 Best of 9JA          \n 3 Late Night Tales     \n 4 Grandmaster Flash    \n 5 Brian Eno            \n 6 Reggae Mix USA       \n 7 Eric Thomas          \n 8 D.VELOPED            \n 9 Deftones             \n10 Jonathan David Helser"
  },
  {
    "objectID": "BlogPosts/Spotify_Blog_files/Spotify_Blog.html#the-five-most-common-artists-in-the-data-frame",
    "href": "BlogPosts/Spotify_Blog_files/Spotify_Blog.html#the-five-most-common-artists-in-the-data-frame",
    "title": "Spotify Artist Dataset",
    "section": "The Five Most Common Artists in the Data Frame 🧑‍🎨",
    "text": "The Five Most Common Artists in the Data Frame 🧑‍🎨\n\nQ1b &lt;- spotify_all %&gt;% \n  group_by(`artist_name`) %&gt;% \n  summarise(count = n()) %&gt;% \n  arrange(-count) %&gt;% \n  slice_head(n = 5)\n\nQ1b\n\n# A tibble: 5 × 2\n  artist_name    count\n  &lt;chr&gt;          &lt;int&gt;\n1 Drake           2715\n2 Kanye West      1065\n3 Kendrick Lamar  1035\n4 Rihanna          915\n5 The Weeknd       913\n\n\n\nggplot(data = Q1b)+\n  geom_col(mapping = aes(x = artist_name,\n                             y = count,\n           color = artist_name))\n\n\n\n\n\n\n\n\nAs we can see Drake is very Popular 🎵\n\nMost popular songs by each of the 5 most popular artists\n\nQ1b2 &lt;- spotify_all %&gt;% \n  group_by(`track_name`, `artist_name`) %&gt;%\n  filter(`artist_name` %in% Q1b$artist_name) %&gt;% \n  summarise(po_song = n()) %&gt;% \n  arrange(-po_song)\n\n`summarise()` has grouped output by 'track_name'. You can override using the\n`.groups` argument.\n\nanswer &lt;- Q1b2 %&gt;% \n  group_by(`artist_name`) %&gt;% \n  filter(po_song == max(po_song)) %&gt;% \n  unite(Name_and_Track, track_name, artist_name, sep = \",\")\n\nanswer\n\n# A tibble: 5 × 2\n  Name_and_Track         po_song\n  &lt;chr&gt;                    &lt;int&gt;\n1 One Dance,Drake            143\n2 HUMBLE.,Kendrick Lamar     142\n3 Starboy,The Weeknd         100\n4 Gold Digger,Kanye West      83\n5 Needed Me,Rihanna           79\n\n\n\nggplot(data = answer)+\n  geom_col(mapping = aes(x = Name_and_Track,\n                         y = po_song,\n  color = Name_and_Track))+\nylab(\"Song Occurences\")"
  },
  {
    "objectID": "quarto-template.html",
    "href": "quarto-template.html",
    "title": "DANL 200: Introduction to Data AnalyticsProjects",
    "section": "",
    "text": "oj &lt;- read_csv('https://bcdanl.github.io/data/dominick_oj.csv')\nnvars &lt;- format(round(ncol(oj), 0), \n                nsmall=0, \n                big.mark=\",\") \nnobs &lt;- format(round(nrow(oj), 0), \n                nsmall=0, \n                big.mark=\",\")\nThe number of variables is 4; the number of observations is 28,947.\nRoses are red\nviolets are blue."
  },
  {
    "objectID": "quarto-template.html#data-summary",
    "href": "quarto-template.html#data-summary",
    "title": "DANL 200: Introduction to Data AnalyticsProjects",
    "section": "2.1 Data Summary",
    "text": "2.1 Data Summary\n\nSummary statistics (Use skimr::skim())"
  },
  {
    "objectID": "quarto-template.html#data-visualization",
    "href": "quarto-template.html#data-visualization",
    "title": "DANL 200: Introduction to Data AnalyticsProjects",
    "section": "2.2 Data Visualization",
    "text": "2.2 Data Visualization\n\noj %&gt;% \n  ggplot(aes(x = log(sales), \n             y = log(price),\n             color = brand)) +\n  geom_point(alpha = .1) +\n  geom_smooth(method = lm, se = F) +\n  facet_wrap(.~ad) +\n  theme_bw() +\n  theme(legend.position = 'top')"
  },
  {
    "objectID": "quarto-template.html#data-transformation",
    "href": "quarto-template.html#data-transformation",
    "title": "DANL 200: Introduction to Data AnalyticsProjects",
    "section": "2.3 Data Transformation",
    "text": "2.3 Data Transformation\n\nob_sum1 &lt;- oj %&gt;% \n  group_by(brand, ad) %&gt;% \n  summarise(sales_tot = sum(sales, na.rm = T),\n            price_mean = round(mean(price, na.rm = T), 2))"
  },
  {
    "objectID": "quarto-template.html#analysis",
    "href": "quarto-template.html#analysis",
    "title": "DANL 200: Introduction to Data AnalyticsProjects",
    "section": "2.4 Analysis",
    "text": "2.4 Analysis"
  },
  {
    "objectID": "quarto-template.html#quotes",
    "href": "quarto-template.html#quotes",
    "title": "DANL 200: Introduction to Data AnalyticsProjects",
    "section": "2.5 Quotes",
    "text": "2.5 Quotes\n\nQuote with &gt;\n\n\n“The truth is rarely pure and never simple.”\n— Oscar Wilde"
  },
  {
    "objectID": "quarto-template.html#inserting-figures",
    "href": "quarto-template.html#inserting-figures",
    "title": "DANL 200: Introduction to Data AnalyticsProjects",
    "section": "2.6 Inserting Figures",
    "text": "2.6 Inserting Figures\nFor a demonstration of a DANL tiger, see Figure 1.\n\n\n\n\n\nFigure 1: DANL Tiger"
  },
  {
    "objectID": "quarto-template.html#inserting-a-html-page",
    "href": "quarto-template.html#inserting-a-html-page",
    "title": "DANL 200: Introduction to Data AnalyticsProjects",
    "section": "2.7 Inserting a HTML page",
    "text": "2.7 Inserting a HTML page"
  },
  {
    "objectID": "Economic_.html",
    "href": "Economic_.html",
    "title": "Economic_Model",
    "section": "",
    "text": "PFAS producing firm is either a monopoly or a competitive firm. How do the optimal quantities of the PFAS product differ for each type of firm? How does the tax rate differ for each firm?"
  },
  {
    "objectID": "Economic_.html#cost-function",
    "href": "Economic_.html#cost-function",
    "title": "Economic_Model",
    "section": "Cost function",
    "text": "Cost function\n\\[\n\\frac{k}{2}q^2\n\\]"
  },
  {
    "objectID": "Economic_.html#external-damage-function",
    "href": "Economic_.html#external-damage-function",
    "title": "Economic_Model",
    "section": "External Damage function",
    "text": "External Damage function\n\\[\n\\frac{d}{2}q^2\n\\]"
  },
  {
    "objectID": "Economic_.html#demand-function",
    "href": "Economic_.html#demand-function",
    "title": "Economic_Model",
    "section": "Demand Function",
    "text": "Demand Function\n\\[\n(1-q)\n\\]"
  },
  {
    "objectID": "Economic_.html#f.o.c.-1",
    "href": "Economic_.html#f.o.c.-1",
    "title": "Economic_Model",
    "section": "F.O.C. 1",
    "text": "F.O.C. 1\n\\[\n\\frac{dSW}{dq} = 1-q(1+k+d)=0\n\\]\n\\[\nq^o = \\frac{1}{1+k+d}\n\\]"
  },
  {
    "objectID": "Economic_.html#f.o.c.-2",
    "href": "Economic_.html#f.o.c.-2",
    "title": "Economic_Model",
    "section": "F.O.C. 2",
    "text": "F.O.C. 2\n\\[\n\\frac{dSW}{dq} = 1-q-kq-dq =0\n\\]\n\\[\nq^0 = \\frac{1}{1+k+d}\n\\]"
  },
  {
    "objectID": "Economic_.html#f.o.c.-3",
    "href": "Economic_.html#f.o.c.-3",
    "title": "Economic_Model",
    "section": "F.O.C. 3",
    "text": "F.O.C. 3\n\\[\n\\frac{d\\pi}{dq} = 1-2q-kq-\\tau = 0\n\\]\n\\[\n\\frac{1-\\tau}{2+k} = q^m\n\\]\n\nCompetitive firm optimal level with tax\n\\[\nmax_q \\ \\pi^c = pq-\\frac{k}{2}q^2-\\tau q\\\n\\]"
  },
  {
    "objectID": "Economic_.html#f.o.c.-4",
    "href": "Economic_.html#f.o.c.-4",
    "title": "Economic_Model",
    "section": "F.O.C. 4",
    "text": "F.O.C. 4\n\\[\n\\frac{d\\pi^c}{dq}=p-kq-\\tau=0\n\\]\n\\[\np=kq+\\tau\n\\]\nSince the competitive firm is the price taker we must assume an individual’s utility function that is representative of the market.\n\\[\nmax_q \\ U = Y - pq\\ + \\frac{1}{2}q\\ (2-q)\n\\]"
  },
  {
    "objectID": "Economic_.html#f.o.c.-5",
    "href": "Economic_.html#f.o.c.-5",
    "title": "Economic_Model",
    "section": "F.O.C. 5",
    "text": "F.O.C. 5\n\\[\n\\frac{dU}{dq} = -p \\ + 1-q =0\n\\]\n\\[\np=1-q\n\\]\nNow that we have derived the supply and demand function we can set them equal to each other to find the market equilibrium quantity of q.\n\nMarket Equilibrium\n\\[\nkq+\\tau = 1-q\n\\]\n\\[\nq^c = \\frac{1-\\tau}{k+1}\n\\]\nThe optimal quantity of q with the tax is greater for the competitive firm than for the monopolistic firm.\n\\[\nq^c &gt; q^m\n\\]"
  },
  {
    "objectID": "Restaurant_Inspection.html",
    "href": "Restaurant_Inspection.html",
    "title": "Restaurant Inspection Data",
    "section": "",
    "text": "restaurant &lt;- read_csv(\n  'https://bcdanl.github.io/data/DOHMH_NYC_Restaurant_Inspection.csv'\n)\n\nRows: 17633 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (10): DBA, BORO, STREET, CUISINE DESCRIPTION, INSPECTION DATE, ACTION, V...\ndbl  (2): CAMIS, SCORE\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThis dataset has 17633 observations and 12 variables. It contains information on health and hygiene rankings. Each restaurant has information about location, type of cuisine and performance on health inspections."
  },
  {
    "objectID": "Restaurant_Inspection.html#loading-in-data",
    "href": "Restaurant_Inspection.html#loading-in-data",
    "title": "Restaurant Inspection Data",
    "section": "",
    "text": "restaurant &lt;- read_csv(\n  'https://bcdanl.github.io/data/DOHMH_NYC_Restaurant_Inspection.csv'\n)\n\nRows: 17633 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (10): DBA, BORO, STREET, CUISINE DESCRIPTION, INSPECTION DATE, ACTION, V...\ndbl  (2): CAMIS, SCORE\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThis dataset has 17633 observations and 12 variables. It contains information on health and hygiene rankings. Each restaurant has information about location, type of cuisine and performance on health inspections."
  },
  {
    "objectID": "Restaurant_Inspection.html#creating-descriptive-statistics",
    "href": "Restaurant_Inspection.html#creating-descriptive-statistics",
    "title": "Restaurant Inspection Data",
    "section": "Creating Descriptive Statistics",
    "text": "Creating Descriptive Statistics\n\nQ1a &lt;- restaurant %&gt;% \n  group_by(GRADE) %&gt;% \n  summarise(SD_value = sd(SCORE),\n            first_quartile = quantile(SCORE, 0.25),\n            median_value = median(SCORE),\n            third_quartile = quantile(SCORE, .75),\n            MAX_value = max(SCORE))\n\n\nRelationship between Grade and Score\nBox plots can show us the relationship between a character variable and a quantitative variable.\n\nggplot(data = restaurant)+\n  geom_boxplot(mapping = aes(x = GRADE, y = SCORE))\n\n\n\n\n###Relationship between grade score and critical flag\n\nggplot(data = restaurant)+\n  geom_boxplot(mapping = aes(x = `CRITICAL FLAG`, y = SCORE))\n\n\n\n\n\n\nIn this ggplot graph we can see which Cuisine Description has the highest proportion of Grade A rankings\n\nQ1e &lt;- restaurant %&gt;% \n  group_by(`CUISINE DESCRIPTION`) %&gt;% \n  summarise(count = n()) %&gt;% \n  arrange(-count) %&gt;% \n  head(n = 10)\n\nQ1e\n\n# A tibble: 10 × 2\n   `CUISINE DESCRIPTION`    count\n   &lt;chr&gt;                    &lt;int&gt;\n 1 American                  3678\n 2 Coffee/Tea                1414\n 3 Chinese                   1372\n 4 Pizza                     1050\n 5 Italian                    689\n 6 Bakery Products/Desserts   615\n 7 Japanese                   584\n 8 Mexican                    582\n 9 Latin American             544\n10 Donuts                     505\n\n\n\nGrades &lt;- restaurant %&gt;% \n  filter(`CUISINE DESCRIPTION` %in% c(Q1e$`CUISINE DESCRIPTION`))\nGrades\n\n# A tibble: 11,033 × 12\n      CAMIS DBA      BORO  STREET `CUISINE DESCRIPTION` `INSPECTION DATE` ACTION\n      &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;                 &lt;chr&gt;             &lt;chr&gt; \n 1 40356018 riviera… Broo… STILL… American              02/01/2022        Viola…\n 2 40357217 wild as… Bronx SOUTH… American              07/28/2021        Viola…\n 3 40359480 1 east … Manh… EAST … American              05/03/2022        Viola…\n 4 40362264 p & s d… Manh… COLUM… American              02/07/2023        Viola…\n 5 40362274 angelik… Manh… WEST … American              02/14/2022        Viola…\n 6 40362432 ho mei … Quee… 37 AV… Chinese               10/25/2022        Viola…\n 7 40363298 cafe me… Manh… 8 AVE… American              09/02/2022        Viola…\n 8 40363920 new gol… Broo… RUTLA… Chinese               02/16/2022        Viola…\n 9 40364305 philade… Broo… 4 AVE… Pizza                 01/23/2023        Viola…\n10 40364347 metropo… Manh… EAST … American              04/12/2022        Viola…\n# ℹ 11,023 more rows\n# ℹ 5 more variables: `VIOLATION CODE` &lt;chr&gt;, `VIOLATION DESCRIPTION` &lt;chr&gt;,\n#   `CRITICAL FLAG` &lt;chr&gt;, SCORE &lt;dbl&gt;, GRADE &lt;chr&gt;\n\n\n\nggplot(data = Grades)+\n  geom_bar(mapping = aes(x = `CUISINE DESCRIPTION`, \n                         y = after_stat(prop),\n                         group = GRADE,\n                         fill = GRADE))\n\n\n\n\nFrom this ggplot we can see that donuts have the highest proportion of A grades for health.\n\n\nMost popular restaurant type\n\nQ1f &lt;- restaurant %&gt;% \n  group_by(`DBA`) %&gt;% \n  summarise(count = n()) %&gt;%   \n  arrange(desc(count))\n\nQ1f\n\n# A tibble: 13,914 × 2\n   DBA                     count\n   &lt;chr&gt;                   &lt;int&gt;\n 1 dunkin                    402\n 2 starbucks                 286\n 3 mcdonald's                170\n 4 subway                    166\n 5 dunkin', baskin robbins   107\n 6 popeyes                    98\n 7 chipotle mexican grill     91\n 8 burger king                63\n 9 domino's                   60\n10 kennedy fried chicken      52\n# ℹ 13,904 more rows"
  },
  {
    "objectID": "Homework2.html",
    "href": "Homework2.html",
    "title": "The Fab Four: An Exploration of the Beatles Solo Careers Through Data",
    "section": "",
    "text": "import pandas as pd\nspotify = pd.read_csv('https://bcdanl.github.io/data/spotify_all.csv')\nThe Beatles were a revolutionary Rock n’ Roll group that were active throughout the 1960’s and 70’s. In their early years they brought together blues and rock n’ roll style with the modern form of the four man rock band. Over the groups career they each developed their own styles and sounds as musicians which then influenced there individual solo careers. Although each of them are talented in their own way they all received varying levels of commercial and critical success. This data analysis takes a look at how each members popularity stands today among modern listeners."
  },
  {
    "objectID": "Homework2.html#the-data",
    "href": "Homework2.html#the-data",
    "title": "The Fab Four: An Exploration of the Beatles Solo Careers Through Data",
    "section": "The Data",
    "text": "The Data\nThe code below uses filtering methods and the isin function to narrow down the data set to just the individual music of each artist\n\nArtists = [\"Paul McCartney\", \"John Lennon\" , \"Ringo Starr\", \"George Harrison\" ]\n\nBeatles = spotify[\"artist_name\"].isin(Artists)\n\nIndividual_Beatles = spotify[Beatles]\n\nIndividual_Beatles.nunique()\n\n\n\n\n\npid              59\nplaylist_name    58\npos              75\nartist_name       4\ntrack_name       50\nduration_ms      50\nalbum_name       31\ndtype: int64"
  },
  {
    "objectID": "Homework2.html#data-manipulation-and-approach",
    "href": "Homework2.html#data-manipulation-and-approach",
    "title": "The Fab Four: An Exploration of the Beatles Solo Careers Through Data",
    "section": "Data manipulation and Approach",
    "text": "Data manipulation and Approach\nMy technique involves filtering the data for each Beatle to then find statistics on, amount of minutes each is listened to, and their individual representation within the dataset. The metric I will be using to determine popularity is amount of playlists they appear in and how many songs they have in the entire dataset.\n\nPaul\nPaul was the bassist for the Beatles and one of the main singers and songwriters. He is known for his signature lefty bass playing.\n\nPaul = (\n    Individual_Beatles\n        .set_index(\"artist_name\")\n        .loc[\"Paul McCartney\"]\n)\n\nPaul.count()\n\npid              44\nplaylist_name    44\npos              44\ntrack_name       44\nduration_ms      44\nalbum_name       44\ndtype: int64\n\n\n\nPaul[\"pid\"].nunique()\n\n30\n\n\n\n\nGeorge\nGeorge was the lead Guitarist of the group and was known for his simple yet elegant solos and fills.\n\nGeorge = (\n    Individual_Beatles\n        .set_index(\"artist_name\")\n        .loc[\"George Harrison\"]\n)\n\nGeorge.count()\n\npid              27\nplaylist_name    27\npos              27\ntrack_name       27\nduration_ms      27\nalbum_name       27\ndtype: int64\n\n\n\nGeorge[\"pid\"].nunique()\n\n18\n\n\n\n\nRingo\nRingo was the groups drummer. He was known for his quirky song writing and goofy personality.\n\nRingo = (\n    Individual_Beatles\n        .set_index(\"artist_name\")\n        .loc[\"Ringo Starr\"]\n)\n\nRingo.count()\n\npid              5\nplaylist_name    5\npos              5\ntrack_name       5\nduration_ms      5\nalbum_name       5\ndtype: int64\n\n\n\nRingo[\"pid\"].nunique()\n\n2\n\n\n\n\nJohn\nJohn was the rythm guitarist and wrote many of the groups most popular songs.\n\nJohn = (\n    Individual_Beatles\n        .set_index(\"artist_name\")\n        .loc[\"John Lennon\"]\n)\n\nJohn.count()\n\npid              32\nplaylist_name    32\npos              32\ntrack_name       32\nduration_ms      32\nalbum_name       32\ndtype: int64\n\n\n\nJohn[\"pid\"].nunique()\n\n26"
  },
  {
    "objectID": "danl-210-quarto-py-only.html",
    "href": "danl-210-quarto-py-only.html",
    "title": "DANL 210: Data Preparation and Management",
    "section": "",
    "text": "reticulate::use_condaenv(\"/Users/bchoe/anaconda3\", required = TRUE)"
  },
  {
    "objectID": "danl-210-quarto-py-only.html#variables-and-data-types",
    "href": "danl-210-quarto-py-only.html#variables-and-data-types",
    "title": "DANL 210: Data Preparation and Management",
    "section": "1.1 Variables and Data Types",
    "text": "1.1 Variables and Data Types\nIn Python, variables can store data of different types without explicitly declaring the type.\nFor example:\n\n\nCode\ninteger_variable = 10\nstring_variable = 'Hello'\nfloat_variable = 10.5\n\nfloat_variable\n\n\n10.5"
  },
  {
    "objectID": "danl-210-quarto-py-only.html#control-structures",
    "href": "danl-210-quarto-py-only.html#control-structures",
    "title": "DANL 210: Data Preparation and Management",
    "section": "1.2 Control Structures",
    "text": "1.2 Control Structures\nPython supports the usual logical conditions from mathematics:\n\n\nCode\n# Equals: a == b\n# Not Equals: a != b\n# Less than: a &lt; b\n# Less than or equal to: a &lt;= b\n# Greater than: a &gt; b\n# Greater than or equal to: a &gt;= b\n\n\nThese conditions can be used in several ways, most commonly in ‘if statements’ and loops.\n\n\nCode\n# if statement:\nif 5 &gt; 2:\n    print('Five is greater than two!')\n\n\nFive is greater than two!"
  },
  {
    "objectID": "danl-210-quarto-py-only.html#functions",
    "href": "danl-210-quarto-py-only.html#functions",
    "title": "DANL 210: Data Preparation and Management",
    "section": "1.3 Functions",
    "text": "1.3 Functions\nA function is a block of code which only runs when it is called.\nYou can pass data, known as parameters, into a function.\nA function can return data as a result.\n\n\nCode\n# Defining a function:\ndef my_function():\n    print('Hello from a function')\n\n# Calling a function:\nmy_function()\n\n\nHello from a function"
  },
  {
    "objectID": "danl-210-quarto-py-only.html#lists-and-dictionaries",
    "href": "danl-210-quarto-py-only.html#lists-and-dictionaries",
    "title": "DANL 210: Data Preparation and Management",
    "section": "1.4 Lists and Dictionaries",
    "text": "1.4 Lists and Dictionaries\nA list is a collection which is ordered and changeable.\nA dictionary is a collection which is unordered, changeable and indexed.\n\n\nCode\n# List example:\nmy_list = ['apple', 'banana', 'cherry']\n\n# Dictionary example:\nmy_dict = {'name': 'John', 'age': 36}"
  },
  {
    "objectID": "danl-210-quarto-py-only.html#import-python-libraries",
    "href": "danl-210-quarto-py-only.html#import-python-libraries",
    "title": "DANL 210: Data Preparation and Management",
    "section": "2.1 Import Python libraries",
    "text": "2.1 Import Python libraries\n\n\nCode\nimport pandas as pd\n\n\n\n\nCode\noj = pd.read_csv('https://bcdanl.github.io/data/dominick_oj.csv')\n\n\n\n\nCode!\noj\n\n\n\n\n\n\n\n\n\nsales\nprice\nbrand\nad\n\n\n\n\n0\n8256.0\n3.87\ntropicana\n0\n\n\n1\n6144.0\n3.87\ntropicana\n0\n\n\n2\n3840.0\n3.87\ntropicana\n0\n\n\n3\n8000.0\n3.87\ntropicana\n0\n\n\n4\n8896.0\n3.87\ntropicana\n0\n\n\n...\n...\n...\n...\n...\n\n\n28942\n2944.0\n2.00\ndominicks\n0\n\n\n28943\n4928.0\n1.94\ndominicks\n0\n\n\n28944\n13440.0\n1.59\ndominicks\n0\n\n\n28945\n55680.0\n1.49\ndominicks\n0\n\n\n28946\n7040.0\n1.75\ndominicks\n0\n\n\n\n\n28947 rows × 4 columns\n\n\n\n\n\nCode\noj.describe()\n\n\n\n\n\n\n\n\n\nsales\nprice\nad\n\n\n\n\ncount\n28947.000000\n28947.000000\n28947.000000\n\n\nmean\n17312.213356\n2.282488\n0.237261\n\n\nstd\n27477.660437\n0.648001\n0.425411\n\n\nmin\n64.000000\n0.520000\n0.000000\n\n\n25%\n4864.000000\n1.790000\n0.000000\n\n\n50%\n8384.000000\n2.170000\n0.000000\n\n\n75%\n17408.000000\n2.730000\n0.000000\n\n\nmax\n716416.000100\n3.870000\n1.000000\n\n\n\n\n\n\n\n\n2.1.1 Interactive DataFrame with itables library\nIn *.ipynb on Google Colab, we can use itables or just Google Colab’s default to print DataFrame.\n\n\nCode\n !pip install itables\nfrom itables import init_notebook_mode, show\ninit_notebook_mode(all_interactive=False)\n\nshow(oj)\n\n\nRequirement already satisfied: itables in /Users/jackkatz/anaconda3/lib/python3.11/site-packages (1.6.3)\nRequirement already satisfied: IPython in /Users/jackkatz/anaconda3/lib/python3.11/site-packages (from itables) (8.15.0)\nRequirement already satisfied: pandas in /Users/jackkatz/anaconda3/lib/python3.11/site-packages (from itables) (2.0.3)\nRequirement already satisfied: numpy in /Users/jackkatz/anaconda3/lib/python3.11/site-packages (from itables) (1.24.3)\nRequirement already satisfied: backcall in /Users/jackkatz/anaconda3/lib/python3.11/site-packages (from IPython-&gt;itables) (0.2.0)\nRequirement already satisfied: decorator in /Users/jackkatz/anaconda3/lib/python3.11/site-packages (from IPython-&gt;itables) (5.1.1)\nRequirement already satisfied: jedi&gt;=0.16 in /Users/jackkatz/anaconda3/lib/python3.11/site-packages (from IPython-&gt;itables) (0.18.1)\nRequirement already satisfied: matplotlib-inline in /Users/jackkatz/anaconda3/lib/python3.11/site-packages (from IPython-&gt;itables) (0.1.6)\nRequirement already satisfied: pickleshare in /Users/jackkatz/anaconda3/lib/python3.11/site-packages (from IPython-&gt;itables) (0.7.5)\nRequirement already satisfied: prompt-toolkit!=3.0.37,&lt;3.1.0,&gt;=3.0.30 in /Users/jackkatz/anaconda3/lib/python3.11/site-packages (from IPython-&gt;itables) (3.0.36)\nRequirement already satisfied: pygments&gt;=2.4.0 in /Users/jackkatz/anaconda3/lib/python3.11/site-packages (from IPython-&gt;itables) (2.15.1)\nRequirement already satisfied: stack-data in /Users/jackkatz/anaconda3/lib/python3.11/site-packages (from IPython-&gt;itables) (0.2.0)\nRequirement already satisfied: traitlets&gt;=5 in /Users/jackkatz/anaconda3/lib/python3.11/site-packages (from IPython-&gt;itables) (5.7.1)\nRequirement already satisfied: pexpect&gt;4.3 in /Users/jackkatz/anaconda3/lib/python3.11/site-packages (from IPython-&gt;itables) (4.8.0)\nRequirement already satisfied: appnope in /Users/jackkatz/anaconda3/lib/python3.11/site-packages (from IPython-&gt;itables) (0.1.2)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /Users/jackkatz/anaconda3/lib/python3.11/site-packages (from pandas-&gt;itables) (2.8.2)\nRequirement already satisfied: pytz&gt;=2020.1 in /Users/jackkatz/anaconda3/lib/python3.11/site-packages (from pandas-&gt;itables) (2023.3.post1)\nRequirement already satisfied: tzdata&gt;=2022.1 in /Users/jackkatz/anaconda3/lib/python3.11/site-packages (from pandas-&gt;itables) (2023.3)\nRequirement already satisfied: parso&lt;0.9.0,&gt;=0.8.0 in /Users/jackkatz/anaconda3/lib/python3.11/site-packages (from jedi&gt;=0.16-&gt;IPython-&gt;itables) (0.8.3)\nRequirement already satisfied: ptyprocess&gt;=0.5 in /Users/jackkatz/anaconda3/lib/python3.11/site-packages (from pexpect&gt;4.3-&gt;IPython-&gt;itables) (0.7.0)\nRequirement already satisfied: wcwidth in /Users/jackkatz/anaconda3/lib/python3.11/site-packages (from prompt-toolkit!=3.0.37,&lt;3.1.0,&gt;=3.0.30-&gt;IPython-&gt;itables) (0.2.5)\nRequirement already satisfied: six&gt;=1.5 in /Users/jackkatz/anaconda3/lib/python3.11/site-packages (from python-dateutil&gt;=2.8.2-&gt;pandas-&gt;itables) (1.16.0)\nRequirement already satisfied: executing in /Users/jackkatz/anaconda3/lib/python3.11/site-packages (from stack-data-&gt;IPython-&gt;itables) (0.8.3)\nRequirement already satisfied: asttokens in /Users/jackkatz/anaconda3/lib/python3.11/site-packages (from stack-data-&gt;IPython-&gt;itables) (2.0.5)\nRequirement already satisfied: pure-eval in /Users/jackkatz/anaconda3/lib/python3.11/site-packages (from stack-data-&gt;IPython-&gt;itables) (0.2.2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsales\nprice\nbrand\nad\n\n\n\n\nLoading... (need help?)"
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "DANL-200 Final Project",
    "section": "",
    "text": "About this project:\nThe purpose of our project is to analyze a basket of ten stocks from the years 2013-2023. Through our analysis we make a stock recommendation for two different types of investors: A long-term and Short-term investor. We do this by analyzing changes in stock price, volatility and correlation between the different stock price movements over time."
  },
  {
    "objectID": "project.html#loading-in-data",
    "href": "project.html#loading-in-data",
    "title": "DANL-200 Final Project",
    "section": "2.1 Loading in Data",
    "text": "2.1 Loading in Data\n\nStock_data &lt;- read.csv(\"/Users/jackkatz/Downloads/data.csv\")\n\nhead(Stock_data)\n\n  Company       Date Close.Last   Volume    Open      High      Low\n1    AAPL 07/17/2023    $193.99 50520160 $191.90   $194.32  $191.81\n2    AAPL 07/14/2023    $190.69 41616240 $190.23 $191.1799  $189.63\n3    AAPL 07/13/2023    $190.54 41342340 $190.50   $191.19  $189.78\n4    AAPL 07-12-2023    $189.77 60750250 $189.68   $191.70  $188.47\n5    AAPL 07-11-2023    $188.08 46638120 $189.16   $189.30  $186.60\n6    AAPL 07-10-2023    $188.61 59922160 $189.26   $189.99 $187.035"
  },
  {
    "objectID": "project.html#summary-statistics",
    "href": "project.html#summary-statistics",
    "title": "DANL-200 Final Project",
    "section": "2.2 Summary Statistics",
    "text": "2.2 Summary Statistics\n\nStock_data_clean &lt;- Stock_data %&gt;% \n  mutate(Close.Last.Numeric = gsub(\"\\\\$\",\"\", Stock_data$Close.Last),\n         Open.Numeric = gsub(\"\\\\$\",\"\", Stock_data$Open),\n         High.Numeric = gsub(\"\\\\$\",\"\", Stock_data$High),\n         Low.Numeric = gsub(\"\\\\$\",\"\", Stock_data$Low)) %&gt;% \n  mutate(Close_Last = as.numeric(Close.Last.Numeric),\n         Open_Numeric = as.numeric(Open.Numeric),\n         High_Numeric = as.numeric(High.Numeric),\n         Low_Numeric  = as.numeric(Low.Numeric)) %&gt;% \n  select(Company, Date, Volume, Close_Last:Low_Numeric)\n\nskim(Stock_data_clean)\n\n\nData summary\n\n\nName\nStock_data_clean\n\n\nNumber of rows\n25160\n\n\nNumber of columns\n7\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nCompany\n0\n1\n3\n4\n0\n10\n0\n\n\nDate\n0\n1\n10\n10\n0\n2516\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nVolume\n0\n1\n51317642.69\n63991428.71\n1143952.00\n12003942.50\n26720830.00\n68572694.00\n1.065209e+09\n▇▁▁▁▁\n\n\nClose_Last\n0\n1\n102.46\n101.99\n1.62\n36.57\n65.68\n134.24\n6.916900e+02\n▇▂▁▁▁\n\n\nOpen_Numeric\n0\n1\n102.43\n102.00\n1.62\n36.51\n65.65\n134.32\n6.923500e+02\n▇▂▁▁▁\n\n\nHigh_Numeric\n0\n1\n103.83\n103.53\n1.69\n36.89\n66.49\n136.23\n7.009900e+02\n▇▂▁▁▁\n\n\nLow_Numeric\n0\n1\n101.01\n100.40\n1.61\n36.13\n64.92\n132.66\n6.860900e+02\n▇▂▁▁▁"
  },
  {
    "objectID": "Directed_Study_Code.html",
    "href": "Directed_Study_Code.html",
    "title": "Directed Study Data Cleaning and Regression Analysis",
    "section": "",
    "text": "Below is the abstract and r code for current economic research I am conducting on returns to English proficiency for Spanish-speaking immigrants and determinants of English proficiency for Spanish speaking immigrants.The code starts with retrieving the data from Integrated Public Use Microdata Series (IPUMS.org), cleaning the data, creating new variables, performing econometric analysis and using stargazer to create charts to dispaly the findings. The current iteration of the paper can be found on the home page under “Economic Research”. I presented these findings at the March 2024 Eastern Economic Conference in Boston.\n\n\nNew immigrants need to be successfully economically and socially integrated if they are to bring positive economic effects. One important piece of integration into a new society is language proficiency. As the U.S. economy increasingly relies on immigrant labor, it will become even more important to understand the returns to speaking English as an immigrant as well as and the determinants of English proficiency among immigrants. I focus specifically on the return to learning English as a Spanish speaker. As the U.S. becomes a more Spanish-speaker accessible country, as more government and other agencies are offering forms in Spanish, as well as broader use of Spanish and Spanish media in American culture, the incentive to learn English may be decreasing for Spanish immigrants.\n\nThis paper hypothesizes that the return to learning English has decreased over the years, both because the United States has become a more Spanish-friendly country and because the number of Spanish speakers has increased in the country over time. I analyze data from the American Community Survey (ACS) from 1980-2019. I first consider the determinants of English proficiency. I then use an instrumental variables approach to consider the effect of English proficiency on income, the return to proficiency.\n\nWhile I find no significant change in returns to English proficiency from decade to decade, there is a significant decrease between the 1980 sample and the 2019 sample. The results also show a significant reduction in the income gap between female and male Hispanic immigrants.\n\n\n\n\nlibrary(ipumsr)\nddi &lt;- read_ipums_ddi(\"usa_00018.xml\")\ndata &lt;- read_ipums_micro(ddi)\n\nUse of data from IPUMS USA is subject to conditions including that users should cite the data appropriately. Use command `ipums_conditions()` for more details.\n\n\n\n\n\nIn this section I recode the English proficiency variable, create a categorical education variable, create a variable to measure the age at arrival for all immigrants, and adjust the income variable for inflation and convert it to log values.\n\nlibrary(tidyverse)\n# SPEAKENG\n\ndata$SPEAKENG_numeric &lt;- as.numeric(data$SPEAKENG)\ndata$SPEAKENG_new &lt;- dplyr::recode(data$SPEAKENG_numeric, `3` = 1, `4` = 2, `5` = 3, `6` = 4, `1` = 5)\n\n#EDUC\n\ndata$EDUC_Grade &lt;- case_when(data$EDUC %in% c(0:1) ~ \"Elementary\",\n                             data$EDUC %in% c(2) ~ \"Middle\",\n                             data$EDUC %in% c(3:6) ~ \"High_School\",\n                             data$EDUC %in% c(7:10) ~ \"College\",\n                             data$EDUC %in% c(11) ~ \"Graduate_Degree\")\n\n#Age At Arrival Variable \n\ndata &lt;- mutate(data, AAA = AGE - (YEAR - YRIMMIG))\n\n#Inflation Adjusted Income Variable \n\ndata &lt;- filter(data, INCTOT &gt; 0) \ndata &lt;- mutate(data, Inflation_Adjusted = INCTOT*CPI99)\n\n#Log wages\ndata &lt;- mutate(data, LogWages = log(Inflation_Adjusted))\n\n\n\n\n\n\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# SPEAKENG\n\ndata$SPEAKENG_numeric &lt;- as.numeric(data$SPEAKENG)\ndata$SPEAKENG_new &lt;- dplyr::recode(data$SPEAKENG_numeric, `3` = 1, `4` = 2, `5` = 3, `6` = 4, `1` = 5)\n\n#EDUC\n\ndata$EDUC_Grade &lt;- case_when(data$EDUC %in% c(0:1) ~ \"Elementary\",\n                             data$EDUC %in% c(2) ~ \"Middle\",\n                             data$EDUC %in% c(3:6) ~ \"High_School\",\n                             data$EDUC %in% c(7:10) ~ \"College\",\n                             data$EDUC %in% c(11) ~ \"Graduate_Degree\")\n\n#Age At Arrival Variable \n\ndata &lt;- mutate(data, AAA = AGE - (YEAR - YRIMMIG))\n\n#Inflation Adjusted Income Variable \n\ndata &lt;- filter(data, INCTOT &gt; 0) \ndata &lt;- mutate(data, Inflation_Adjusted = INCTOT*CPI99)\n\n#Log wages\ndata &lt;- mutate(data, LogWages = log(Inflation_Adjusted))\n\n\n\n\n\n# 1980 --------------------------------------------------------------------\nlibrary(tidyverse)\nlibrary(AER)\n\nLoading required package: car\nLoading required package: carData\nAttaching package: 'car'\nThe following object is masked from 'package:dplyr':\n    recode\nThe following object is masked from 'package:purrr':\n    some\nLoading required package: lmtest\nLoading required package: zoo\nAttaching package: 'zoo'\nThe following objects are masked from 'package:base':\n    as.Date, as.Date.numeric\nLoading required package: sandwich\nLoading required package: survival\n\nCities_1980 &lt;- data %&gt;% \n  filter(YEAR == 1980) %&gt;% \n  filter(AAA %in% c(0:18)) %&gt;% \n  filter(YRIMMIG %in% c(1950:1964)) %&gt;% \n  mutate(Hispanic_Origin = ifelse(HISPAN %in% c(1:4), 1, 0)) %&gt;% \n  mutate(Middle = ifelse(EDUC_Grade == \"Middle\", 1, 0)) %&gt;% \n  mutate(High_School = ifelse(EDUC_Grade == \"High_School\", 1, 0)) %&gt;% \n  mutate(College = ifelse(EDUC_Grade == \"College\", 1, 0)) %&gt;% \n  mutate(Graduate_Degree = ifelse(EDUC_Grade == \"Graduate_Degree\", 1, 0)) %&gt;% \n  mutate(Gender = ifelse(SEX == 2 , 1 , 0)) %&gt;% \n  filter(Hispanic_Origin == 1) %&gt;% \n  mutate(Citizenship_Status = ifelse(CITIZEN == 2, 1, 0)) %&gt;% \n  filter(Graduate_Degree != 1) %&gt;% \n  filter(ELDCH != 99 |\n           YNGCH!= 99) %&gt;% \n  filter(STATEFIP %in% c(6, 12, 36, 48, 34) |\n           COUNTYFIP %in% c(37, 86, 73, 81, 215, 59, 85, 19, 47, 17)) %&gt;% \n  mutate(YSM = (AGE - AAA)) %&gt;% \n  filter(STATEFIP != 0 |\n           COUNTYFIP != 0) %&gt;% \n  mutate(Experience_Squared = ifelse(EDUC_Grade == \"Elementary\", ((AGE - 5 - 5)^2)/100, \n                                     ifelse(EDUC_Grade == \"Middle\", ((AGE - 8 - 5)^2)/100, \n                                            ifelse(EDUC_Grade == \"High_School\", ((AGE - 12 - 5)^2)/100, \n                                                   ifelse(EDUC_Grade == \"College\", ((AGE - 16 - 5)^2)/100, 0))))) %&gt;% \n  mutate(Experience = ifelse(EDUC_Grade == \"Elementary\", AGE - 5 - 5, \n                             ifelse(EDUC_Grade == \"Middle\", AGE - 8 - 5, \n                                    ifelse(EDUC_Grade == \"High_School\", AGE - 12 - 5, \n                                           ifelse(EDUC_Grade == \"College\", AGE - 16 - 5, 0))))) %&gt;% \n  mutate(Years_Education = ifelse(EDUC_Grade == \"Elementary\", 5, \n                                  ifelse(EDUC_Grade == \"Middle\", 8,\n                                         ifelse(EDUC_Grade == \"High_School\", 12,\n                                                ifelse(EDUC_Grade == \"College\", 16, 0))))) %&gt;% \n  mutate(Education_Dummy = ifelse(EDUC_Grade %in% c(\"Elementary\", \"Middle\"), 0, 1)) %&gt;% \n  filter(INCTOT != 9999999 | INCTOT != 9999998) %&gt;% \n  filter(ELDCH != 99 |\n           YNGCH != 99) %&gt;% \n  mutate(Hispanic_Population = \n           if_else(COUNTYFIP == 37, 27.6, \n                   if_else(COUNTYFIP == 86, 35.7, \n                           if_else(COUNTYFIP == 73, 14.8,\n                                   if_else(COUNTYFIP == 81, 13.9,\n                                           if_else(COUNTYFIP == 215, 81.3, \n                                                   \n                                                   if_else(COUNTYFIP == 59, 14.8,\n                                                           if_else(COUNTYFIP == 85, 17.5,\n                                                                   if_else(COUNTYFIP == 19, 29.3,\n                                                                           if_else(COUNTYFIP == 57, 17.6,\n                                                                                   if_else(COUNTYFIP == 17, 26.1, 0))))))))))) %&gt;% \n  filter(Hispanic_Population != 0)\n\n\nview(head(Cities_1980, 100))\n\n\n\n\n# OLS Regression on Log Wages\nOLS1980 &lt;- lm(LogWages ~ AGE + Gender  +  SPEAKENG_new + Citizenship_Status + Experience_Squared , data = Cities_1980)\nsummary(OLS1980)\n\n\nCall:\nlm(formula = LogWages ~ AGE + Gender + SPEAKENG_new + Citizenship_Status + \n    Experience_Squared, data = Cities_1980)\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.9014 -0.3299  0.1692  0.5495  1.9816 \nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         9.186201   0.308223  29.804  &lt; 2e-16 ***\nAGE                 0.054796   0.009766   5.611 2.68e-08 ***\nGender             -0.966164   0.060255 -16.035  &lt; 2e-16 ***\nSPEAKENG_new       -0.138774   0.039882  -3.480 0.000526 ***\nCitizenship_Status  0.038659   0.062648   0.617 0.537336    \nExperience_Squared -0.104708   0.021019  -4.982 7.56e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nResidual standard error: 0.8977 on 904 degrees of freedom\nMultiple R-squared:  0.2822,    Adjusted R-squared:  0.2782 \nF-statistic: 71.09 on 5 and 904 DF,  p-value: &lt; 2.2e-16\n\n#OLS Regression on SPEAKEN_eng\n\nENG1980 &lt;- lm(SPEAKENG_new ~ AAA  + ELDCH + YNGCH + \n                Hispanic_Population + Years_Education,\n              data = Cities_1980)\nsummary(ENG1980)\n\n\nCall:\nlm(formula = SPEAKENG_new ~ AAA + ELDCH + YNGCH + Hispanic_Population + \n    Years_Education, data = Cities_1980)\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.2925 -0.4762 -0.0577  0.4248  2.8313 \nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          3.573438   0.132603  26.948  &lt; 2e-16 ***\nAAA                  0.029550   0.005313   5.561 3.52e-08 ***\nELDCH                0.021850   0.005949   3.673 0.000254 ***\nYNGCH               -0.021568   0.006347  -3.398 0.000708 ***\nHispanic_Population  0.005802   0.001840   3.154 0.001665 ** \nYears_Education     -0.127407   0.007262 -17.545  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nResidual standard error: 0.7129 on 904 degrees of freedom\nMultiple R-squared:  0.3532,    Adjusted R-squared:  0.3496 \nF-statistic: 98.72 on 5 and 904 DF,  p-value: &lt; 2.2e-16\n\nEnglish1980 &lt;- predict(ENG1980)\nEnglish1980\n\n       1        2        3        4        5        6        7        8 \n2.826605 3.099828 2.799153 3.643958 2.392700 3.436822 2.670706 3.350664 \n       9       10       11       12       13       14       15       16 \n3.415485 2.870141 2.962242 2.663352 2.636064 3.211926 2.285201 1.871242 \n      17       18       19       20       21       22       23       24 \n3.124406 3.248209 2.328931 3.270625 2.201898 2.967561 2.581553 3.786320 \n      25       26       27       28       29       30       31       32 \n2.463352 2.522452 3.213622 1.955422 3.091343 3.094290 3.474411 2.558453 \n      33       34       35       36       37       38       39       40 \n2.446820 2.285083 2.234395 1.769855 2.948235 3.266548 2.753224 3.584838 \n      41       42       43       44       45       46       47       48 \n3.192263 2.536299 2.088548 2.333187 3.493531 2.487107 3.170695 2.509641 \n      49       50       51       52       53       54       55       56 \n1.882945 3.378553 3.020845 2.151443 2.555887 2.212068 2.630388 2.153423 \n      57       58       59       60       61       62       63       64 \n2.218125 2.639100 2.510772 2.775075 2.215551 2.728993 2.627724 2.794262 \n      65       66       67       68       69       70       71       72 \n2.811477 2.818210 3.316229 1.792880 2.453533 2.111365 2.816091 2.790757 \n      73       74       75       76       77       78       79       80 \n2.336720 2.043315 2.446234 2.556735 2.818210 3.013428 1.646542 3.657971 \n      81       82       83       84       85       86       87       88 \n2.729394 2.955011 1.916567 2.307620 2.985706 2.610812 2.606789 3.557735 \n      89       90       91       92       93       94       95       96 \n3.070452 2.233985 1.946419 2.738009 2.308010 2.554473 2.406721 2.004823 \n      97       98       99      100      101      102      103      104 \n2.542585 2.739922 2.338506 1.904960 2.543716 2.543716 2.274251 2.305637 \n     105      106      107      108      109      110      111      112 \n2.783340 3.391690 2.675212 2.125769 2.575529 2.598621 2.490217 2.559521 \n     113      114      115      116      117      118      119      120 \n3.239194 2.871555 3.277130 2.550579 2.657687 2.990129 3.503451 2.435553 \n     121      122      123      124      125      126      127      128 \n2.020061 2.650881 2.755423 3.073908 3.367477 2.787658 3.142681 2.839907 \n     129      130      131      132      133      134      135      136 \n3.513413 3.409035 3.255115 3.410613 2.303484 3.213276 3.531358 3.763684 \n     137      138      139      140      141      142      143      144 \n1.823125 2.512599 2.275258 3.449551 3.608687 2.300858 3.573682 2.542068 \n     145      146      147      148      149      150      151      152 \n3.440482 3.731224 2.616962 2.460543 3.279548 3.094426 2.009460 3.275306 \n     153      154      155      156      157      158      159      160 \n2.328876 1.867429 2.892967 3.489392 3.194803 2.307874 2.472558 2.080567 \n     161      162      163      164      165      166      167      168 \n2.169354 1.798649 2.720755 2.303997 2.659138 2.746257 2.716707 3.654343 \n     169      170      171      172      173      174      175      176 \n2.468927 1.874281 3.760209 3.259788 3.389083 2.174820 3.284302 2.265140 \n     177      178      179      180      181      182      183      184 \n2.197746 2.640972 2.324020 2.212863 2.442787 2.127724 2.504150 1.844509 \n     185      186      187      188      189      190      191      192 \n2.772877 2.293621 3.510402 1.726591 3.569220 3.322738 2.621094 2.441090 \n     193      194      195      196      197      198      199      200 \n2.323737 3.301178 2.749494 2.827106 2.674035 3.059284 2.829761 3.153545 \n     201      202      203      204      205      206      207      208 \n2.844195 2.981680 2.293621 2.554985 2.286114 2.764493 2.721641 1.975164 \n     209      210      211      212      213      214      215      216 \n2.621503 2.362312 2.109496 2.721358 2.235086 3.655380 2.624842 3.379018 \n     217      218      219      220      221      222      223      224 \n3.394866 3.085613 2.646975 3.134633 3.516852 2.614926 3.098750 2.381989 \n     225      226      227      228      229      230      231      232 \n2.234237 2.329904 3.326486 2.806104 2.198548 2.555543 1.843378 2.626421 \n     233      234      235      236      237      238      239      240 \n2.969108 3.633812 2.926395 3.451702 2.109331 3.337014 3.583535 2.510592 \n     241      242      243      244      245      246      247      248 \n2.279946 3.574994 3.568371 3.494438 2.344338 2.853964 2.653598 2.287646 \n     249      250      251      252      253      254      255      256 \n1.727439 3.285614 3.195713 3.089801 2.021410 2.198713 3.722455 2.587592 \n     257      258      259      260      261      262      263      264 \n2.957166 2.012579 2.476296 1.905024 3.044568 2.236528 3.060250 2.744339 \n     265      266      267      268      269      270      271      272 \n3.372167 2.551190 2.035114 2.229230 2.889710 2.293338 2.280483 3.194983 \n     273      274      275      276      277      278      279      280 \n2.168031 2.906614 1.997298 2.456654 3.694273 2.974427 2.315353 2.412269 \n     281      282      283      284      285      286      287      288 \n2.412269 2.840408 1.813262 3.136047 2.169563 2.213429 3.172329 3.387731 \n     289      290      291      292      293      294      295      296 \n2.994699 3.694273 3.053116 2.750625 1.902361 3.293879 2.368569 2.604972 \n     297      298      299      300      301      302      303      304 \n2.264070 2.841978 2.213028 3.393167 2.474055 2.743609 2.715072 2.650369 \n     305      306      307      308      309      310      311      312 \n2.323171 2.296166 2.323454 2.981397 2.324020 3.128866 1.911192 3.101460 \n     313      314      315      316      317      318      319      320 \n3.442472 2.712809 1.812296 2.980549 2.648672 2.293621 1.754162 2.698093 \n     321      322      323      324      325      326      327      328 \n2.035396 1.858094 1.873093 3.292465 2.714679 2.654675 2.770550 2.816578 \n     329      330      331      332      333      334      335      336 \n2.870524 3.833239 2.519431 2.663106 2.153479 3.333502 2.677657 3.084929 \n     337      338      339      340      341      342      343      344 \n1.932759 2.622068 3.130846 2.350935 3.251420 3.247232 3.119068 3.533219 \n     345      346      347      348      349      350      351      352 \n3.032798 2.066078 2.056116 1.937914 2.502736 2.532286 2.535225 2.323737 \n     353      354      355      356      357      358      359      360 \n2.168715 1.879945 2.735627 1.841846 3.208285 2.536757 2.316037 3.458437 \n     361      362      363      364      365      366      367      368 \n3.449323 2.803723 2.685521 2.535983 2.417782 3.795424 2.804124 3.305484 \n     369      370      371      372      373      374      375      376 \n2.293220 2.612270 3.204261 3.513631 3.242196 3.762652 2.324302 2.677374 \n     377      378      379      380      381      382      383      384 \n3.186599 2.588322 2.665769 2.049948 2.950225 2.920675 2.294752 2.505564 \n     385      386      387      388      389      390      391      392 \n2.338764 2.640972 2.785384 2.611422 2.308648 2.782438 1.815525 3.384620 \n     393      394      395      396      397      398      399      400 \n3.414171 3.631826 3.395423 2.853992 2.489315 2.647942 2.331036 2.516202 \n     401      402      403      404      405      406      407      408 \n3.538422 2.521357 3.584337 3.285614 3.166467 2.543891 2.630892 3.103841 \n     409      410      411      412      413      414      415      416 \n2.535507 1.966781 2.264636 3.041064 3.735081 2.209634 2.914478 3.252670 \n     417      418      419      420      421      422      423      424 \n3.313750 2.500190 2.805255 2.739295 3.084764 2.516038 3.761521 3.473719 \n     425      426      427      428      429      430      431      432 \n1.991413 1.855996 2.702171 1.950815 1.754162 3.423850 3.112217 3.666136 \n     433      434      435      436      437      438      439      440 \n2.269984 2.772877 2.499507 2.263788 2.621785 3.640255 2.763243 2.293338 \n     441      442      443      444      445      446      447      448 \n3.710284 3.680734 2.264070 2.530024 2.507206 2.613237 2.832797 2.323171 \n     449      450      451      452      453      454      455      456 \n2.823594 1.900664 2.442504 2.880596 2.384251 2.469391 2.682300 3.074476 \n     457      458      459      460      461      462      463      464 \n2.413236 1.961179 3.117426 2.947397 2.663507 3.159831 2.650487 3.247350 \n     465      466      467      468      469      470      471      472 \n3.299363 2.443069 2.726960 2.622917 2.543207 2.713375 2.733410 2.822061 \n     473      474      475      476      477      478      479      480 \n3.576125 2.603840 3.275533 1.769160 3.675698 2.661856 3.621186 3.502702 \n     481      482      483      484      485      486      487      488 \n3.463756 3.522857 2.846575 3.155916 3.306497 3.847508 3.465289 2.640407 \n     489      490      491      492      493      494      495      496 \n3.120482 2.523337 3.222325 2.907938 2.548416 2.300071 2.111877 2.444766 \n     497      498      499      500      501      502      503      504 \n2.386370 2.925547 3.201032 2.699225 2.647706 2.682858 2.262001 2.744740 \n     505      506      507      508      509      510      511      512 \n2.736876 3.430018 1.878813 2.618556 2.235368 3.195430 4.000461 3.342899 \n     513      514      515      516      517      518      519      520 \n2.108648 2.388722 2.588841 2.602472 2.324302 2.013993 2.252777 2.137797 \n     521      522      523      524      525      526      527      528 \n4.050211 2.265202 2.193959 2.789855 2.774008 3.383772 3.912494 3.593615 \n     529      530      531      532      533      534      535      536 \n2.737606 2.811140 2.781590 2.711851 2.677657 2.214113 2.797668 2.636457 \n     537      538      539      540      541      542      543      544 \n2.565862 1.814514 2.750062 2.196542 2.947235 3.170708 3.142453 3.411233 \n     545      546      547      548      549      550      551      552 \n3.788811 1.739000 2.571181 3.121287 1.967585 2.127963 2.558280 3.318223 \n     553      554      555      556      557      558      559      560 \n2.711799 1.711147 2.712930 2.919664 2.552725 1.970130 3.048483 3.162888 \n     561      562      563      564      565      566      567      568 \n2.148399 2.138884 2.380978 2.993971 3.033788 3.567078 2.456610 2.722610 \n     569      570      571      572      573      574      575      576 \n3.602396 3.065580 2.559576 2.758729 2.093204 3.348174 3.259523 3.056466 \n     577      578      579      580      581      582      583      584 \n2.632544 1.827652 2.233792 2.145736 1.921393 2.014232 2.486726 2.387712 \n     585      586      587      588      589      590      591      592 \n1.746865 3.274075 2.041237 3.111607 3.099884 3.688673 3.010950 2.626660 \n     593      594      595      596      597      598      599      600 \n3.000311 2.468380 2.236337 2.186773 3.680973 3.303060 1.832852 1.681879 \n     601      602      603      604      605      606      607      608 \n2.250606 2.374410 3.623003 3.645538 3.261786 2.070788 2.705631 3.602403 \n     609      610      611      612      613      614      615      616 \n3.131249 3.072148 3.327455 2.640527 3.398609 2.270670 2.576390 3.374377 \n     617      618      619      620      621      622      623      624 \n2.196542 3.083589 2.219359 2.722885 1.887035 2.279025 2.190374 2.744460 \n     625      626      627      628      629      630      631      632 \n3.628724 3.368210 2.337278 3.562206 1.889863 3.114317 2.461135 3.648595 \n     633      634      635      636      637      638      639      640 \n3.648595 2.739821 1.939445 2.658304 2.688986 2.475966 2.940781 2.894113 \n     641      642      643      644      645      646      647      648 \n2.244911 2.726236 3.453182 2.489498 2.051308 2.490063 2.399909 2.578714 \n     649      650      651      652      653      654      655      656 \n2.573842 2.460230 2.665668 2.902683 2.310781 2.310781 3.467332 2.422296 \n     657      658      659      660      661      662      663      664 \n3.626459 3.338893 2.580411 2.547750 2.122469 3.309508 2.918083 2.728163 \n     665      666      667      668      669      670      671      672 \n2.648736 2.549164 3.052808 2.786698 2.609396 3.211349 2.738920 2.705911 \n     673      674      675      676      677      678      679      680 \n2.705911 1.979023 2.518199 3.237789 2.400564 2.371013 1.830706 2.575768 \n     681      682      683      684      685      686      687      688 \n2.007041 2.357622 2.383496 2.023007 2.072199 2.320207 2.547750 2.729011 \n     689      690      691      692      693      694      695      696 \n2.783587 2.742030 2.830681 3.046923 2.791452 2.340615 2.340615 3.103643 \n     697      698      699      700      701      702      703      704 \n2.767220 2.832944 2.714742 1.981286 3.382476 2.792300 3.047206 3.387348 \n     705      706      707      708      709      710      711      712 \n2.150605 2.009705 3.241465 2.172738 2.547184 2.374263 2.404097 2.311064 \n     713      714      715      716      717      718      719      720 \n2.347377 2.842852 2.210554 2.763315 2.799316 2.755734 3.295074 2.365605 \n     721      722      723      724      725      726      727      728 \n2.336054 2.163058 2.163058 2.688933 2.271745 2.501833 2.451964 2.835600 \n     729      730      731      732      733      734      735      736 \n3.125776 2.519048 2.276105 3.072375 2.008856 2.542312 2.067109 2.095975 \n     737      738      739      740      741      742      743      744 \n2.202288 1.847119 2.895667 2.121903 2.052274 2.794845 2.706194 2.385357 \n     745      746      747      748      749      750      751      752 \n2.821850 2.298074 2.209706 2.127058 2.681680 2.918083 2.942589 2.431245 \n     753      754      755      756      757      758      759      760 \n2.401695 2.097225 2.294845 2.176643 2.858982 2.260705 2.310781 3.155588 \n     761      762      763      764      765      766      767      768 \n2.341775 2.041235 1.949473 1.860822 2.705063 2.823264 1.889806 2.312224 \n     769      770      771      772      773      774      775      776 \n1.891503 2.764164 2.781371 2.840472 2.333108 2.067674 1.963340 1.904240 \n     777      778      779      780      781      782      783      784 \n2.669062 2.639512 2.609113 2.550012 2.511065 1.922468 2.389663 1.949473 \n     785      786      787      788      789      790      791      792 \n2.038124 2.609961 2.639512 2.451116 2.340897 1.992891 2.315335 2.690064 \n     793      794      795      796      797      798      799      800 \n2.601413 2.772429 2.282957 1.987453 2.067109 2.096659 1.801155 2.310781 \n     801      802      803      804      805      806      807      808 \n2.303676 2.554483 2.833903 2.121055 2.662776 2.667648 2.209304 2.541628 \n     809      810      811      812      813      814      815      816 \n2.776900 2.639512 2.805290 1.978458 2.669062 2.584316 2.341209 2.791734 \n     817      818      819      820      821      822      823      824 \n1.889806 2.547184 2.296660 2.616412 2.586861 2.791452 2.606331 2.250743 \n     825      826      827      828      829      830      831      832 \n2.546783 2.428582 2.828136 2.382812 2.399715 1.830989 2.251427 2.111375 \n     833      834      835      836      837      838      839      840 \n2.037841 2.725900 2.696350 2.133225 2.456911 2.205793 2.200026 2.068240 \n     841      842      843      844      845      846      847      848 \n2.577300 2.187572 2.341463 2.966820 2.008008 2.669062 2.550861 3.345061 \n     849      850      851      852      853      854      855      856 \n1.992891 2.068240 3.270278 1.837156 2.577017 2.386889 2.772712 2.518199 \n     857      858      859      860      861      862      863      864 \n1.905088 2.733765 2.218820 3.147441 2.319212 3.384008 2.755050 2.725499 \n     865      866      867      868      869      870      871      872 \n2.117260 2.420033 2.009304 2.865834 2.156608 3.077486 3.268470 3.120719 \n     873      874      875      876      877      878      879      880 \n1.852958 3.693107 4.016517 3.241430 3.981074 2.242049 3.928832 2.829569 \n     881      882      883      884      885      886      887      888 \n2.605455 2.235480 2.867094 3.911854 3.598853 3.571119 3.541569 3.931771 \n     889      890      891      892      893      894      895      896 \n3.392686 3.312419 2.802793 3.467750 3.614820 3.936815 3.525045 3.542983 \n     897      898      899      900      901      902      903      904 \n3.806271 2.094297 4.022520 3.937656 3.885414 2.794720 3.797329 3.413695 \n     905      906      907      908      909      910 \n4.029089 2.930391 3.583855 3.813122 4.028688 2.956438 \n\n# IV Regression 1980\n\niv1980 &lt;- ivreg(LogWages ~  SPEAKENG_new + Experience_Squared  + Gender  |    Experience_Squared   +\n                  Gender + AAA + Hispanic_Population +\n                  ELDCH + YNGCH + Years_Education , data = Cities_1980)\nsummary(iv1980)\n\n\nCall:\nivreg(formula = LogWages ~ SPEAKENG_new + Experience_Squared + \n    Gender | Experience_Squared + Gender + AAA + Hispanic_Population + \n    ELDCH + YNGCH + Years_Education, data = Cities_1980)\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.1518 -0.4208  0.1368  0.5918  2.7804 \nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        11.82506    0.20959  56.421  &lt; 2e-16 ***\nSPEAKENG_new       -0.64873    0.09515  -6.818 1.68e-11 ***\nExperience_Squared  0.05318    0.01501   3.542 0.000417 ***\nGender             -0.91953    0.06671 -13.784  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nResidual standard error: 0.9782 on 906 degrees of freedom\nMultiple R-Squared: 0.1457, Adjusted R-squared: 0.1429 \nWald test:  97.2 on 3 and 906 DF,  p-value: &lt; 2.2e-16 \n\nEducation1980 &lt;- lm(LogWages ~ Years_Education + Experience + Experience_Squared + AGE + Gender, data = Cities_1980)\nsummary(Education1980)\n\n\nCall:\nlm(formula = LogWages ~ Years_Education + Experience + Experience_Squared + \n    AGE + Gender, data = Cities_1980)\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.0296 -0.2925  0.1588  0.5471  2.0473 \nCoefficients: (1 not defined because of singularities)\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         8.97043    0.22179  40.445  &lt; 2e-16 ***\nYears_Education     0.08584    0.01136   7.553 1.04e-13 ***\nExperience          0.02766    0.01542   1.793   0.0733 .  \nExperience_Squared -0.03554    0.03910  -0.909   0.3636    \nAGE                      NA         NA      NA       NA    \nGender             -0.97887    0.06013 -16.279  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nResidual standard error: 0.8992 on 905 degrees of freedom\nMultiple R-squared:  0.2789,    Adjusted R-squared:  0.2757 \nF-statistic: 87.51 on 4 and 905 DF,  p-value: &lt; 2.2e-16\n\nUsed_variables &lt;- Cities_1980 %&gt;% \n  select(AGE, SPEAKENG_new, Experience_Squared, Experience, AAA, Years_Education)\n\n\n\ncor(Used_variables, use=\"complete.obs\", method=\"pearson\")\n\n                          AGE SPEAKENG_new Experience_Squared Experience\nAGE                 1.0000000    0.2763966          0.8771535  0.9187336\nSPEAKENG_new        0.2763966    1.0000000          0.4502289  0.4496153\nExperience_Squared  0.8771535    0.4502289          1.0000000  0.9728432\nExperience          0.9187336    0.4496153          0.9728432  1.0000000\nAAA                 0.6795759    0.2758524          0.5686798  0.6247809\nYears_Education    -0.2643848   -0.5509890         -0.6397067 -0.6237263\n                          AAA Years_Education\nAGE                 0.6795759      -0.2643848\nSPEAKENG_new        0.2758524      -0.5509890\nExperience_Squared  0.5686798      -0.6397067\nExperience          0.6247809      -0.6237263\nAAA                 1.0000000      -0.1807239\nYears_Education    -0.1807239       1.0000000\n\n?cor\n\nclass(Cities_1980$COUNTY_POPULATION)\n\nWarning: Unknown or uninitialised column: `COUNTY_POPULATION`.\n\n\n[1] \"NULL\"\n\n#Creating Population Concentration variable\n\nCounties1980 &lt;- Cities_1980 %&gt;%\n  filter(COUNTYFIP != 0) %&gt;% \n  group_by(STATEFIP, COUNTYFIP) %&gt;% \n  summarise(n = n()) %&gt;% \n  arrange(-n)\n\n`summarise()` has grouped output by 'STATEFIP'. You can override using the\n`.groups` argument.\n\n# County Codes\n# 37 = Los Angeles, California, 27.6% \n\n# 86 = Dade/Miami, Florida, 35.7%\n\n# 73 = San Diego, California, 14.8%   \n\n# 81 = Queens, New York, 13.9%\n\n# 215 = Hidalgo, Texas, 81.3%\n\n# 59 = Orange County, California, 14.8%\n\n# 85 = Santa Clara County, California, 17.5%\n\n# 19 = fresno county, California, 29.3%\n\n# 57 = kings County, New york, 17.6% \n\n# 17 = hudson County, New Jersey, 26.1%\n\n\n\nmutate(Cities_1980, Hispanic_Population = \n         if_else(COUNTYFIP == 37, 27.6, \n                 if_else(COUNTYFIP == 86, 35.7, \n                         if_else(COUNTYFIP == 73, 14.8,\n                                 if_else(COUNTYFIP == 81, 13.9,\n                                         if_else(COUNTYFIP == 215, 81.3, \n                                                 \n                                                 if_else(COUNTYFIP == 59, 14.8,\n                                                         if_else(COUNTYFIP == 85, 17.5,\n                                                                 if_else(COUNTYFIP == 19, 29.3,\n                                                                         if_else(COUNTYFIP == 57, 17.6,\n                                                                                 if_else(COUNTYFIP == 17, 26.1, 0)))))))))))\n\n# A tibble: 910 × 54\n    YEAR SAMPLE           SERIAL CBSERIAL  HHWT  CLUSTER CPI99 STATEICP STATEFIP\n   &lt;int&gt; &lt;int+lbl&gt;         &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;int+lb&gt; &lt;int+lb&gt;\n 1  1980 198002 [1980 1%]   2475       NA   100  1.98e12  2.30 61 [Ari… 4 [Ariz…\n 2  1980 198002 [1980 1%]   2475       NA   100  1.98e12  2.30 61 [Ari… 4 [Ariz…\n 3  1980 198002 [1980 1%]   2718       NA   100  1.98e12  2.30 61 [Ari… 4 [Ariz…\n 4  1980 198002 [1980 1%]   2869       NA   100  1.98e12  2.30 61 [Ari… 4 [Ariz…\n 5  1980 198002 [1980 1%]   3197       NA   100  1.98e12  2.30 61 [Ari… 4 [Ariz…\n 6  1980 198002 [1980 1%]   3206       NA   100  1.98e12  2.30 61 [Ari… 4 [Ariz…\n 7  1980 198002 [1980 1%]   3519       NA   100  1.98e12  2.30 61 [Ari… 4 [Ariz…\n 8  1980 198002 [1980 1%]   3775       NA   100  1.98e12  2.30 61 [Ari… 4 [Ariz…\n 9  1980 198002 [1980 1%]   3811       NA   100  1.98e12  2.30 61 [Ari… 4 [Ariz…\n10  1980 198002 [1980 1%]   3813       NA   100  1.98e12  2.30 61 [Ari… 4 [Ariz…\n# ℹ 900 more rows\n# ℹ 45 more variables: COUNTYFIP &lt;dbl+lbl&gt;, CITY &lt;int+lbl&gt;, STRATA &lt;dbl&gt;,\n#   GQ &lt;int+lbl&gt;, PERNUM &lt;dbl&gt;, PERWT &lt;dbl&gt;, ELDCH &lt;int+lbl&gt;, YNGCH &lt;int+lbl&gt;,\n#   SEX &lt;int+lbl&gt;, AGE &lt;int+lbl&gt;, MARST &lt;int+lbl&gt;, RACE &lt;int+lbl&gt;,\n#   RACED &lt;int+lbl&gt;, HISPAN &lt;int+lbl&gt;, HISPAND &lt;int+lbl&gt;, BPL &lt;int+lbl&gt;,\n#   BPLD &lt;int+lbl&gt;, CITIZEN &lt;int+lbl&gt;, YRIMMIG &lt;int+lbl&gt;, LANGUAGE &lt;int+lbl&gt;,\n#   LANGUAGED &lt;int+lbl&gt;, SPEAKENG &lt;int+lbl&gt;, EDUC &lt;int+lbl&gt;, EDUCD &lt;int+lbl&gt;, …\n\nview(head(Cities_1980, 10))\n\n\nENG80 &lt;- lm(SPEAKENG_new ~ AAA + Citizenship_Status + ELDCH + YNGCH + Hispanic_Population + Citizenship_Status + Years_Education,\n            data = Cities_1980)\nsummary(ENG80)\n\n\nCall:\nlm(formula = SPEAKENG_new ~ AAA + Citizenship_Status + ELDCH + \n    YNGCH + Hispanic_Population + Citizenship_Status + Years_Education, \n    data = Cities_1980)\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.16632 -0.45188 -0.04948  0.40844  2.69892 \nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          3.531428   0.131826  26.789  &lt; 2e-16 ***\nAAA                  0.030225   0.005269   5.736 1.32e-08 ***\nCitizenship_Status  -0.202931   0.049017  -4.140 3.80e-05 ***\nELDCH                0.021707   0.005897   3.681 0.000246 ***\nYNGCH               -0.019993   0.006302  -3.172 0.001563 ** \nHispanic_Population  0.006846   0.001841   3.719 0.000212 ***\nYears_Education     -0.119252   0.007463 -15.980  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nResidual standard error: 0.7066 on 903 degrees of freedom\nMultiple R-squared:  0.3652,    Adjusted R-squared:  0.361 \nF-statistic: 86.59 on 6 and 903 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(AER)\n\n# SPEAKENG\n\ndata$SPEAKENG_numeric &lt;- as.numeric(data$SPEAKENG)\ndata$SPEAKENG_new &lt;- dplyr::recode(data$SPEAKENG_numeric, `3` = 1, `4` = 2, `5` = 3, `6` = 4, `1` = 5)\n\n#EDUC\n\ndata$EDUC_Grade &lt;- case_when(data$EDUC %in% c(0:1) ~ \"Elementary\",\n                             data$EDUC %in% c(2) ~ \"Middle\",\n                             data$EDUC %in% c(3:6) ~ \"High_School\",\n                             data$EDUC %in% c(7:10) ~ \"College\",\n                             data$EDUC %in% c(11) ~ \"Graduate_Degree\")\n\n#Age At Arrival Variable \n\ndata &lt;- mutate(data, AAA = AGE - (YEAR - YRIMMIG))\n\n#Inflation Adjusted Income Variable \n\ndata &lt;- filter(data, INCTOT &gt; 0) \ndata &lt;- mutate(data, Inflation_Adjusted = INCTOT*CPI99)\n\n#Log wages\ndata &lt;- mutate(data, LogWages = log(Inflation_Adjusted))\n\n\n\n# 1990 --------------------------------------------------------------------\nlibrary(tidyverse)\n# SPEAKENG\n\ndata$SPEAKENG_numeric &lt;- as.numeric(data$SPEAKENG)\ndata$SPEAKENG_new &lt;- dplyr::recode(data$SPEAKENG_numeric, `3` = 1, `4` = 2, `5` = 3, `6` = 4, `1` = 5)\n\n#EDUC\n\ndata$EDUC_Grade &lt;- case_when(data$EDUC %in% c(0:1) ~ \"Elementary\",\n                             data$EDUC %in% c(2) ~ \"Middle\",\n                             data$EDUC %in% c(3:6) ~ \"High_School\",\n                             data$EDUC %in% c(7:10) ~ \"College\",\n                             data$EDUC %in% c(11) ~ \"Graduate_Degree\")\n\n#Age At Arrival Variable \n\ndata &lt;- mutate(data, AAA = AGE - (YEAR - YRIMMIG))\n\n#Inflation Adjusted Income Variable \n\ndata &lt;- filter(data, INCTOT &gt; 0) \ndata &lt;- mutate(data, Inflation_Adjusted = INCTOT*CPI99)\n\n#Log wages\ndata &lt;- mutate(data, LogWages = log(Inflation_Adjusted))\n\n\nCities_1990 &lt;- Sample1990 &lt;- data %&gt;% \n  filter(YEAR == 1990) %&gt;% \n  filter(AAA %in% c(0:18)) %&gt;% \n  filter(YRIMMIG %in% c(1960:1974)) %&gt;% \n  mutate(Hispanic_Origin = ifelse(HISPAN %in% c(1:4), 1, 0)) %&gt;% \n  mutate(Middle = ifelse(EDUC_Grade == \"Middle\", 1, 0)) %&gt;% \n  mutate(High_School = ifelse(EDUC_Grade == \"High_School\", 1, 0)) %&gt;% \n  mutate(College = ifelse(EDUC_Grade == \"College\", 1, 0)) %&gt;% \n  mutate(Graduate_Degree = ifelse(EDUC_Grade == \"Graduate_Degree\", 1, 0)) %&gt;% \n  mutate(Gender = ifelse(SEX == 2 , 1 , 0)) %&gt;% \n  filter(Hispanic_Origin == 1) %&gt;% \n  mutate(Citizenship_Status = ifelse(CITIZEN == 2, 1, 0)) %&gt;% \n  filter(Graduate_Degree != 1) %&gt;% \n  filter(ELDCH != 99 |\n           YNGCH!= 99) %&gt;% \n  filter(STATEFIP %in% c(6, 12, 17, 36)|\n           COUNTYFIP %in% c(37, 86, 31, 59, 5, 48)) %&gt;% \n  mutate(YSM = (AGE - AAA)) %&gt;% \n  filter(STATEFIP != 0 |\n           COUNTYFIP != 0)%&gt;% \n  mutate(Experience_Squared = ifelse(EDUC_Grade == \"Elementary\", ((AGE - 5 - 5)^2)/100, \n                                     ifelse(EDUC_Grade == \"Middle\", ((AGE - 8 - 5)^2)/100, \n                                            ifelse(EDUC_Grade == \"High_School\", ((AGE - 12 - 5)^2)/100, \n                                                   ifelse(EDUC_Grade == \"College\", ((AGE - 16 - 5)^2)/100, 0))))) %&gt;% \n  mutate(Experience = ifelse(EDUC_Grade == \"Elementary\", AGE - 5 - 5, \n                             ifelse(EDUC_Grade == \"Middle\", AGE - 8 - 5, \n                                    ifelse(EDUC_Grade == \"High_School\", AGE - 12 - 5, \n                                           ifelse(EDUC_Grade == \"College\", AGE - 16 - 5, 0))))) %&gt;% \n  mutate(Years_Education = ifelse(EDUC_Grade == \"Elementary\", 5, \n                                  ifelse(EDUC_Grade == \"Middle\", 8,\n                                         ifelse(EDUC_Grade == \"High_School\", 12,\n                                                ifelse(EDUC_Grade == \"College\", 16, 0))))) %&gt;% \n  mutate(Education_Dummy = ifelse(EDUC_Grade %in% c(\"Elementary\", \"Middle\"), 0, 1)) %&gt;% \n  filter(INCTOT != 9999999 | INCTOT != 9999998) %&gt;% \n  mutate( Hispanic_Population = \n            if_else(COUNTYFIP == 37, 37.8 , \n                    if_else(COUNTYFIP == 86, 49.2 , \n                            if_else(COUNTYFIP == 31, 13.6 ,\n                                    if_else(COUNTYFIP == 59, 23.4 ,\n                                            if_else(COUNTYFIP == 5, 43.5,\n                                                    if_else(COUNTYFIP == 47, 20.1,\n                                                            if_else(COUNTYFIP == 73, 20.4,\n                                                                    if_else(COUNTYFIP == 81, 19.5,\n                                                                            if_else(COUNTYFIP == 61, 26.0,\n                                                                                    if_else(COUNTYFIP == 141, 69.6, 0))))))))))) %&gt;% \n  filter(Hispanic_Population != 0)\n\n\n\n\n#1990 OLS \nOLS1990 &lt;- lm(LogWages ~ AGE + Gender  +  SPEAKENG_new + Citizenship_Status + Experience_Squared, data = Cities_1990)\nsummary(OLS1990)\n\n\nCall:\nlm(formula = LogWages ~ AGE + Gender + SPEAKENG_new + Citizenship_Status + \n    Experience_Squared, data = Cities_1990)\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.7612 -0.3327  0.1675  0.5374  2.8682 \nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         9.12540    0.16636  54.854  &lt; 2e-16 ***\nAGE                 0.04991    0.00525   9.507  &lt; 2e-16 ***\nGender             -0.76589    0.03214 -23.831  &lt; 2e-16 ***\nSPEAKENG_new       -0.13018    0.01929  -6.750 1.76e-11 ***\nCitizenship_Status  0.20821    0.03422   6.084 1.31e-09 ***\nExperience_Squared -0.09769    0.01171  -8.341  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nResidual standard error: 0.8876 on 3071 degrees of freedom\nMultiple R-squared:  0.2203,    Adjusted R-squared:  0.219 \nF-statistic: 173.5 on 5 and 3071 DF,  p-value: &lt; 2.2e-16\n\n#1990 OLS on SPEAKENG\nENG1990 &lt;- lm(SPEAKENG_new ~ AAA + ELDCH + YNGCH + \n                Hispanic_Population + Years_Education,\n              data = Cities_1990)\nsummary(ENG1990)\n\n\nCall:\nlm(formula = SPEAKENG_new ~ AAA + ELDCH + YNGCH + Hispanic_Population + \n    Years_Education, data = Cities_1990)\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.67283 -0.47408 -0.09447  0.49429  3.10204 \nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          3.5731076  0.0818509  43.654  &lt; 2e-16 ***\nAAA                  0.0370415  0.0033118  11.185  &lt; 2e-16 ***\nELDCH                0.0074700  0.0033038   2.261   0.0238 *  \nYNGCH               -0.0135626  0.0033779  -4.015 6.08e-05 ***\nHispanic_Population -0.0004506  0.0012241  -0.368   0.7128    \nYears_Education     -0.1084347  0.0043794 -24.760  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nResidual standard error: 0.7894 on 3071 degrees of freedom\nMultiple R-squared:  0.2737,    Adjusted R-squared:  0.2725 \nF-statistic: 231.4 on 5 and 3071 DF,  p-value: &lt; 2.2e-16\n\n#1990 IV \niv1990 &lt;- ivreg(LogWages ~  SPEAKENG_new + Experience_Squared + Gender|Experience_Squared   +\n                  Gender + AAA + Hispanic_Population +\n                  ELDCH + YNGCH + Years_Education , data = Cities_1990)\n\nsummary(iv1990)\n\n\nCall:\nivreg(formula = LogWages ~ SPEAKENG_new + Experience_Squared + \n    Gender | Experience_Squared + Gender + AAA + Hispanic_Population + \n    ELDCH + YNGCH + Years_Education, data = Cities_1990)\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.0700 -0.4207  0.1362  0.6159  3.4617 \nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        11.570098   0.106056 109.094  &lt; 2e-16 ***\nSPEAKENG_new       -0.555160   0.045156 -12.294  &lt; 2e-16 ***\nExperience_Squared  0.035691   0.008004   4.459 8.53e-06 ***\nGender             -0.733319   0.034858 -21.037  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nResidual standard error: 0.9597 on 3073 degrees of freedom\nMultiple R-Squared: 0.08787,    Adjusted R-squared: 0.08698 \nWald test: 218.8 on 3 and 3073 DF,  p-value: &lt; 2.2e-16 \n\n#Creating Population variable \n\nCounties_1990 &lt;- Cities_1990 %&gt;% \n  filter(COUNTYFIP != 0) %&gt;% \n  group_by(STATEFIP, COUNTYFIP) %&gt;% \n  summarise(n=n()) %&gt;% \n  arrange(-n)\n\n`summarise()` has grouped output by 'STATEFIP'. You can override using the\n`.groups` argument.\n\n# County codes\n\n# 37 = Los Angeles, California, 37.8\n\n# 86 = Miami/Dade county, 49.2\n\n# 31 = Cook County, Illinois\n\n# 59 = Orange County, California \n\n# 5 = Bronx County, New York\n\n\nCities_1990 &lt;- mutate(Cities_1990, Hispanic_Population = \n                        if_else(COUNTYFIP == 37, 37.8 , \n                                if_else(COUNTYFIP == 86, 49.2 , \n                                        if_else(COUNTYFIP == 31, 13.6 ,\n                                                if_else(COUNTYFIP == 59, 23.4 ,\n                                                        if_else(COUNTYFIP == 5, 43.5,\n                                                                if_else(COUNTYFIP == 47, 20.1,\n                                                                        if_else(COUNTYFIP == 73, 20.4,\n                                                                                if_else(COUNTYFIP == 81, 19.5,\n                                                                                        if_else(COUNTYFIP == 61, 26.0,\n                                                                                                if_else(COUNTYFIP == 141, 69.6, 0)))))))))))\n\nview(head(Cities_1990))\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(AER)\n\n# SPEAKENG\n\ndata$SPEAKENG_numeric &lt;- as.numeric(data$SPEAKENG)\ndata$SPEAKENG_new &lt;- dplyr::recode(data$SPEAKENG_numeric, `3` = 1, `4` = 2, `5` = 3, `6` = 4, `1` = 5)\n\n#EDUC\n\ndata$EDUC_Grade &lt;- case_when(data$EDUC %in% c(0:1) ~ \"Elementary\",\n                             data$EDUC %in% c(2) ~ \"Middle\",\n                             data$EDUC %in% c(3:6) ~ \"High_School\",\n                             data$EDUC %in% c(7:10) ~ \"College\",\n                             data$EDUC %in% c(11) ~ \"Graduate_Degree\")\n\n#Age At Arrival Variable \n\ndata &lt;- mutate(data, AAA = AGE - (YEAR - YRIMMIG))\n\n#Inflation Adjusted Income Variable \n\ndata &lt;- filter(data, INCTOT &gt; 0) \ndata &lt;- mutate(data, Inflation_Adjusted = INCTOT*CPI99)\n\n#Log wages\ndata &lt;- mutate(data, LogWages = log(Inflation_Adjusted))\n\n\n\n# 2000 --------------------------------------------------------------------\n\nlibrary(tidyverse)\n# SPEAKENG\n\ndata$SPEAKENG_numeric &lt;- as.numeric(data$SPEAKENG)\ndata$SPEAKENG_new &lt;- dplyr::recode(data$SPEAKENG_numeric, `3` = 1, `4` = 2, `5` = 3, `6` = 4, `1` = 5)\n\n#EDUC\n\ndata$EDUC_Grade &lt;- case_when(data$EDUC %in% c(0:1) ~ \"Elementary\",\n                             data$EDUC %in% c(2) ~ \"Middle\",\n                             data$EDUC %in% c(3:6) ~ \"High_School\",\n                             data$EDUC %in% c(7:10) ~ \"College\",\n                             data$EDUC %in% c(11) ~ \"Graduate_Degree\")\n\n#Age At Arrival Variable \n\ndata &lt;- mutate(data, AAA = AGE - (YEAR - YRIMMIG))\n\n#Inflation Adjusted Income Variable \n\ndata &lt;- filter(data, INCTOT &gt; 0) \ndata &lt;- mutate(data, Inflation_Adjusted = INCTOT*CPI99)\n\n#Log wages\ndata &lt;- mutate(data, LogWages = log(Inflation_Adjusted))\n\n\n\nCities_2000 &lt;- data %&gt;% \n  filter(YEAR == 2000) %&gt;% \n  filter(AAA %in% c(0:18)) %&gt;% \n  filter(YRIMMIG %in% c(1970:1984)) %&gt;% \n  mutate(Hispanic_Origin = ifelse(HISPAN %in% c(1:4), 1, 0)) %&gt;% \n  mutate(Middle = ifelse(EDUC_Grade == \"Middle\", 1, 0)) %&gt;% \n  mutate(High_School = ifelse(EDUC_Grade == \"High_School\", 1, 0)) %&gt;% \n  mutate(College = ifelse(EDUC_Grade == \"College\", 1, 0)) %&gt;% \n  mutate(Graduate_Degree = ifelse(EDUC_Grade == \"Graduate_Degree\", 1, 0)) %&gt;% \n  mutate(Gender = ifelse(SEX == 2 , 1 , 0)) %&gt;% \n  filter(Hispanic_Origin == 1) %&gt;% \n  mutate(Citizenship_Status = ifelse(CITIZEN == 2, 1, 0)) %&gt;% \n  filter(Graduate_Degree != 1) %&gt;% \n  filter(ELDCH != 99 |\n           YNGCH!= 99) %&gt;% \n  filter(STATEFIP %in% c(6, 48, 17, 4, 36) |\n           COUNTYFIP %in% c(37, 59, 201, 31, 73, 71, 113, 13, 81, 5)) %&gt;% \n  mutate(YSM = (AGE - AAA)) %&gt;% \n  filter(STATEFIP != 0 |\n           COUNTYFIP != 0) %&gt;% \n  mutate(Experience = ifelse(EDUC_Grade == \"Elementary\", AGE - 5 - 5, \n                             ifelse(EDUC_Grade == \"Middle\", AGE - 8 - 5, \n                                    ifelse(EDUC_Grade == \"High_School\", AGE - 12 - 5, \n                                           ifelse(EDUC_Grade == \"College\", AGE - 16 - 5, 0))))) %&gt;% \n  mutate(Experience_Squared = ifelse(EDUC_Grade == \"Elementary\", ((AGE - 5 - 5)^2)/100, \n                                     ifelse(EDUC_Grade == \"Middle\", ((AGE - 8 - 5)^2)/100, \n                                            ifelse(EDUC_Grade == \"High_School\", ((AGE - 12 - 5)^2)/100, \n                                                   ifelse(EDUC_Grade == \"College\", ((AGE - 16 - 5)^2)/100, 0))))) %&gt;% \n  mutate(Years_Education = ifelse(EDUC_Grade == \"Elementary\", 5, \n                                  ifelse(EDUC_Grade == \"Middle\", 8,\n                                         ifelse(EDUC_Grade == \"High_School\", 12,\n                                                ifelse(EDUC_Grade == \"College\", 16, 0)))))%&gt;% \n  mutate(Education_Dummy = ifelse(EDUC_Grade %in% c(\"Elementary\", \"Middle\"), 0, 1)) %&gt;% \n  filter(INCTOT != 9999999 | INCTOT != 9999998) %&gt;% \n  mutate(Hispanic_Population = \n           if_else(COUNTYFIP == 37, 44.6, \n                   if_else(COUNTYFIP == 59, 30.8, \n                           if_else(COUNTYFIP == 201, 32.9,\n                                   if_else(COUNTYFIP == 31, 19.9,\n                                           if_else(COUNTYFIP == 73, 26.7, \n                                                   if_else(COUNTYFIP == 71,39.2 , \n                                                           if_else(COUNTYFIP == 113,29.9 , \n                                                                   if_else(COUNTYFIP == 13, 24.8, \n                                                                           if_else(COUNTYFIP == 81,25.0 , \n                                                                                   if_else(COUNTYFIP == 5,48.4 ,0))))))))))) %&gt;% \n  filter(Hispanic_Population != 0)\n\n\n\n#  filter(COUNTYFIP %in% c(37, 59, 31, 201, 73))\n\n\n#Creating Population variable\nCounties2000 &lt;- Cities_2000 %&gt;% \n  filter(COUNTYFIP != 0 ) %&gt;% \n  group_by(STATEFIP, COUNTYFIP) %&gt;% \n  summarise(n=n()) %&gt;% \n  arrange(-n)\n\n`summarise()` has grouped output by 'STATEFIP'. You can override using the\n`.groups` argument.\n\n# 37 = Los Angeles, 44.6\n\n# 59 = Daviess County, Kentucky, 0.9\n\n# 31 = Whitefeld, Georgia, 22.1\n\n# 201 = Winnebago, Illinois, 6.9\n\n# Colombia, Georgia, 2.6\n\nCities_2000 &lt;- mutate(Cities_2000, Hispanic_Population = \n                        if_else(COUNTYFIP == 37, 44.6, \n                                if_else(COUNTYFIP == 59, 30.8, \n                                        if_else(COUNTYFIP == 201, 32.9,\n                                                if_else(COUNTYFIP == 31, 19.9,\n                                                        if_else(COUNTYFIP == 73, 26.7, \n                                                                if_else(COUNTYFIP == 71,39.2 , \n                                                                        if_else(COUNTYFIP == 113,29.9 , \n                                                                                if_else(COUNTYFIP == 13, 24.8, \n                                                                                        if_else(COUNTYFIP == 81,25.0 , \n                                                                                                if_else(COUNTYFIP == 5,48.4 ,0)))))))))))                  \n\n\n\n\n#OLS 2000\nOLS2000 &lt;- lm(LogWages ~ AGE + Gender  + SPEAKENG_new + Citizenship_Status + Experience_Squared, data = Cities_2000)\nsummary(OLS2000)\n\n\nCall:\nlm(formula = LogWages ~ AGE + Gender + SPEAKENG_new + Citizenship_Status + \n    Experience_Squared, data = Cities_2000)\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.6268 -0.3163  0.1171  0.5050  3.6166 \nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         8.924337   0.132088  67.564  &lt; 2e-16 ***\nAGE                 0.048381   0.004496  10.762  &lt; 2e-16 ***\nGender             -0.685819   0.026174 -26.202  &lt; 2e-16 ***\nSPEAKENG_new       -0.090270   0.014617  -6.176 7.13e-10 ***\nCitizenship_Status  0.211848   0.026951   7.861 4.69e-15 ***\nExperience_Squared -0.090454   0.009809  -9.221  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nResidual standard error: 0.8875 on 4777 degrees of freedom\nMultiple R-squared:  0.1699,    Adjusted R-squared:  0.1691 \nF-statistic: 195.6 on 5 and 4777 DF,  p-value: &lt; 2.2e-16\n\n#OlS on SPEAKEng\n\nENG2000 &lt;- lm(SPEAKENG_new ~ AAA  + ELDCH + YNGCH + \n                Hispanic_Population + Years_Education\n              ,\n              data = Cities_2000)\nsummary(ENG2000)\n\n\nCall:\nlm(formula = SPEAKENG_new ~ AAA + ELDCH + YNGCH + Hispanic_Population + \n    Years_Education, data = Cities_2000)\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.76757 -0.49072 -0.03791  0.50982  3.10585 \nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          3.147011   0.080610  39.040   &lt;2e-16 ***\nAAA                  0.055786   0.002692  20.726   &lt;2e-16 ***\nELDCH                0.003078   0.002764   1.114   0.2655    \nYNGCH               -0.007491   0.003094  -2.421   0.0155 *  \nHispanic_Population  0.002927   0.001307   2.239   0.0252 *  \nYears_Education     -0.101866   0.004114 -24.760   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nResidual standard error: 0.8321 on 4777 degrees of freedom\nMultiple R-squared:  0.2615,    Adjusted R-squared:  0.2607 \nF-statistic: 338.3 on 5 and 4777 DF,  p-value: &lt; 2.2e-16\n\n#2000 IV \niv2000 &lt;- ivreg(LogWages ~  SPEAKENG_new + Experience_Squared   + Gender|    Experience_Squared  + \n                  Gender + AAA + Hispanic_Population +\n                  ELDCH + YNGCH + Years_Education , data = Cities_2000)\nsummary(iv2000)\n\n\nCall:\nivreg(formula = LogWages ~ SPEAKENG_new + Experience_Squared + \n    Gender | Experience_Squared + Gender + AAA + Hispanic_Population + \n    ELDCH + YNGCH + Years_Education, data = Cities_2000)\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.7656 -0.3732  0.1232  0.5480  4.0718 \nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        11.048908   0.092693 119.199  &lt; 2e-16 ***\nSPEAKENG_new       -0.405624   0.039200 -10.347  &lt; 2e-16 ***\nExperience_Squared  0.039022   0.006983   5.588 2.42e-08 ***\nGender             -0.658475   0.027574 -23.880  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nResidual standard error: 0.94 on 4779 degrees of freedom\nMultiple R-Squared: 0.06845,    Adjusted R-squared: 0.06786 \nWald test: 233.9 on 3 and 4779 DF,  p-value: &lt; 2.2e-16 \n\n\n\n\n\n\n# SPEAKENG\n\ndata$SPEAKENG_numeric &lt;- as.numeric(data$SPEAKENG)\ndata$SPEAKENG_new &lt;- dplyr::recode(data$SPEAKENG_numeric, `3` = 1, `4` = 2, `5` = 3, `6` = 4, `1` = 5)\n\n#EDUC\n\ndata$EDUC_Grade &lt;- case_when(data$EDUC %in% c(0:1) ~ \"Elementary\",\n                             data$EDUC %in% c(2) ~ \"Middle\",\n                             data$EDUC %in% c(3:6) ~ \"High_School\",\n                             data$EDUC %in% c(7:10) ~ \"College\",\n                             data$EDUC %in% c(11) ~ \"Graduate_Degree\")\n\n#Age At Arrival Variable \n\ndata &lt;- mutate(data, AAA = AGE - (YEAR - YRIMMIG))\n\n#Inflation Adjusted Income Variable \n\ndata &lt;- filter(data, INCTOT &gt; 0) \ndata &lt;- mutate(data, Inflation_Adjusted = INCTOT*CPI99)\n\n#Log wages\ndata &lt;- mutate(data, LogWages = log(Inflation_Adjusted))\n\n\n\n# 2010 --------------------------------------------------------------------\nlibrary(tidyverse)\n# SPEAKENG\n\ndata$SPEAKENG_numeric &lt;- as.numeric(data$SPEAKENG)\ndata$SPEAKENG_new &lt;- dplyr::recode(data$SPEAKENG_numeric, `3` = 1, `4` = 2, `5` = 3, `6` = 4, `1` = 5)\n\n#EDUC\n\ndata$EDUC_Grade &lt;- case_when(data$EDUC %in% c(0:1) ~ \"Elementary\",\n                             data$EDUC %in% c(2) ~ \"Middle\",\n                             data$EDUC %in% c(3:6) ~ \"High_School\",\n                             data$EDUC %in% c(7:10) ~ \"College\",\n                             data$EDUC %in% c(11) ~ \"Graduate_Degree\")\n\n#Age At Arrival Variable \n\ndata &lt;- mutate(data, AAA = AGE - (YEAR - YRIMMIG))\n\n#Inflation Adjusted Income Variable \n\ndata &lt;- filter(data, INCTOT &gt; 0) \ndata &lt;- mutate(data, Inflation_Adjusted = INCTOT*CPI99)\n\n#Log wages\ndata &lt;- mutate(data, LogWages = log(Inflation_Adjusted))\n\n\nlibrary(tidyverse)\nlibrary(AER)\n\nCities_2010 &lt;- data %&gt;% \n  filter(YEAR == 2010) %&gt;% \n  filter(AAA %in% c(0:18)) %&gt;% \n  filter(YRIMMIG %in% c(1980:1994)) %&gt;% \n  mutate(Hispanic_Origin = ifelse(HISPAN %in% c(1:4), 1, 0)) %&gt;% \n  mutate(Middle = ifelse(EDUC_Grade == \"Middle\", 1, 0)) %&gt;% \n  mutate(High_School = ifelse(EDUC_Grade == \"High_School\", 1, 0)) %&gt;% \n  mutate(College = ifelse(EDUC_Grade == \"College\", 1, 0)) %&gt;% \n  mutate(Graduate_Degree = ifelse(EDUC_Grade == \"Graduate_Degree\", 1, 0)) %&gt;% \n  mutate(Gender = ifelse(SEX == 2 , 1 , 0)) %&gt;% \n  filter(Hispanic_Origin == 1) %&gt;% \n  mutate(Citizenship_Status = ifelse(CITIZEN == 2, 1, 0)) %&gt;% \n  filter(Graduate_Degree != 1) %&gt;% \n  filter(ELDCH != 99 |\n           YNGCH!= 99) %&gt;% \n  filter(COUNTYFIP %in% c(37, 201, 31, 59, 13)) %&gt;% \n  filter(STATEFIP %in% c(6, 48, 17, 4, 36, 24, 34,32) |\n           COUNTYFIP %in% c(37, 201, 59, 31, 13)) %&gt;% \n  mutate(YSM = (AGE - AAA)) %&gt;% \n  filter(STATEFIP != 0 |\n           COUNTYFIP != 0) %&gt;% \n  mutate(Experience = ifelse(EDUC_Grade == \"Elementary\", AGE - 5 - 5, \n                             ifelse(EDUC_Grade == \"Middle\", AGE - 8 - 5, \n                                    ifelse(EDUC_Grade == \"High_School\", AGE - 12 - 5, \n                                           ifelse(EDUC_Grade == \"College\", AGE - 16 - 5, 0))))) %&gt;% \n  mutate(Experience_Squared = ifelse(EDUC_Grade == \"Elementary\", ((AGE - 5 - 5)^2)/100, \n                                     ifelse(EDUC_Grade == \"Middle\", ((AGE - 8 - 5)^2)/100, \n                                            ifelse(EDUC_Grade == \"High_School\", ((AGE - 12 - 5)^2)/100, \n                                                   ifelse(EDUC_Grade == \"College\", ((AGE - 16 - 5)^2)/100, 0))))) %&gt;% \n  mutate(Years_Education = ifelse(EDUC_Grade == \"Elementary\", 5, \n                                  ifelse(EDUC_Grade == \"Middle\", 8,\n                                         ifelse(EDUC_Grade == \"High_School\", 12,\n                                                ifelse(EDUC_Grade == \"College\", 16, 0)))))%&gt;% \n  mutate(Education_Dummy = ifelse(EDUC_Grade %in% c(\"Elementary\", \"Middle\"), 0, 1)) %&gt;% \n  filter(INCTOT != 9999999 | INCTOT != 9999998) %&gt;% \n  mutate(Hispanic_Population = \n           if_else(COUNTYFIP == 37,47.7 , \n                   if_else(COUNTYFIP == 201, 11.1, \n                           if_else(COUNTYFIP == 59 | STATEFIP == 6, 33.7,\n                                   if_else(COUNTYFIP == 31, 24.0 ,\n                                           if_else(COUNTYFIP == 13 | STATEFIP == 4,29.6 , \n                                                   if_else(COUNTYFIP == 13 | STATEFIP == 6, 24.4,\n                                                           if_else(COUNTYFIP == 59 | STATEFIP == 36, 14.6 ,\n                                                                   if_else(COUNTYFIP == 31 | STATEFIP == 24, 17.0,\n                                                                           if_else(COUNTYFIP == 13 | STATEFIP == 34,20.3 , \n                                                                                   if_else(COUNTYFIP == 31 | STATEFIP == 32,22.2 ,0))))))))))) %&gt;% \n  filter(Hispanic_Population != 0)\n\n#Creating population variable \n\nCities_2010 %&gt;% \n  filter(COUNTYFIP != 0 ) %&gt;% \n  group_by(STATEFIP, COUNTYFIP) %&gt;% \n  summarise(n=n()) %&gt;% \n  arrange(-n)\n\n`summarise()` has grouped output by 'STATEFIP'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 23 × 3\n# Groups:   STATEFIP [16]\n   STATEFIP        COUNTYFIP     n\n   &lt;int+lbl&gt;       &lt;dbl+lbl&gt; &lt;int&gt;\n 1  6 [California]  37        1812\n 2 48 [Texas]      201         483\n 3  6 [California]  59         392\n 4 17 [Illinois]    31         311\n 5  4 [Arizona]     13         270\n 6  6 [California]  13          91\n 7 36 [New York]    59          52\n 8 24 [Maryland]    31          46\n 9 34 [New Jersey]  13          40\n10 32 [Nevada]      31          39\n# ℹ 13 more rows\n\n# 37 = Los Angeles, California, 47.7\n\n# 201 = Winnebago, Illinois, 10.9\n\n# 31 = Witefeld, Georgia, 31.6\n\n# 59 = Daviess County, Kentucky, 2.6 \n\n# 13 = Black Hawk Iowa, 3.7\n\nCities_2010 &lt;- mutate(Cities_2010, Hispanic_Population = \n                        if_else(COUNTYFIP == 37,47.7 , \n                                if_else(COUNTYFIP == 201, 11.1, \n                                        if_else(COUNTYFIP == 59 | STATEFIP == 6, 33.7,\n                                                if_else(COUNTYFIP == 31, 24.0 ,\n                                                        if_else(COUNTYFIP == 13 | STATEFIP == 4,29.6 , \n                                                                if_else(COUNTYFIP == 13 | STATEFIP == 6, 24.4,\n                                                                        if_else(COUNTYFIP == 59 | STATEFIP == 36, 14.6 ,\n                                                                                if_else(COUNTYFIP == 31 | STATEFIP == 24, 17.0,\n                                                                                        if_else(COUNTYFIP == 13 | STATEFIP == 34,20.3 , \n                                                                                                if_else(COUNTYFIP == 31 | STATEFIP == 32,22.2 ,0)))))))))))\n\n#OLS 2010\nOLS2010 &lt;- lm(LogWages ~ AGE + Gender  +   SPEAKENG_new  + Citizenship_Status + Experience_Squared, data = Cities_2010)\nsummary(OLS2010)\n\n\nCall:\nlm(formula = LogWages ~ AGE + Gender + SPEAKENG_new + Citizenship_Status + \n    Experience_Squared, data = Cities_2010)\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.1747 -0.3406  0.1192  0.5125  3.7177 \nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         8.564208   0.148643  57.616  &lt; 2e-16 ***\nAGE                 0.056858   0.004841  11.745  &lt; 2e-16 ***\nGender             -0.606501   0.028578 -21.222  &lt; 2e-16 ***\nSPEAKENG_new       -0.130377   0.015774  -8.265  &lt; 2e-16 ***\nCitizenship_Status  0.252150   0.030908   8.158 4.63e-16 ***\nExperience_Squared -0.095373   0.009965  -9.570  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nResidual standard error: 0.842 on 3640 degrees of freedom\nMultiple R-squared:  0.1911,    Adjusted R-squared:   0.19 \nF-statistic:   172 on 5 and 3640 DF,  p-value: &lt; 2.2e-16\n\n#OlS on SPEAKEng\nENG2010 &lt;- lm(SPEAKENG_new ~ AAA  + ELDCH + YNGCH + \n                Hispanic_Population + Years_Education \n              ,\n              data = Cities_2010)\nsummary(ENG2010)\n\n\nCall:\nlm(formula = SPEAKENG_new ~ AAA + ELDCH + YNGCH + Hispanic_Population + \n    Years_Education, data = Cities_2010)\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0044 -0.5890 -0.0560  0.5369  3.2370 \nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          3.414101   0.089348  38.211  &lt; 2e-16 ***\nAAA                  0.046987   0.003384  13.883  &lt; 2e-16 ***\nELDCH                0.013476   0.003180   4.238 2.31e-05 ***\nYNGCH               -0.008053   0.003299  -2.441   0.0147 *  \nHispanic_Population  0.002044   0.001082   1.888   0.0591 .  \nYears_Education     -0.110588   0.004817 -22.958  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nResidual standard error: 0.8581 on 3640 degrees of freedom\nMultiple R-squared:  0.2462,    Adjusted R-squared:  0.2452 \nF-statistic: 237.8 on 5 and 3640 DF,  p-value: &lt; 2.2e-16\n\n#IV 2010\niv2010 &lt;- ivreg(LogWages ~  SPEAKENG_new + Experience_Squared  + Gender |    Experience_Squared  + \n                  Gender + AAA + Hispanic_Population +\n                  ELDCH + YNGCH + Years_Education, data = Cities_2010)\nsummary(iv2010)\n\n\nCall:\nivreg(formula = LogWages ~ SPEAKENG_new + Experience_Squared + \n    Gender | Experience_Squared + Gender + AAA + Hispanic_Population + \n    ELDCH + YNGCH + Years_Education, data = Cities_2010)\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.0532 -0.3925  0.1141  0.5793  3.9114 \nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        11.235664   0.112564   99.82  &lt; 2e-16 ***\nSPEAKENG_new       -0.533560   0.044591  -11.97  &lt; 2e-16 ***\nExperience_Squared  0.054478   0.007402    7.36 2.26e-13 ***\nGender             -0.586243   0.031282  -18.74  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nResidual standard error: 0.9265 on 3642 degrees of freedom\nMultiple R-Squared: 0.02005,    Adjusted R-squared: 0.01925 \nWald test: 165.7 on 3 and 3642 DF,  p-value: &lt; 2.2e-16 \n\nview(head(Cities_1980, 10))\n\n\n\n\n\n# SPEAKENG\n\ndata$SPEAKENG_numeric &lt;- as.numeric(data$SPEAKENG)\ndata$SPEAKENG_new &lt;- dplyr::recode(data$SPEAKENG_numeric, `3` = 1, `4` = 2, `5` = 3, `6` = 4, `1` = 5)\n\n#EDUC\n\ndata$EDUC_Grade &lt;- case_when(data$EDUC %in% c(0:1) ~ \"Elementary\",\n                             data$EDUC %in% c(2) ~ \"Middle\",\n                             data$EDUC %in% c(3:6) ~ \"High_School\",\n                             data$EDUC %in% c(7:10) ~ \"College\",\n                             data$EDUC %in% c(11) ~ \"Graduate_Degree\")\n\n#Age At Arrival Variable \n\ndata &lt;- mutate(data, AAA = AGE - (YEAR - YRIMMIG))\n\n#Inflation Adjusted Income Variable \n\ndata &lt;- filter(data, INCTOT &gt; 0) \ndata &lt;- mutate(data, Inflation_Adjusted = INCTOT*CPI99)\n\n#Log wages\ndata &lt;- mutate(data, LogWages = log(Inflation_Adjusted))\n\n\n\n# 2019 --------------------------------------------------------------------\nlibrary(tidyverse)\n# SPEAKENG\n\ndata$SPEAKENG_numeric &lt;- as.numeric(data$SPEAKENG)\ndata$SPEAKENG_new &lt;- dplyr::recode(data$SPEAKENG_numeric, `3` = 1, `4` = 2, `5` = 3, `6` = 4, `1` = 5)\n\n#EDUC\n\ndata$EDUC_Grade &lt;- case_when(data$EDUC %in% c(0:1) ~ \"Elementary\",\n                             data$EDUC %in% c(2) ~ \"Middle\",\n                             data$EDUC %in% c(3:6) ~ \"High_School\",\n                             data$EDUC %in% c(7:10) ~ \"College\",\n                             data$EDUC %in% c(11) ~ \"Graduate_Degree\")\n\n#Age At Arrival Variable \n\ndata &lt;- mutate(data, AAA = AGE - (YEAR - YRIMMIG))\n\n#Inflation Adjusted Income Variable \n\ndata &lt;- filter(data, INCTOT &gt; 0) \ndata &lt;- mutate(data, Inflation_Adjusted = INCTOT*CPI99)\n\n#Log wages\ndata &lt;- mutate(data, LogWages = log(Inflation_Adjusted))\n\n\nlibrary(tidyverse)\nlibrary(AER)\n\nCities_2019 &lt;- data %&gt;% \n  filter(YEAR == 2019) %&gt;% \n  filter(AAA %in% c(0:18)) %&gt;% \n  filter(YRIMMIG %in% c(1989:2003)) %&gt;% \n  mutate(Hispanic_Origin = ifelse(HISPAN %in% c(1:4), 1, 0)) %&gt;% \n  mutate(Middle = ifelse(EDUC_Grade == \"Middle\", 1, 0)) %&gt;% \n  mutate(High_School = ifelse(EDUC_Grade == \"High_School\", 1, 0)) %&gt;% \n  mutate(College = ifelse(EDUC_Grade == \"College\", 1, 0)) %&gt;% \n  mutate(Graduate_Degree = ifelse(EDUC_Grade == \"Graduate_Degree\", 1, 0)) %&gt;% \n  mutate(Gender = ifelse(SEX == 2 , 1 , 0)) %&gt;% \n  filter(Hispanic_Origin == 1) %&gt;% \n  mutate(Citizenship_Status = ifelse(CITIZEN == 2, 1, 0)) %&gt;%\n  filter(Graduate_Degree != 1) %&gt;% \n  filter(ELDCH != 99 |\n           YNGCH != 99) %&gt;% \n  filter(COUNTYFIP %in% c(37, 31, 201, 113, 13)) %&gt;% \n  mutate(Years_In_Country = (AGE- AAA)) %&gt;% \n  filter(STATEFIP %in% c(6, 48, 17, 4, 24, 32, 34) |\n           COUNTYFIP %in% c(37, 201, 113, 31, 13)) %&gt;% \n  mutate(YSM = (AGE - AAA)) %&gt;% \n  filter(STATEFIP != 0 |\n           COUNTYFIP != 0) %&gt;% \n  mutate(Experience = ifelse(EDUC_Grade == \"Elementary\", AGE - 5 - 5, \n                             ifelse(EDUC_Grade == \"Middle\", AGE - 8 - 5, \n                                    ifelse(EDUC_Grade == \"High_School\", AGE - 12 - 5, \n                                           ifelse(EDUC_Grade == \"College\", AGE - 16 - 5, 0))))) %&gt;% \n  mutate(Experience_Squared = ifelse(EDUC_Grade == \"Elementary\", ((AGE - 5 - 5)^2)/100, \n                                     ifelse(EDUC_Grade == \"Middle\", ((AGE - 8 - 5)^2)/100, \n                                            ifelse(EDUC_Grade == \"High_School\", ((AGE - 12 - 5)^2)/100, \n                                                   ifelse(EDUC_Grade == \"College\", ((AGE - 16 - 5)^2)/100, 0))))) %&gt;% \n  mutate(Education_Dummy = ifelse(EDUC_Grade %in% c(\"Elementary\", \"Middle\"), 0, 1)) %&gt;% \n  mutate(Years_Education = ifelse(EDUC_Grade == \"Elementary\", 5, \n                                  ifelse(EDUC_Grade == \"Middle\", 8,\n                                         ifelse(EDUC_Grade == \"High_School\", 12,\n                                                ifelse(EDUC_Grade == \"College\", 16, 0))))) %&gt;% \n  filter(INCTOT != 9999999 | INCTOT != 9999998) %&gt;% \n  mutate(Hispanic_Population = \n           ifelse(COUNTYFIP == 37 | STATEFIP == 6, 48.0,\n                  ifelse(COUNTYFIP == 201 | STATEFIP == 48, 43.0,\n                         ifelse(COUNTYFIP == 113 | STATEFIP == 48, 40.5,\n                                ifelse(COUNTYFIP == 31 | STATEFIP == 17, 26.2,\n                                       ifelse(COUNTYFIP == 13 | STATEFIP == 4, 30.6,\n                                              ifelse(COUNTYFIP == 13 | STATEFIP == 6, 27.0,\n                                                     ifelse(COUNTYFIP == 31 | STATEFIP == 24, 20.5,\n                                                            ifelse(COUNTYFIP == 31 | STATEFIP == 34, 42.7, \n                                                                   ifelse(COUNTYFIP == 31 | STATEFIP == 32, 25.1,\n                                                                          ifelse(COUNTYFIP == 13 | STATEFIP == 34, 24.4, 0))))))))))) %&gt;% \n  filter(Hispanic_Population != 0 )\n\n\n#mutate(COUNTY_POPULATION = ifelse(STATEFIP %in% c(6,12, 36, 48, 36, 34), 1, 0))\n\nview(head(Cities_2019\n))\n\n# Creating Population Variable \nCities_2019 %&gt;% \n  group_by(STATEFIP, COUNTYFIP) %&gt;% \n  summarise(n=n()) %&gt;% \n  arrange(-n)\n\n`summarise()` has grouped output by 'STATEFIP'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 26 × 3\n# Groups:   STATEFIP [16]\n   STATEFIP        COUNTYFIP     n\n   &lt;int+lbl&gt;       &lt;dbl+lbl&gt; &lt;int&gt;\n 1  6 [California]  37        1146\n 2 48 [Texas]      201         378\n 3 48 [Texas]      113         305\n 4 17 [Illinois]    31         232\n 5  4 [Arizona]     13         230\n 6  6 [California]  13          69\n 7 24 [Maryland]    31          63\n 8 34 [New Jersey]  31          58\n 9 32 [Nevada]      31          41\n10 34 [New Jersey]  13          25\n# ℹ 16 more rows\n\nCities_2019 &lt;- mutate(Cities_2019, Hispanic_Population = \n                        ifelse(COUNTYFIP == 37 | STATEFIP == 6, 48.0,\n                               ifelse(COUNTYFIP == 201 | STATEFIP == 48, 43.0,\n                                      ifelse(COUNTYFIP == 113 | STATEFIP == 48, 40.5,\n                                             ifelse(COUNTYFIP == 31 | STATEFIP == 17, 26.2,\n                                                    ifelse(COUNTYFIP == 13 | STATEFIP == 4, 30.6,\n                                                           ifelse(COUNTYFIP == 13 | STATEFIP == 6, 27.0,\n                                                                  ifelse(COUNTYFIP == 31 | STATEFIP == 24, 20.5,\n                                                                         ifelse(COUNTYFIP == 31 | STATEFIP == 34, 42.7, \n                                                                                ifelse(COUNTYFIP == 31 | STATEFIP == 32, 25.1,\n                                                                                       ifelse(COUNTYFIP == 13 | STATEFIP == 34, 24.4, 0)))))))))))\n\n#OLS 2019\nOLS2019 &lt;- lm(LogWages ~ AGE + Gender   +SPEAKENG_new + Citizenship_Status + Experience_Squared  , data = Cities_2019)\nsummary(OLS2019)\n\n\nCall:\nlm(formula = LogWages ~ AGE + Gender + SPEAKENG_new + Citizenship_Status + \n    Experience_Squared, data = Cities_2019)\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.9358 -0.3293  0.1055  0.4698  3.6066 \nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         8.94440    0.17506  51.092  &lt; 2e-16 ***\nAGE                 0.04485    0.00576   7.786 9.86e-15 ***\nGender             -0.61124    0.03230 -18.923  &lt; 2e-16 ***\nSPEAKENG_new       -0.08795    0.01800  -4.887 1.09e-06 ***\nCitizenship_Status  0.21076    0.03541   5.952 3.00e-09 ***\nExperience_Squared -0.08482    0.01216  -6.974 3.88e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nResidual standard error: 0.8206 on 2649 degrees of freedom\nMultiple R-squared:  0.1576,    Adjusted R-squared:  0.156 \nF-statistic: 99.08 on 5 and 2649 DF,  p-value: &lt; 2.2e-16\n\n#OlS on SPEAKEng\nENG2019 &lt;- lm(SPEAKENG_new ~ AAA  + ELDCH + YNGCH + \n                Hispanic_Population + Years_Education \n              ,\n              data = Cities_2019)\nsummary(ENG2019)\n\n\nCall:\nlm(formula = SPEAKENG_new ~ AAA + ELDCH + YNGCH + Hispanic_Population + \n    Years_Education, data = Cities_2019)\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.7504 -0.5614 -0.0670  0.5146  3.0462 \nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          2.928088   0.127784  22.914  &lt; 2e-16 ***\nAAA                  0.052016   0.003846  13.523  &lt; 2e-16 ***\nELDCH                0.015652   0.003709   4.220 2.53e-05 ***\nYNGCH               -0.017050   0.003853  -4.425 1.00e-05 ***\nHispanic_Population  0.005954   0.002013   2.958  0.00313 ** \nYears_Education     -0.094606   0.005913 -16.000  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nResidual standard error: 0.861 on 2649 degrees of freedom\nMultiple R-squared:  0.2248,    Adjusted R-squared:  0.2233 \nF-statistic: 153.6 on 5 and 2649 DF,  p-value: &lt; 2.2e-16\n\n#IV 2019\niv2019 &lt;- ivreg(LogWages ~  SPEAKENG_new + Experience_Squared  + Gender|    Experience_Squared  + \n                  Gender + AAA + Hispanic_Population +\n                  ELDCH + YNGCH  + Years_Education, data = Cities_2019)\nsummary(iv2019)\n\n\nCall:\nivreg(formula = LogWages ~ SPEAKENG_new + Experience_Squared + \n    Gender | Experience_Squared + Gender + AAA + Hispanic_Population + \n    ELDCH + YNGCH + Years_Education, data = Cities_2019)\nResiduals:\n     Min       1Q   Median       3Q      Max \n-6.19642 -0.37649  0.09258  0.51456  3.37004 \nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        10.956875   0.128300  85.400  &lt; 2e-16 ***\nSPEAKENG_new       -0.378763   0.053393  -7.094 1.67e-12 ***\nExperience_Squared  0.029789   0.008468   3.518 0.000443 ***\nGender             -0.594880   0.034157 -17.416  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nResidual standard error: 0.8681 on 2651 degrees of freedom\nMultiple R-Squared: 0.05643,    Adjusted R-squared: 0.05537 \nWald test: 110.7 on 3 and 2651 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "Directed_Study_Code.html#retrieving-packages",
    "href": "Directed_Study_Code.html#retrieving-packages",
    "title": "Directed Study Data Cleaning and Regression Analysis",
    "section": "",
    "text": "library(tidyverse)\nlibrary(AER)\nlibrary(ipumsr)"
  },
  {
    "objectID": "Directed_Study_Code.html#loading-in-data",
    "href": "Directed_Study_Code.html#loading-in-data",
    "title": "Directed Study Data Cleaning and Regression Analysis",
    "section": "",
    "text": "library(ipumsr)\nddi &lt;- read_ipums_ddi(\"usa_00018.xml\")\ndata &lt;- read_ipums_micro(ddi)\n\nUse of data from IPUMS USA is subject to conditions including that users should cite the data appropriately. Use command `ipums_conditions()` for more details."
  },
  {
    "objectID": "Directed_Study_Code.html#creating-new-variables",
    "href": "Directed_Study_Code.html#creating-new-variables",
    "title": "Directed Study Data Cleaning and Regression Analysis",
    "section": "",
    "text": "In this section I recode the English proficiency variable, create a categorical education variable, create a variable to measure the age at arrival for all immigrants, and adjust the income variable for inflation and convert it to log values.\n\nlibrary(tidyverse)\n# SPEAKENG\n\ndata$SPEAKENG_numeric &lt;- as.numeric(data$SPEAKENG)\ndata$SPEAKENG_new &lt;- dplyr::recode(data$SPEAKENG_numeric, `3` = 1, `4` = 2, `5` = 3, `6` = 4, `1` = 5)\n\n#EDUC\n\ndata$EDUC_Grade &lt;- case_when(data$EDUC %in% c(0:1) ~ \"Elementary\",\n                             data$EDUC %in% c(2) ~ \"Middle\",\n                             data$EDUC %in% c(3:6) ~ \"High_School\",\n                             data$EDUC %in% c(7:10) ~ \"College\",\n                             data$EDUC %in% c(11) ~ \"Graduate_Degree\")\n\n#Age At Arrival Variable \n\ndata &lt;- mutate(data, AAA = AGE - (YEAR - YRIMMIG))\n\n#Inflation Adjusted Income Variable \n\ndata &lt;- filter(data, INCTOT &gt; 0) \ndata &lt;- mutate(data, Inflation_Adjusted = INCTOT*CPI99)\n\n#Log wages\ndata &lt;- mutate(data, LogWages = log(Inflation_Adjusted))"
  },
  {
    "objectID": "Directed_Study_Code.html#abstract",
    "href": "Directed_Study_Code.html#abstract",
    "title": "Directed Study Data Cleaning and Regression Analysis",
    "section": "",
    "text": "New immigrants need to be successfully economically and socially integrated if they are to bring positive economic effects. One important piece of integration into a new society is language proficiency. As the U.S. economy increasingly relies on immigrant labor, it will become even more important to understand the returns to speaking English as an immigrant as well as and the determinants of English proficiency among immigrants. I focus specifically on the return to learning English as a Spanish speaker. As the U.S. becomes a more Spanish-speaker accessible country, as more government and other agencies are offering forms in Spanish, as well as broader use of Spanish and Spanish media in American culture, the incentive to learn English may be decreasing for Spanish immigrants.\n\nThis paper hypothesizes that the return to learning English has decreased over the years, both because the United States has become a more Spanish-friendly country and because the number of Spanish speakers has increased in the country over time. I analyze data from the American Community Survey (ACS) from 1980-2019. I first consider the determinants of English proficiency. I then use an instrumental variables approach to consider the effect of English proficiency on income, the return to proficiency.\n\nWhile I find no significant change in returns to English proficiency from decade to decade, there is a significant decrease between the 1980 sample and the 2019 sample. The results also show a significant reduction in the income gap between female and male Hispanic immigrants."
  },
  {
    "objectID": "Directed_Study_Code.html#creating-samples",
    "href": "Directed_Study_Code.html#creating-samples",
    "title": "Directed Study Data Cleaning and Regression Analysis",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# SPEAKENG\n\ndata$SPEAKENG_numeric &lt;- as.numeric(data$SPEAKENG)\ndata$SPEAKENG_new &lt;- dplyr::recode(data$SPEAKENG_numeric, `3` = 1, `4` = 2, `5` = 3, `6` = 4, `1` = 5)\n\n#EDUC\n\ndata$EDUC_Grade &lt;- case_when(data$EDUC %in% c(0:1) ~ \"Elementary\",\n                             data$EDUC %in% c(2) ~ \"Middle\",\n                             data$EDUC %in% c(3:6) ~ \"High_School\",\n                             data$EDUC %in% c(7:10) ~ \"College\",\n                             data$EDUC %in% c(11) ~ \"Graduate_Degree\")\n\n#Age At Arrival Variable \n\ndata &lt;- mutate(data, AAA = AGE - (YEAR - YRIMMIG))\n\n#Inflation Adjusted Income Variable \n\ndata &lt;- filter(data, INCTOT &gt; 0) \ndata &lt;- mutate(data, Inflation_Adjusted = INCTOT*CPI99)\n\n#Log wages\ndata &lt;- mutate(data, LogWages = log(Inflation_Adjusted))\n\n\n\n\n\n# 1980 --------------------------------------------------------------------\nlibrary(tidyverse)\nlibrary(AER)\n\nLoading required package: car\nLoading required package: carData\nAttaching package: 'car'\nThe following object is masked from 'package:dplyr':\n    recode\nThe following object is masked from 'package:purrr':\n    some\nLoading required package: lmtest\nLoading required package: zoo\nAttaching package: 'zoo'\nThe following objects are masked from 'package:base':\n    as.Date, as.Date.numeric\nLoading required package: sandwich\nLoading required package: survival\n\nCities_1980 &lt;- data %&gt;% \n  filter(YEAR == 1980) %&gt;% \n  filter(AAA %in% c(0:18)) %&gt;% \n  filter(YRIMMIG %in% c(1950:1964)) %&gt;% \n  mutate(Hispanic_Origin = ifelse(HISPAN %in% c(1:4), 1, 0)) %&gt;% \n  mutate(Middle = ifelse(EDUC_Grade == \"Middle\", 1, 0)) %&gt;% \n  mutate(High_School = ifelse(EDUC_Grade == \"High_School\", 1, 0)) %&gt;% \n  mutate(College = ifelse(EDUC_Grade == \"College\", 1, 0)) %&gt;% \n  mutate(Graduate_Degree = ifelse(EDUC_Grade == \"Graduate_Degree\", 1, 0)) %&gt;% \n  mutate(Gender = ifelse(SEX == 2 , 1 , 0)) %&gt;% \n  filter(Hispanic_Origin == 1) %&gt;% \n  mutate(Citizenship_Status = ifelse(CITIZEN == 2, 1, 0)) %&gt;% \n  filter(Graduate_Degree != 1) %&gt;% \n  filter(ELDCH != 99 |\n           YNGCH!= 99) %&gt;% \n  filter(STATEFIP %in% c(6, 12, 36, 48, 34) |\n           COUNTYFIP %in% c(37, 86, 73, 81, 215, 59, 85, 19, 47, 17)) %&gt;% \n  mutate(YSM = (AGE - AAA)) %&gt;% \n  filter(STATEFIP != 0 |\n           COUNTYFIP != 0) %&gt;% \n  mutate(Experience_Squared = ifelse(EDUC_Grade == \"Elementary\", ((AGE - 5 - 5)^2)/100, \n                                     ifelse(EDUC_Grade == \"Middle\", ((AGE - 8 - 5)^2)/100, \n                                            ifelse(EDUC_Grade == \"High_School\", ((AGE - 12 - 5)^2)/100, \n                                                   ifelse(EDUC_Grade == \"College\", ((AGE - 16 - 5)^2)/100, 0))))) %&gt;% \n  mutate(Experience = ifelse(EDUC_Grade == \"Elementary\", AGE - 5 - 5, \n                             ifelse(EDUC_Grade == \"Middle\", AGE - 8 - 5, \n                                    ifelse(EDUC_Grade == \"High_School\", AGE - 12 - 5, \n                                           ifelse(EDUC_Grade == \"College\", AGE - 16 - 5, 0))))) %&gt;% \n  mutate(Years_Education = ifelse(EDUC_Grade == \"Elementary\", 5, \n                                  ifelse(EDUC_Grade == \"Middle\", 8,\n                                         ifelse(EDUC_Grade == \"High_School\", 12,\n                                                ifelse(EDUC_Grade == \"College\", 16, 0))))) %&gt;% \n  mutate(Education_Dummy = ifelse(EDUC_Grade %in% c(\"Elementary\", \"Middle\"), 0, 1)) %&gt;% \n  filter(INCTOT != 9999999 | INCTOT != 9999998) %&gt;% \n  filter(ELDCH != 99 |\n           YNGCH != 99) %&gt;% \n  mutate(Hispanic_Population = \n           if_else(COUNTYFIP == 37, 27.6, \n                   if_else(COUNTYFIP == 86, 35.7, \n                           if_else(COUNTYFIP == 73, 14.8,\n                                   if_else(COUNTYFIP == 81, 13.9,\n                                           if_else(COUNTYFIP == 215, 81.3, \n                                                   \n                                                   if_else(COUNTYFIP == 59, 14.8,\n                                                           if_else(COUNTYFIP == 85, 17.5,\n                                                                   if_else(COUNTYFIP == 19, 29.3,\n                                                                           if_else(COUNTYFIP == 57, 17.6,\n                                                                                   if_else(COUNTYFIP == 17, 26.1, 0))))))))))) %&gt;% \n  filter(Hispanic_Population != 0)\n\n\nview(head(Cities_1980, 100))\n\n\n\n\n# OLS Regression on Log Wages\nOLS1980 &lt;- lm(LogWages ~ AGE + Gender  +  SPEAKENG_new + Citizenship_Status + Experience_Squared , data = Cities_1980)\nsummary(OLS1980)\n\n\nCall:\nlm(formula = LogWages ~ AGE + Gender + SPEAKENG_new + Citizenship_Status + \n    Experience_Squared, data = Cities_1980)\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.9014 -0.3299  0.1692  0.5495  1.9816 \nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         9.186201   0.308223  29.804  &lt; 2e-16 ***\nAGE                 0.054796   0.009766   5.611 2.68e-08 ***\nGender             -0.966164   0.060255 -16.035  &lt; 2e-16 ***\nSPEAKENG_new       -0.138774   0.039882  -3.480 0.000526 ***\nCitizenship_Status  0.038659   0.062648   0.617 0.537336    \nExperience_Squared -0.104708   0.021019  -4.982 7.56e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nResidual standard error: 0.8977 on 904 degrees of freedom\nMultiple R-squared:  0.2822,    Adjusted R-squared:  0.2782 \nF-statistic: 71.09 on 5 and 904 DF,  p-value: &lt; 2.2e-16\n\n#OLS Regression on SPEAKEN_eng\n\nENG1980 &lt;- lm(SPEAKENG_new ~ AAA  + ELDCH + YNGCH + \n                Hispanic_Population + Years_Education,\n              data = Cities_1980)\nsummary(ENG1980)\n\n\nCall:\nlm(formula = SPEAKENG_new ~ AAA + ELDCH + YNGCH + Hispanic_Population + \n    Years_Education, data = Cities_1980)\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.2925 -0.4762 -0.0577  0.4248  2.8313 \nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          3.573438   0.132603  26.948  &lt; 2e-16 ***\nAAA                  0.029550   0.005313   5.561 3.52e-08 ***\nELDCH                0.021850   0.005949   3.673 0.000254 ***\nYNGCH               -0.021568   0.006347  -3.398 0.000708 ***\nHispanic_Population  0.005802   0.001840   3.154 0.001665 ** \nYears_Education     -0.127407   0.007262 -17.545  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nResidual standard error: 0.7129 on 904 degrees of freedom\nMultiple R-squared:  0.3532,    Adjusted R-squared:  0.3496 \nF-statistic: 98.72 on 5 and 904 DF,  p-value: &lt; 2.2e-16\n\nEnglish1980 &lt;- predict(ENG1980)\nEnglish1980\n\n       1        2        3        4        5        6        7        8 \n2.826605 3.099828 2.799153 3.643958 2.392700 3.436822 2.670706 3.350664 \n       9       10       11       12       13       14       15       16 \n3.415485 2.870141 2.962242 2.663352 2.636064 3.211926 2.285201 1.871242 \n      17       18       19       20       21       22       23       24 \n3.124406 3.248209 2.328931 3.270625 2.201898 2.967561 2.581553 3.786320 \n      25       26       27       28       29       30       31       32 \n2.463352 2.522452 3.213622 1.955422 3.091343 3.094290 3.474411 2.558453 \n      33       34       35       36       37       38       39       40 \n2.446820 2.285083 2.234395 1.769855 2.948235 3.266548 2.753224 3.584838 \n      41       42       43       44       45       46       47       48 \n3.192263 2.536299 2.088548 2.333187 3.493531 2.487107 3.170695 2.509641 \n      49       50       51       52       53       54       55       56 \n1.882945 3.378553 3.020845 2.151443 2.555887 2.212068 2.630388 2.153423 \n      57       58       59       60       61       62       63       64 \n2.218125 2.639100 2.510772 2.775075 2.215551 2.728993 2.627724 2.794262 \n      65       66       67       68       69       70       71       72 \n2.811477 2.818210 3.316229 1.792880 2.453533 2.111365 2.816091 2.790757 \n      73       74       75       76       77       78       79       80 \n2.336720 2.043315 2.446234 2.556735 2.818210 3.013428 1.646542 3.657971 \n      81       82       83       84       85       86       87       88 \n2.729394 2.955011 1.916567 2.307620 2.985706 2.610812 2.606789 3.557735 \n      89       90       91       92       93       94       95       96 \n3.070452 2.233985 1.946419 2.738009 2.308010 2.554473 2.406721 2.004823 \n      97       98       99      100      101      102      103      104 \n2.542585 2.739922 2.338506 1.904960 2.543716 2.543716 2.274251 2.305637 \n     105      106      107      108      109      110      111      112 \n2.783340 3.391690 2.675212 2.125769 2.575529 2.598621 2.490217 2.559521 \n     113      114      115      116      117      118      119      120 \n3.239194 2.871555 3.277130 2.550579 2.657687 2.990129 3.503451 2.435553 \n     121      122      123      124      125      126      127      128 \n2.020061 2.650881 2.755423 3.073908 3.367477 2.787658 3.142681 2.839907 \n     129      130      131      132      133      134      135      136 \n3.513413 3.409035 3.255115 3.410613 2.303484 3.213276 3.531358 3.763684 \n     137      138      139      140      141      142      143      144 \n1.823125 2.512599 2.275258 3.449551 3.608687 2.300858 3.573682 2.542068 \n     145      146      147      148      149      150      151      152 \n3.440482 3.731224 2.616962 2.460543 3.279548 3.094426 2.009460 3.275306 \n     153      154      155      156      157      158      159      160 \n2.328876 1.867429 2.892967 3.489392 3.194803 2.307874 2.472558 2.080567 \n     161      162      163      164      165      166      167      168 \n2.169354 1.798649 2.720755 2.303997 2.659138 2.746257 2.716707 3.654343 \n     169      170      171      172      173      174      175      176 \n2.468927 1.874281 3.760209 3.259788 3.389083 2.174820 3.284302 2.265140 \n     177      178      179      180      181      182      183      184 \n2.197746 2.640972 2.324020 2.212863 2.442787 2.127724 2.504150 1.844509 \n     185      186      187      188      189      190      191      192 \n2.772877 2.293621 3.510402 1.726591 3.569220 3.322738 2.621094 2.441090 \n     193      194      195      196      197      198      199      200 \n2.323737 3.301178 2.749494 2.827106 2.674035 3.059284 2.829761 3.153545 \n     201      202      203      204      205      206      207      208 \n2.844195 2.981680 2.293621 2.554985 2.286114 2.764493 2.721641 1.975164 \n     209      210      211      212      213      214      215      216 \n2.621503 2.362312 2.109496 2.721358 2.235086 3.655380 2.624842 3.379018 \n     217      218      219      220      221      222      223      224 \n3.394866 3.085613 2.646975 3.134633 3.516852 2.614926 3.098750 2.381989 \n     225      226      227      228      229      230      231      232 \n2.234237 2.329904 3.326486 2.806104 2.198548 2.555543 1.843378 2.626421 \n     233      234      235      236      237      238      239      240 \n2.969108 3.633812 2.926395 3.451702 2.109331 3.337014 3.583535 2.510592 \n     241      242      243      244      245      246      247      248 \n2.279946 3.574994 3.568371 3.494438 2.344338 2.853964 2.653598 2.287646 \n     249      250      251      252      253      254      255      256 \n1.727439 3.285614 3.195713 3.089801 2.021410 2.198713 3.722455 2.587592 \n     257      258      259      260      261      262      263      264 \n2.957166 2.012579 2.476296 1.905024 3.044568 2.236528 3.060250 2.744339 \n     265      266      267      268      269      270      271      272 \n3.372167 2.551190 2.035114 2.229230 2.889710 2.293338 2.280483 3.194983 \n     273      274      275      276      277      278      279      280 \n2.168031 2.906614 1.997298 2.456654 3.694273 2.974427 2.315353 2.412269 \n     281      282      283      284      285      286      287      288 \n2.412269 2.840408 1.813262 3.136047 2.169563 2.213429 3.172329 3.387731 \n     289      290      291      292      293      294      295      296 \n2.994699 3.694273 3.053116 2.750625 1.902361 3.293879 2.368569 2.604972 \n     297      298      299      300      301      302      303      304 \n2.264070 2.841978 2.213028 3.393167 2.474055 2.743609 2.715072 2.650369 \n     305      306      307      308      309      310      311      312 \n2.323171 2.296166 2.323454 2.981397 2.324020 3.128866 1.911192 3.101460 \n     313      314      315      316      317      318      319      320 \n3.442472 2.712809 1.812296 2.980549 2.648672 2.293621 1.754162 2.698093 \n     321      322      323      324      325      326      327      328 \n2.035396 1.858094 1.873093 3.292465 2.714679 2.654675 2.770550 2.816578 \n     329      330      331      332      333      334      335      336 \n2.870524 3.833239 2.519431 2.663106 2.153479 3.333502 2.677657 3.084929 \n     337      338      339      340      341      342      343      344 \n1.932759 2.622068 3.130846 2.350935 3.251420 3.247232 3.119068 3.533219 \n     345      346      347      348      349      350      351      352 \n3.032798 2.066078 2.056116 1.937914 2.502736 2.532286 2.535225 2.323737 \n     353      354      355      356      357      358      359      360 \n2.168715 1.879945 2.735627 1.841846 3.208285 2.536757 2.316037 3.458437 \n     361      362      363      364      365      366      367      368 \n3.449323 2.803723 2.685521 2.535983 2.417782 3.795424 2.804124 3.305484 \n     369      370      371      372      373      374      375      376 \n2.293220 2.612270 3.204261 3.513631 3.242196 3.762652 2.324302 2.677374 \n     377      378      379      380      381      382      383      384 \n3.186599 2.588322 2.665769 2.049948 2.950225 2.920675 2.294752 2.505564 \n     385      386      387      388      389      390      391      392 \n2.338764 2.640972 2.785384 2.611422 2.308648 2.782438 1.815525 3.384620 \n     393      394      395      396      397      398      399      400 \n3.414171 3.631826 3.395423 2.853992 2.489315 2.647942 2.331036 2.516202 \n     401      402      403      404      405      406      407      408 \n3.538422 2.521357 3.584337 3.285614 3.166467 2.543891 2.630892 3.103841 \n     409      410      411      412      413      414      415      416 \n2.535507 1.966781 2.264636 3.041064 3.735081 2.209634 2.914478 3.252670 \n     417      418      419      420      421      422      423      424 \n3.313750 2.500190 2.805255 2.739295 3.084764 2.516038 3.761521 3.473719 \n     425      426      427      428      429      430      431      432 \n1.991413 1.855996 2.702171 1.950815 1.754162 3.423850 3.112217 3.666136 \n     433      434      435      436      437      438      439      440 \n2.269984 2.772877 2.499507 2.263788 2.621785 3.640255 2.763243 2.293338 \n     441      442      443      444      445      446      447      448 \n3.710284 3.680734 2.264070 2.530024 2.507206 2.613237 2.832797 2.323171 \n     449      450      451      452      453      454      455      456 \n2.823594 1.900664 2.442504 2.880596 2.384251 2.469391 2.682300 3.074476 \n     457      458      459      460      461      462      463      464 \n2.413236 1.961179 3.117426 2.947397 2.663507 3.159831 2.650487 3.247350 \n     465      466      467      468      469      470      471      472 \n3.299363 2.443069 2.726960 2.622917 2.543207 2.713375 2.733410 2.822061 \n     473      474      475      476      477      478      479      480 \n3.576125 2.603840 3.275533 1.769160 3.675698 2.661856 3.621186 3.502702 \n     481      482      483      484      485      486      487      488 \n3.463756 3.522857 2.846575 3.155916 3.306497 3.847508 3.465289 2.640407 \n     489      490      491      492      493      494      495      496 \n3.120482 2.523337 3.222325 2.907938 2.548416 2.300071 2.111877 2.444766 \n     497      498      499      500      501      502      503      504 \n2.386370 2.925547 3.201032 2.699225 2.647706 2.682858 2.262001 2.744740 \n     505      506      507      508      509      510      511      512 \n2.736876 3.430018 1.878813 2.618556 2.235368 3.195430 4.000461 3.342899 \n     513      514      515      516      517      518      519      520 \n2.108648 2.388722 2.588841 2.602472 2.324302 2.013993 2.252777 2.137797 \n     521      522      523      524      525      526      527      528 \n4.050211 2.265202 2.193959 2.789855 2.774008 3.383772 3.912494 3.593615 \n     529      530      531      532      533      534      535      536 \n2.737606 2.811140 2.781590 2.711851 2.677657 2.214113 2.797668 2.636457 \n     537      538      539      540      541      542      543      544 \n2.565862 1.814514 2.750062 2.196542 2.947235 3.170708 3.142453 3.411233 \n     545      546      547      548      549      550      551      552 \n3.788811 1.739000 2.571181 3.121287 1.967585 2.127963 2.558280 3.318223 \n     553      554      555      556      557      558      559      560 \n2.711799 1.711147 2.712930 2.919664 2.552725 1.970130 3.048483 3.162888 \n     561      562      563      564      565      566      567      568 \n2.148399 2.138884 2.380978 2.993971 3.033788 3.567078 2.456610 2.722610 \n     569      570      571      572      573      574      575      576 \n3.602396 3.065580 2.559576 2.758729 2.093204 3.348174 3.259523 3.056466 \n     577      578      579      580      581      582      583      584 \n2.632544 1.827652 2.233792 2.145736 1.921393 2.014232 2.486726 2.387712 \n     585      586      587      588      589      590      591      592 \n1.746865 3.274075 2.041237 3.111607 3.099884 3.688673 3.010950 2.626660 \n     593      594      595      596      597      598      599      600 \n3.000311 2.468380 2.236337 2.186773 3.680973 3.303060 1.832852 1.681879 \n     601      602      603      604      605      606      607      608 \n2.250606 2.374410 3.623003 3.645538 3.261786 2.070788 2.705631 3.602403 \n     609      610      611      612      613      614      615      616 \n3.131249 3.072148 3.327455 2.640527 3.398609 2.270670 2.576390 3.374377 \n     617      618      619      620      621      622      623      624 \n2.196542 3.083589 2.219359 2.722885 1.887035 2.279025 2.190374 2.744460 \n     625      626      627      628      629      630      631      632 \n3.628724 3.368210 2.337278 3.562206 1.889863 3.114317 2.461135 3.648595 \n     633      634      635      636      637      638      639      640 \n3.648595 2.739821 1.939445 2.658304 2.688986 2.475966 2.940781 2.894113 \n     641      642      643      644      645      646      647      648 \n2.244911 2.726236 3.453182 2.489498 2.051308 2.490063 2.399909 2.578714 \n     649      650      651      652      653      654      655      656 \n2.573842 2.460230 2.665668 2.902683 2.310781 2.310781 3.467332 2.422296 \n     657      658      659      660      661      662      663      664 \n3.626459 3.338893 2.580411 2.547750 2.122469 3.309508 2.918083 2.728163 \n     665      666      667      668      669      670      671      672 \n2.648736 2.549164 3.052808 2.786698 2.609396 3.211349 2.738920 2.705911 \n     673      674      675      676      677      678      679      680 \n2.705911 1.979023 2.518199 3.237789 2.400564 2.371013 1.830706 2.575768 \n     681      682      683      684      685      686      687      688 \n2.007041 2.357622 2.383496 2.023007 2.072199 2.320207 2.547750 2.729011 \n     689      690      691      692      693      694      695      696 \n2.783587 2.742030 2.830681 3.046923 2.791452 2.340615 2.340615 3.103643 \n     697      698      699      700      701      702      703      704 \n2.767220 2.832944 2.714742 1.981286 3.382476 2.792300 3.047206 3.387348 \n     705      706      707      708      709      710      711      712 \n2.150605 2.009705 3.241465 2.172738 2.547184 2.374263 2.404097 2.311064 \n     713      714      715      716      717      718      719      720 \n2.347377 2.842852 2.210554 2.763315 2.799316 2.755734 3.295074 2.365605 \n     721      722      723      724      725      726      727      728 \n2.336054 2.163058 2.163058 2.688933 2.271745 2.501833 2.451964 2.835600 \n     729      730      731      732      733      734      735      736 \n3.125776 2.519048 2.276105 3.072375 2.008856 2.542312 2.067109 2.095975 \n     737      738      739      740      741      742      743      744 \n2.202288 1.847119 2.895667 2.121903 2.052274 2.794845 2.706194 2.385357 \n     745      746      747      748      749      750      751      752 \n2.821850 2.298074 2.209706 2.127058 2.681680 2.918083 2.942589 2.431245 \n     753      754      755      756      757      758      759      760 \n2.401695 2.097225 2.294845 2.176643 2.858982 2.260705 2.310781 3.155588 \n     761      762      763      764      765      766      767      768 \n2.341775 2.041235 1.949473 1.860822 2.705063 2.823264 1.889806 2.312224 \n     769      770      771      772      773      774      775      776 \n1.891503 2.764164 2.781371 2.840472 2.333108 2.067674 1.963340 1.904240 \n     777      778      779      780      781      782      783      784 \n2.669062 2.639512 2.609113 2.550012 2.511065 1.922468 2.389663 1.949473 \n     785      786      787      788      789      790      791      792 \n2.038124 2.609961 2.639512 2.451116 2.340897 1.992891 2.315335 2.690064 \n     793      794      795      796      797      798      799      800 \n2.601413 2.772429 2.282957 1.987453 2.067109 2.096659 1.801155 2.310781 \n     801      802      803      804      805      806      807      808 \n2.303676 2.554483 2.833903 2.121055 2.662776 2.667648 2.209304 2.541628 \n     809      810      811      812      813      814      815      816 \n2.776900 2.639512 2.805290 1.978458 2.669062 2.584316 2.341209 2.791734 \n     817      818      819      820      821      822      823      824 \n1.889806 2.547184 2.296660 2.616412 2.586861 2.791452 2.606331 2.250743 \n     825      826      827      828      829      830      831      832 \n2.546783 2.428582 2.828136 2.382812 2.399715 1.830989 2.251427 2.111375 \n     833      834      835      836      837      838      839      840 \n2.037841 2.725900 2.696350 2.133225 2.456911 2.205793 2.200026 2.068240 \n     841      842      843      844      845      846      847      848 \n2.577300 2.187572 2.341463 2.966820 2.008008 2.669062 2.550861 3.345061 \n     849      850      851      852      853      854      855      856 \n1.992891 2.068240 3.270278 1.837156 2.577017 2.386889 2.772712 2.518199 \n     857      858      859      860      861      862      863      864 \n1.905088 2.733765 2.218820 3.147441 2.319212 3.384008 2.755050 2.725499 \n     865      866      867      868      869      870      871      872 \n2.117260 2.420033 2.009304 2.865834 2.156608 3.077486 3.268470 3.120719 \n     873      874      875      876      877      878      879      880 \n1.852958 3.693107 4.016517 3.241430 3.981074 2.242049 3.928832 2.829569 \n     881      882      883      884      885      886      887      888 \n2.605455 2.235480 2.867094 3.911854 3.598853 3.571119 3.541569 3.931771 \n     889      890      891      892      893      894      895      896 \n3.392686 3.312419 2.802793 3.467750 3.614820 3.936815 3.525045 3.542983 \n     897      898      899      900      901      902      903      904 \n3.806271 2.094297 4.022520 3.937656 3.885414 2.794720 3.797329 3.413695 \n     905      906      907      908      909      910 \n4.029089 2.930391 3.583855 3.813122 4.028688 2.956438 \n\n# IV Regression 1980\n\niv1980 &lt;- ivreg(LogWages ~  SPEAKENG_new + Experience_Squared  + Gender  |    Experience_Squared   +\n                  Gender + AAA + Hispanic_Population +\n                  ELDCH + YNGCH + Years_Education , data = Cities_1980)\nsummary(iv1980)\n\n\nCall:\nivreg(formula = LogWages ~ SPEAKENG_new + Experience_Squared + \n    Gender | Experience_Squared + Gender + AAA + Hispanic_Population + \n    ELDCH + YNGCH + Years_Education, data = Cities_1980)\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.1518 -0.4208  0.1368  0.5918  2.7804 \nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        11.82506    0.20959  56.421  &lt; 2e-16 ***\nSPEAKENG_new       -0.64873    0.09515  -6.818 1.68e-11 ***\nExperience_Squared  0.05318    0.01501   3.542 0.000417 ***\nGender             -0.91953    0.06671 -13.784  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nResidual standard error: 0.9782 on 906 degrees of freedom\nMultiple R-Squared: 0.1457, Adjusted R-squared: 0.1429 \nWald test:  97.2 on 3 and 906 DF,  p-value: &lt; 2.2e-16 \n\nEducation1980 &lt;- lm(LogWages ~ Years_Education + Experience + Experience_Squared + AGE + Gender, data = Cities_1980)\nsummary(Education1980)\n\n\nCall:\nlm(formula = LogWages ~ Years_Education + Experience + Experience_Squared + \n    AGE + Gender, data = Cities_1980)\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.0296 -0.2925  0.1588  0.5471  2.0473 \nCoefficients: (1 not defined because of singularities)\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         8.97043    0.22179  40.445  &lt; 2e-16 ***\nYears_Education     0.08584    0.01136   7.553 1.04e-13 ***\nExperience          0.02766    0.01542   1.793   0.0733 .  \nExperience_Squared -0.03554    0.03910  -0.909   0.3636    \nAGE                      NA         NA      NA       NA    \nGender             -0.97887    0.06013 -16.279  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nResidual standard error: 0.8992 on 905 degrees of freedom\nMultiple R-squared:  0.2789,    Adjusted R-squared:  0.2757 \nF-statistic: 87.51 on 4 and 905 DF,  p-value: &lt; 2.2e-16\n\nUsed_variables &lt;- Cities_1980 %&gt;% \n  select(AGE, SPEAKENG_new, Experience_Squared, Experience, AAA, Years_Education)\n\n\n\ncor(Used_variables, use=\"complete.obs\", method=\"pearson\")\n\n                          AGE SPEAKENG_new Experience_Squared Experience\nAGE                 1.0000000    0.2763966          0.8771535  0.9187336\nSPEAKENG_new        0.2763966    1.0000000          0.4502289  0.4496153\nExperience_Squared  0.8771535    0.4502289          1.0000000  0.9728432\nExperience          0.9187336    0.4496153          0.9728432  1.0000000\nAAA                 0.6795759    0.2758524          0.5686798  0.6247809\nYears_Education    -0.2643848   -0.5509890         -0.6397067 -0.6237263\n                          AAA Years_Education\nAGE                 0.6795759      -0.2643848\nSPEAKENG_new        0.2758524      -0.5509890\nExperience_Squared  0.5686798      -0.6397067\nExperience          0.6247809      -0.6237263\nAAA                 1.0000000      -0.1807239\nYears_Education    -0.1807239       1.0000000\n\n?cor\n\nclass(Cities_1980$COUNTY_POPULATION)\n\nWarning: Unknown or uninitialised column: `COUNTY_POPULATION`.\n\n\n[1] \"NULL\"\n\n#Creating Population Concentration variable\n\nCounties1980 &lt;- Cities_1980 %&gt;%\n  filter(COUNTYFIP != 0) %&gt;% \n  group_by(STATEFIP, COUNTYFIP) %&gt;% \n  summarise(n = n()) %&gt;% \n  arrange(-n)\n\n`summarise()` has grouped output by 'STATEFIP'. You can override using the\n`.groups` argument.\n\n# County Codes\n# 37 = Los Angeles, California, 27.6% \n\n# 86 = Dade/Miami, Florida, 35.7%\n\n# 73 = San Diego, California, 14.8%   \n\n# 81 = Queens, New York, 13.9%\n\n# 215 = Hidalgo, Texas, 81.3%\n\n# 59 = Orange County, California, 14.8%\n\n# 85 = Santa Clara County, California, 17.5%\n\n# 19 = fresno county, California, 29.3%\n\n# 57 = kings County, New york, 17.6% \n\n# 17 = hudson County, New Jersey, 26.1%\n\n\n\nmutate(Cities_1980, Hispanic_Population = \n         if_else(COUNTYFIP == 37, 27.6, \n                 if_else(COUNTYFIP == 86, 35.7, \n                         if_else(COUNTYFIP == 73, 14.8,\n                                 if_else(COUNTYFIP == 81, 13.9,\n                                         if_else(COUNTYFIP == 215, 81.3, \n                                                 \n                                                 if_else(COUNTYFIP == 59, 14.8,\n                                                         if_else(COUNTYFIP == 85, 17.5,\n                                                                 if_else(COUNTYFIP == 19, 29.3,\n                                                                         if_else(COUNTYFIP == 57, 17.6,\n                                                                                 if_else(COUNTYFIP == 17, 26.1, 0)))))))))))\n\n# A tibble: 910 × 54\n    YEAR SAMPLE           SERIAL CBSERIAL  HHWT  CLUSTER CPI99 STATEICP STATEFIP\n   &lt;int&gt; &lt;int+lbl&gt;         &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;int+lb&gt; &lt;int+lb&gt;\n 1  1980 198002 [1980 1%]   2475       NA   100  1.98e12  2.30 61 [Ari… 4 [Ariz…\n 2  1980 198002 [1980 1%]   2475       NA   100  1.98e12  2.30 61 [Ari… 4 [Ariz…\n 3  1980 198002 [1980 1%]   2718       NA   100  1.98e12  2.30 61 [Ari… 4 [Ariz…\n 4  1980 198002 [1980 1%]   2869       NA   100  1.98e12  2.30 61 [Ari… 4 [Ariz…\n 5  1980 198002 [1980 1%]   3197       NA   100  1.98e12  2.30 61 [Ari… 4 [Ariz…\n 6  1980 198002 [1980 1%]   3206       NA   100  1.98e12  2.30 61 [Ari… 4 [Ariz…\n 7  1980 198002 [1980 1%]   3519       NA   100  1.98e12  2.30 61 [Ari… 4 [Ariz…\n 8  1980 198002 [1980 1%]   3775       NA   100  1.98e12  2.30 61 [Ari… 4 [Ariz…\n 9  1980 198002 [1980 1%]   3811       NA   100  1.98e12  2.30 61 [Ari… 4 [Ariz…\n10  1980 198002 [1980 1%]   3813       NA   100  1.98e12  2.30 61 [Ari… 4 [Ariz…\n# ℹ 900 more rows\n# ℹ 45 more variables: COUNTYFIP &lt;dbl+lbl&gt;, CITY &lt;int+lbl&gt;, STRATA &lt;dbl&gt;,\n#   GQ &lt;int+lbl&gt;, PERNUM &lt;dbl&gt;, PERWT &lt;dbl&gt;, ELDCH &lt;int+lbl&gt;, YNGCH &lt;int+lbl&gt;,\n#   SEX &lt;int+lbl&gt;, AGE &lt;int+lbl&gt;, MARST &lt;int+lbl&gt;, RACE &lt;int+lbl&gt;,\n#   RACED &lt;int+lbl&gt;, HISPAN &lt;int+lbl&gt;, HISPAND &lt;int+lbl&gt;, BPL &lt;int+lbl&gt;,\n#   BPLD &lt;int+lbl&gt;, CITIZEN &lt;int+lbl&gt;, YRIMMIG &lt;int+lbl&gt;, LANGUAGE &lt;int+lbl&gt;,\n#   LANGUAGED &lt;int+lbl&gt;, SPEAKENG &lt;int+lbl&gt;, EDUC &lt;int+lbl&gt;, EDUCD &lt;int+lbl&gt;, …\n\nview(head(Cities_1980, 10))\n\n\nENG80 &lt;- lm(SPEAKENG_new ~ AAA + Citizenship_Status + ELDCH + YNGCH + Hispanic_Population + Citizenship_Status + Years_Education,\n            data = Cities_1980)\nsummary(ENG80)\n\n\nCall:\nlm(formula = SPEAKENG_new ~ AAA + Citizenship_Status + ELDCH + \n    YNGCH + Hispanic_Population + Citizenship_Status + Years_Education, \n    data = Cities_1980)\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.16632 -0.45188 -0.04948  0.40844  2.69892 \nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          3.531428   0.131826  26.789  &lt; 2e-16 ***\nAAA                  0.030225   0.005269   5.736 1.32e-08 ***\nCitizenship_Status  -0.202931   0.049017  -4.140 3.80e-05 ***\nELDCH                0.021707   0.005897   3.681 0.000246 ***\nYNGCH               -0.019993   0.006302  -3.172 0.001563 ** \nHispanic_Population  0.006846   0.001841   3.719 0.000212 ***\nYears_Education     -0.119252   0.007463 -15.980  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nResidual standard error: 0.7066 on 903 degrees of freedom\nMultiple R-squared:  0.3652,    Adjusted R-squared:  0.361 \nF-statistic: 86.59 on 6 and 903 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(AER)\n\n# SPEAKENG\n\ndata$SPEAKENG_numeric &lt;- as.numeric(data$SPEAKENG)\ndata$SPEAKENG_new &lt;- dplyr::recode(data$SPEAKENG_numeric, `3` = 1, `4` = 2, `5` = 3, `6` = 4, `1` = 5)\n\n#EDUC\n\ndata$EDUC_Grade &lt;- case_when(data$EDUC %in% c(0:1) ~ \"Elementary\",\n                             data$EDUC %in% c(2) ~ \"Middle\",\n                             data$EDUC %in% c(3:6) ~ \"High_School\",\n                             data$EDUC %in% c(7:10) ~ \"College\",\n                             data$EDUC %in% c(11) ~ \"Graduate_Degree\")\n\n#Age At Arrival Variable \n\ndata &lt;- mutate(data, AAA = AGE - (YEAR - YRIMMIG))\n\n#Inflation Adjusted Income Variable \n\ndata &lt;- filter(data, INCTOT &gt; 0) \ndata &lt;- mutate(data, Inflation_Adjusted = INCTOT*CPI99)\n\n#Log wages\ndata &lt;- mutate(data, LogWages = log(Inflation_Adjusted))\n\n\n\n# 1990 --------------------------------------------------------------------\nlibrary(tidyverse)\n# SPEAKENG\n\ndata$SPEAKENG_numeric &lt;- as.numeric(data$SPEAKENG)\ndata$SPEAKENG_new &lt;- dplyr::recode(data$SPEAKENG_numeric, `3` = 1, `4` = 2, `5` = 3, `6` = 4, `1` = 5)\n\n#EDUC\n\ndata$EDUC_Grade &lt;- case_when(data$EDUC %in% c(0:1) ~ \"Elementary\",\n                             data$EDUC %in% c(2) ~ \"Middle\",\n                             data$EDUC %in% c(3:6) ~ \"High_School\",\n                             data$EDUC %in% c(7:10) ~ \"College\",\n                             data$EDUC %in% c(11) ~ \"Graduate_Degree\")\n\n#Age At Arrival Variable \n\ndata &lt;- mutate(data, AAA = AGE - (YEAR - YRIMMIG))\n\n#Inflation Adjusted Income Variable \n\ndata &lt;- filter(data, INCTOT &gt; 0) \ndata &lt;- mutate(data, Inflation_Adjusted = INCTOT*CPI99)\n\n#Log wages\ndata &lt;- mutate(data, LogWages = log(Inflation_Adjusted))\n\n\nCities_1990 &lt;- Sample1990 &lt;- data %&gt;% \n  filter(YEAR == 1990) %&gt;% \n  filter(AAA %in% c(0:18)) %&gt;% \n  filter(YRIMMIG %in% c(1960:1974)) %&gt;% \n  mutate(Hispanic_Origin = ifelse(HISPAN %in% c(1:4), 1, 0)) %&gt;% \n  mutate(Middle = ifelse(EDUC_Grade == \"Middle\", 1, 0)) %&gt;% \n  mutate(High_School = ifelse(EDUC_Grade == \"High_School\", 1, 0)) %&gt;% \n  mutate(College = ifelse(EDUC_Grade == \"College\", 1, 0)) %&gt;% \n  mutate(Graduate_Degree = ifelse(EDUC_Grade == \"Graduate_Degree\", 1, 0)) %&gt;% \n  mutate(Gender = ifelse(SEX == 2 , 1 , 0)) %&gt;% \n  filter(Hispanic_Origin == 1) %&gt;% \n  mutate(Citizenship_Status = ifelse(CITIZEN == 2, 1, 0)) %&gt;% \n  filter(Graduate_Degree != 1) %&gt;% \n  filter(ELDCH != 99 |\n           YNGCH!= 99) %&gt;% \n  filter(STATEFIP %in% c(6, 12, 17, 36)|\n           COUNTYFIP %in% c(37, 86, 31, 59, 5, 48)) %&gt;% \n  mutate(YSM = (AGE - AAA)) %&gt;% \n  filter(STATEFIP != 0 |\n           COUNTYFIP != 0)%&gt;% \n  mutate(Experience_Squared = ifelse(EDUC_Grade == \"Elementary\", ((AGE - 5 - 5)^2)/100, \n                                     ifelse(EDUC_Grade == \"Middle\", ((AGE - 8 - 5)^2)/100, \n                                            ifelse(EDUC_Grade == \"High_School\", ((AGE - 12 - 5)^2)/100, \n                                                   ifelse(EDUC_Grade == \"College\", ((AGE - 16 - 5)^2)/100, 0))))) %&gt;% \n  mutate(Experience = ifelse(EDUC_Grade == \"Elementary\", AGE - 5 - 5, \n                             ifelse(EDUC_Grade == \"Middle\", AGE - 8 - 5, \n                                    ifelse(EDUC_Grade == \"High_School\", AGE - 12 - 5, \n                                           ifelse(EDUC_Grade == \"College\", AGE - 16 - 5, 0))))) %&gt;% \n  mutate(Years_Education = ifelse(EDUC_Grade == \"Elementary\", 5, \n                                  ifelse(EDUC_Grade == \"Middle\", 8,\n                                         ifelse(EDUC_Grade == \"High_School\", 12,\n                                                ifelse(EDUC_Grade == \"College\", 16, 0))))) %&gt;% \n  mutate(Education_Dummy = ifelse(EDUC_Grade %in% c(\"Elementary\", \"Middle\"), 0, 1)) %&gt;% \n  filter(INCTOT != 9999999 | INCTOT != 9999998) %&gt;% \n  mutate( Hispanic_Population = \n            if_else(COUNTYFIP == 37, 37.8 , \n                    if_else(COUNTYFIP == 86, 49.2 , \n                            if_else(COUNTYFIP == 31, 13.6 ,\n                                    if_else(COUNTYFIP == 59, 23.4 ,\n                                            if_else(COUNTYFIP == 5, 43.5,\n                                                    if_else(COUNTYFIP == 47, 20.1,\n                                                            if_else(COUNTYFIP == 73, 20.4,\n                                                                    if_else(COUNTYFIP == 81, 19.5,\n                                                                            if_else(COUNTYFIP == 61, 26.0,\n                                                                                    if_else(COUNTYFIP == 141, 69.6, 0))))))))))) %&gt;% \n  filter(Hispanic_Population != 0)\n\n\n\n\n#1990 OLS \nOLS1990 &lt;- lm(LogWages ~ AGE + Gender  +  SPEAKENG_new + Citizenship_Status + Experience_Squared, data = Cities_1990)\nsummary(OLS1990)\n\n\nCall:\nlm(formula = LogWages ~ AGE + Gender + SPEAKENG_new + Citizenship_Status + \n    Experience_Squared, data = Cities_1990)\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.7612 -0.3327  0.1675  0.5374  2.8682 \nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         9.12540    0.16636  54.854  &lt; 2e-16 ***\nAGE                 0.04991    0.00525   9.507  &lt; 2e-16 ***\nGender             -0.76589    0.03214 -23.831  &lt; 2e-16 ***\nSPEAKENG_new       -0.13018    0.01929  -6.750 1.76e-11 ***\nCitizenship_Status  0.20821    0.03422   6.084 1.31e-09 ***\nExperience_Squared -0.09769    0.01171  -8.341  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nResidual standard error: 0.8876 on 3071 degrees of freedom\nMultiple R-squared:  0.2203,    Adjusted R-squared:  0.219 \nF-statistic: 173.5 on 5 and 3071 DF,  p-value: &lt; 2.2e-16\n\n#1990 OLS on SPEAKENG\nENG1990 &lt;- lm(SPEAKENG_new ~ AAA + ELDCH + YNGCH + \n                Hispanic_Population + Years_Education,\n              data = Cities_1990)\nsummary(ENG1990)\n\n\nCall:\nlm(formula = SPEAKENG_new ~ AAA + ELDCH + YNGCH + Hispanic_Population + \n    Years_Education, data = Cities_1990)\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.67283 -0.47408 -0.09447  0.49429  3.10204 \nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          3.5731076  0.0818509  43.654  &lt; 2e-16 ***\nAAA                  0.0370415  0.0033118  11.185  &lt; 2e-16 ***\nELDCH                0.0074700  0.0033038   2.261   0.0238 *  \nYNGCH               -0.0135626  0.0033779  -4.015 6.08e-05 ***\nHispanic_Population -0.0004506  0.0012241  -0.368   0.7128    \nYears_Education     -0.1084347  0.0043794 -24.760  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nResidual standard error: 0.7894 on 3071 degrees of freedom\nMultiple R-squared:  0.2737,    Adjusted R-squared:  0.2725 \nF-statistic: 231.4 on 5 and 3071 DF,  p-value: &lt; 2.2e-16\n\n#1990 IV \niv1990 &lt;- ivreg(LogWages ~  SPEAKENG_new + Experience_Squared + Gender|Experience_Squared   +\n                  Gender + AAA + Hispanic_Population +\n                  ELDCH + YNGCH + Years_Education , data = Cities_1990)\n\nsummary(iv1990)\n\n\nCall:\nivreg(formula = LogWages ~ SPEAKENG_new + Experience_Squared + \n    Gender | Experience_Squared + Gender + AAA + Hispanic_Population + \n    ELDCH + YNGCH + Years_Education, data = Cities_1990)\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.0700 -0.4207  0.1362  0.6159  3.4617 \nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        11.570098   0.106056 109.094  &lt; 2e-16 ***\nSPEAKENG_new       -0.555160   0.045156 -12.294  &lt; 2e-16 ***\nExperience_Squared  0.035691   0.008004   4.459 8.53e-06 ***\nGender             -0.733319   0.034858 -21.037  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nResidual standard error: 0.9597 on 3073 degrees of freedom\nMultiple R-Squared: 0.08787,    Adjusted R-squared: 0.08698 \nWald test: 218.8 on 3 and 3073 DF,  p-value: &lt; 2.2e-16 \n\n#Creating Population variable \n\nCounties_1990 &lt;- Cities_1990 %&gt;% \n  filter(COUNTYFIP != 0) %&gt;% \n  group_by(STATEFIP, COUNTYFIP) %&gt;% \n  summarise(n=n()) %&gt;% \n  arrange(-n)\n\n`summarise()` has grouped output by 'STATEFIP'. You can override using the\n`.groups` argument.\n\n# County codes\n\n# 37 = Los Angeles, California, 37.8\n\n# 86 = Miami/Dade county, 49.2\n\n# 31 = Cook County, Illinois\n\n# 59 = Orange County, California \n\n# 5 = Bronx County, New York\n\n\nCities_1990 &lt;- mutate(Cities_1990, Hispanic_Population = \n                        if_else(COUNTYFIP == 37, 37.8 , \n                                if_else(COUNTYFIP == 86, 49.2 , \n                                        if_else(COUNTYFIP == 31, 13.6 ,\n                                                if_else(COUNTYFIP == 59, 23.4 ,\n                                                        if_else(COUNTYFIP == 5, 43.5,\n                                                                if_else(COUNTYFIP == 47, 20.1,\n                                                                        if_else(COUNTYFIP == 73, 20.4,\n                                                                                if_else(COUNTYFIP == 81, 19.5,\n                                                                                        if_else(COUNTYFIP == 61, 26.0,\n                                                                                                if_else(COUNTYFIP == 141, 69.6, 0)))))))))))\n\nview(head(Cities_1990))\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(AER)\n\n# SPEAKENG\n\ndata$SPEAKENG_numeric &lt;- as.numeric(data$SPEAKENG)\ndata$SPEAKENG_new &lt;- dplyr::recode(data$SPEAKENG_numeric, `3` = 1, `4` = 2, `5` = 3, `6` = 4, `1` = 5)\n\n#EDUC\n\ndata$EDUC_Grade &lt;- case_when(data$EDUC %in% c(0:1) ~ \"Elementary\",\n                             data$EDUC %in% c(2) ~ \"Middle\",\n                             data$EDUC %in% c(3:6) ~ \"High_School\",\n                             data$EDUC %in% c(7:10) ~ \"College\",\n                             data$EDUC %in% c(11) ~ \"Graduate_Degree\")\n\n#Age At Arrival Variable \n\ndata &lt;- mutate(data, AAA = AGE - (YEAR - YRIMMIG))\n\n#Inflation Adjusted Income Variable \n\ndata &lt;- filter(data, INCTOT &gt; 0) \ndata &lt;- mutate(data, Inflation_Adjusted = INCTOT*CPI99)\n\n#Log wages\ndata &lt;- mutate(data, LogWages = log(Inflation_Adjusted))\n\n\n\n# 2000 --------------------------------------------------------------------\n\nlibrary(tidyverse)\n# SPEAKENG\n\ndata$SPEAKENG_numeric &lt;- as.numeric(data$SPEAKENG)\ndata$SPEAKENG_new &lt;- dplyr::recode(data$SPEAKENG_numeric, `3` = 1, `4` = 2, `5` = 3, `6` = 4, `1` = 5)\n\n#EDUC\n\ndata$EDUC_Grade &lt;- case_when(data$EDUC %in% c(0:1) ~ \"Elementary\",\n                             data$EDUC %in% c(2) ~ \"Middle\",\n                             data$EDUC %in% c(3:6) ~ \"High_School\",\n                             data$EDUC %in% c(7:10) ~ \"College\",\n                             data$EDUC %in% c(11) ~ \"Graduate_Degree\")\n\n#Age At Arrival Variable \n\ndata &lt;- mutate(data, AAA = AGE - (YEAR - YRIMMIG))\n\n#Inflation Adjusted Income Variable \n\ndata &lt;- filter(data, INCTOT &gt; 0) \ndata &lt;- mutate(data, Inflation_Adjusted = INCTOT*CPI99)\n\n#Log wages\ndata &lt;- mutate(data, LogWages = log(Inflation_Adjusted))\n\n\n\nCities_2000 &lt;- data %&gt;% \n  filter(YEAR == 2000) %&gt;% \n  filter(AAA %in% c(0:18)) %&gt;% \n  filter(YRIMMIG %in% c(1970:1984)) %&gt;% \n  mutate(Hispanic_Origin = ifelse(HISPAN %in% c(1:4), 1, 0)) %&gt;% \n  mutate(Middle = ifelse(EDUC_Grade == \"Middle\", 1, 0)) %&gt;% \n  mutate(High_School = ifelse(EDUC_Grade == \"High_School\", 1, 0)) %&gt;% \n  mutate(College = ifelse(EDUC_Grade == \"College\", 1, 0)) %&gt;% \n  mutate(Graduate_Degree = ifelse(EDUC_Grade == \"Graduate_Degree\", 1, 0)) %&gt;% \n  mutate(Gender = ifelse(SEX == 2 , 1 , 0)) %&gt;% \n  filter(Hispanic_Origin == 1) %&gt;% \n  mutate(Citizenship_Status = ifelse(CITIZEN == 2, 1, 0)) %&gt;% \n  filter(Graduate_Degree != 1) %&gt;% \n  filter(ELDCH != 99 |\n           YNGCH!= 99) %&gt;% \n  filter(STATEFIP %in% c(6, 48, 17, 4, 36) |\n           COUNTYFIP %in% c(37, 59, 201, 31, 73, 71, 113, 13, 81, 5)) %&gt;% \n  mutate(YSM = (AGE - AAA)) %&gt;% \n  filter(STATEFIP != 0 |\n           COUNTYFIP != 0) %&gt;% \n  mutate(Experience = ifelse(EDUC_Grade == \"Elementary\", AGE - 5 - 5, \n                             ifelse(EDUC_Grade == \"Middle\", AGE - 8 - 5, \n                                    ifelse(EDUC_Grade == \"High_School\", AGE - 12 - 5, \n                                           ifelse(EDUC_Grade == \"College\", AGE - 16 - 5, 0))))) %&gt;% \n  mutate(Experience_Squared = ifelse(EDUC_Grade == \"Elementary\", ((AGE - 5 - 5)^2)/100, \n                                     ifelse(EDUC_Grade == \"Middle\", ((AGE - 8 - 5)^2)/100, \n                                            ifelse(EDUC_Grade == \"High_School\", ((AGE - 12 - 5)^2)/100, \n                                                   ifelse(EDUC_Grade == \"College\", ((AGE - 16 - 5)^2)/100, 0))))) %&gt;% \n  mutate(Years_Education = ifelse(EDUC_Grade == \"Elementary\", 5, \n                                  ifelse(EDUC_Grade == \"Middle\", 8,\n                                         ifelse(EDUC_Grade == \"High_School\", 12,\n                                                ifelse(EDUC_Grade == \"College\", 16, 0)))))%&gt;% \n  mutate(Education_Dummy = ifelse(EDUC_Grade %in% c(\"Elementary\", \"Middle\"), 0, 1)) %&gt;% \n  filter(INCTOT != 9999999 | INCTOT != 9999998) %&gt;% \n  mutate(Hispanic_Population = \n           if_else(COUNTYFIP == 37, 44.6, \n                   if_else(COUNTYFIP == 59, 30.8, \n                           if_else(COUNTYFIP == 201, 32.9,\n                                   if_else(COUNTYFIP == 31, 19.9,\n                                           if_else(COUNTYFIP == 73, 26.7, \n                                                   if_else(COUNTYFIP == 71,39.2 , \n                                                           if_else(COUNTYFIP == 113,29.9 , \n                                                                   if_else(COUNTYFIP == 13, 24.8, \n                                                                           if_else(COUNTYFIP == 81,25.0 , \n                                                                                   if_else(COUNTYFIP == 5,48.4 ,0))))))))))) %&gt;% \n  filter(Hispanic_Population != 0)\n\n\n\n#  filter(COUNTYFIP %in% c(37, 59, 31, 201, 73))\n\n\n#Creating Population variable\nCounties2000 &lt;- Cities_2000 %&gt;% \n  filter(COUNTYFIP != 0 ) %&gt;% \n  group_by(STATEFIP, COUNTYFIP) %&gt;% \n  summarise(n=n()) %&gt;% \n  arrange(-n)\n\n`summarise()` has grouped output by 'STATEFIP'. You can override using the\n`.groups` argument.\n\n# 37 = Los Angeles, 44.6\n\n# 59 = Daviess County, Kentucky, 0.9\n\n# 31 = Whitefeld, Georgia, 22.1\n\n# 201 = Winnebago, Illinois, 6.9\n\n# Colombia, Georgia, 2.6\n\nCities_2000 &lt;- mutate(Cities_2000, Hispanic_Population = \n                        if_else(COUNTYFIP == 37, 44.6, \n                                if_else(COUNTYFIP == 59, 30.8, \n                                        if_else(COUNTYFIP == 201, 32.9,\n                                                if_else(COUNTYFIP == 31, 19.9,\n                                                        if_else(COUNTYFIP == 73, 26.7, \n                                                                if_else(COUNTYFIP == 71,39.2 , \n                                                                        if_else(COUNTYFIP == 113,29.9 , \n                                                                                if_else(COUNTYFIP == 13, 24.8, \n                                                                                        if_else(COUNTYFIP == 81,25.0 , \n                                                                                                if_else(COUNTYFIP == 5,48.4 ,0)))))))))))                  \n\n\n\n\n#OLS 2000\nOLS2000 &lt;- lm(LogWages ~ AGE + Gender  + SPEAKENG_new + Citizenship_Status + Experience_Squared, data = Cities_2000)\nsummary(OLS2000)\n\n\nCall:\nlm(formula = LogWages ~ AGE + Gender + SPEAKENG_new + Citizenship_Status + \n    Experience_Squared, data = Cities_2000)\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.6268 -0.3163  0.1171  0.5050  3.6166 \nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         8.924337   0.132088  67.564  &lt; 2e-16 ***\nAGE                 0.048381   0.004496  10.762  &lt; 2e-16 ***\nGender             -0.685819   0.026174 -26.202  &lt; 2e-16 ***\nSPEAKENG_new       -0.090270   0.014617  -6.176 7.13e-10 ***\nCitizenship_Status  0.211848   0.026951   7.861 4.69e-15 ***\nExperience_Squared -0.090454   0.009809  -9.221  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nResidual standard error: 0.8875 on 4777 degrees of freedom\nMultiple R-squared:  0.1699,    Adjusted R-squared:  0.1691 \nF-statistic: 195.6 on 5 and 4777 DF,  p-value: &lt; 2.2e-16\n\n#OlS on SPEAKEng\n\nENG2000 &lt;- lm(SPEAKENG_new ~ AAA  + ELDCH + YNGCH + \n                Hispanic_Population + Years_Education\n              ,\n              data = Cities_2000)\nsummary(ENG2000)\n\n\nCall:\nlm(formula = SPEAKENG_new ~ AAA + ELDCH + YNGCH + Hispanic_Population + \n    Years_Education, data = Cities_2000)\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.76757 -0.49072 -0.03791  0.50982  3.10585 \nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          3.147011   0.080610  39.040   &lt;2e-16 ***\nAAA                  0.055786   0.002692  20.726   &lt;2e-16 ***\nELDCH                0.003078   0.002764   1.114   0.2655    \nYNGCH               -0.007491   0.003094  -2.421   0.0155 *  \nHispanic_Population  0.002927   0.001307   2.239   0.0252 *  \nYears_Education     -0.101866   0.004114 -24.760   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nResidual standard error: 0.8321 on 4777 degrees of freedom\nMultiple R-squared:  0.2615,    Adjusted R-squared:  0.2607 \nF-statistic: 338.3 on 5 and 4777 DF,  p-value: &lt; 2.2e-16\n\n#2000 IV \niv2000 &lt;- ivreg(LogWages ~  SPEAKENG_new + Experience_Squared   + Gender|    Experience_Squared  + \n                  Gender + AAA + Hispanic_Population +\n                  ELDCH + YNGCH + Years_Education , data = Cities_2000)\nsummary(iv2000)\n\n\nCall:\nivreg(formula = LogWages ~ SPEAKENG_new + Experience_Squared + \n    Gender | Experience_Squared + Gender + AAA + Hispanic_Population + \n    ELDCH + YNGCH + Years_Education, data = Cities_2000)\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.7656 -0.3732  0.1232  0.5480  4.0718 \nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        11.048908   0.092693 119.199  &lt; 2e-16 ***\nSPEAKENG_new       -0.405624   0.039200 -10.347  &lt; 2e-16 ***\nExperience_Squared  0.039022   0.006983   5.588 2.42e-08 ***\nGender             -0.658475   0.027574 -23.880  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nResidual standard error: 0.94 on 4779 degrees of freedom\nMultiple R-Squared: 0.06845,    Adjusted R-squared: 0.06786 \nWald test: 233.9 on 3 and 4779 DF,  p-value: &lt; 2.2e-16 \n\n\n\n\n\n\n# SPEAKENG\n\ndata$SPEAKENG_numeric &lt;- as.numeric(data$SPEAKENG)\ndata$SPEAKENG_new &lt;- dplyr::recode(data$SPEAKENG_numeric, `3` = 1, `4` = 2, `5` = 3, `6` = 4, `1` = 5)\n\n#EDUC\n\ndata$EDUC_Grade &lt;- case_when(data$EDUC %in% c(0:1) ~ \"Elementary\",\n                             data$EDUC %in% c(2) ~ \"Middle\",\n                             data$EDUC %in% c(3:6) ~ \"High_School\",\n                             data$EDUC %in% c(7:10) ~ \"College\",\n                             data$EDUC %in% c(11) ~ \"Graduate_Degree\")\n\n#Age At Arrival Variable \n\ndata &lt;- mutate(data, AAA = AGE - (YEAR - YRIMMIG))\n\n#Inflation Adjusted Income Variable \n\ndata &lt;- filter(data, INCTOT &gt; 0) \ndata &lt;- mutate(data, Inflation_Adjusted = INCTOT*CPI99)\n\n#Log wages\ndata &lt;- mutate(data, LogWages = log(Inflation_Adjusted))\n\n\n\n# 2010 --------------------------------------------------------------------\nlibrary(tidyverse)\n# SPEAKENG\n\ndata$SPEAKENG_numeric &lt;- as.numeric(data$SPEAKENG)\ndata$SPEAKENG_new &lt;- dplyr::recode(data$SPEAKENG_numeric, `3` = 1, `4` = 2, `5` = 3, `6` = 4, `1` = 5)\n\n#EDUC\n\ndata$EDUC_Grade &lt;- case_when(data$EDUC %in% c(0:1) ~ \"Elementary\",\n                             data$EDUC %in% c(2) ~ \"Middle\",\n                             data$EDUC %in% c(3:6) ~ \"High_School\",\n                             data$EDUC %in% c(7:10) ~ \"College\",\n                             data$EDUC %in% c(11) ~ \"Graduate_Degree\")\n\n#Age At Arrival Variable \n\ndata &lt;- mutate(data, AAA = AGE - (YEAR - YRIMMIG))\n\n#Inflation Adjusted Income Variable \n\ndata &lt;- filter(data, INCTOT &gt; 0) \ndata &lt;- mutate(data, Inflation_Adjusted = INCTOT*CPI99)\n\n#Log wages\ndata &lt;- mutate(data, LogWages = log(Inflation_Adjusted))\n\n\nlibrary(tidyverse)\nlibrary(AER)\n\nCities_2010 &lt;- data %&gt;% \n  filter(YEAR == 2010) %&gt;% \n  filter(AAA %in% c(0:18)) %&gt;% \n  filter(YRIMMIG %in% c(1980:1994)) %&gt;% \n  mutate(Hispanic_Origin = ifelse(HISPAN %in% c(1:4), 1, 0)) %&gt;% \n  mutate(Middle = ifelse(EDUC_Grade == \"Middle\", 1, 0)) %&gt;% \n  mutate(High_School = ifelse(EDUC_Grade == \"High_School\", 1, 0)) %&gt;% \n  mutate(College = ifelse(EDUC_Grade == \"College\", 1, 0)) %&gt;% \n  mutate(Graduate_Degree = ifelse(EDUC_Grade == \"Graduate_Degree\", 1, 0)) %&gt;% \n  mutate(Gender = ifelse(SEX == 2 , 1 , 0)) %&gt;% \n  filter(Hispanic_Origin == 1) %&gt;% \n  mutate(Citizenship_Status = ifelse(CITIZEN == 2, 1, 0)) %&gt;% \n  filter(Graduate_Degree != 1) %&gt;% \n  filter(ELDCH != 99 |\n           YNGCH!= 99) %&gt;% \n  filter(COUNTYFIP %in% c(37, 201, 31, 59, 13)) %&gt;% \n  filter(STATEFIP %in% c(6, 48, 17, 4, 36, 24, 34,32) |\n           COUNTYFIP %in% c(37, 201, 59, 31, 13)) %&gt;% \n  mutate(YSM = (AGE - AAA)) %&gt;% \n  filter(STATEFIP != 0 |\n           COUNTYFIP != 0) %&gt;% \n  mutate(Experience = ifelse(EDUC_Grade == \"Elementary\", AGE - 5 - 5, \n                             ifelse(EDUC_Grade == \"Middle\", AGE - 8 - 5, \n                                    ifelse(EDUC_Grade == \"High_School\", AGE - 12 - 5, \n                                           ifelse(EDUC_Grade == \"College\", AGE - 16 - 5, 0))))) %&gt;% \n  mutate(Experience_Squared = ifelse(EDUC_Grade == \"Elementary\", ((AGE - 5 - 5)^2)/100, \n                                     ifelse(EDUC_Grade == \"Middle\", ((AGE - 8 - 5)^2)/100, \n                                            ifelse(EDUC_Grade == \"High_School\", ((AGE - 12 - 5)^2)/100, \n                                                   ifelse(EDUC_Grade == \"College\", ((AGE - 16 - 5)^2)/100, 0))))) %&gt;% \n  mutate(Years_Education = ifelse(EDUC_Grade == \"Elementary\", 5, \n                                  ifelse(EDUC_Grade == \"Middle\", 8,\n                                         ifelse(EDUC_Grade == \"High_School\", 12,\n                                                ifelse(EDUC_Grade == \"College\", 16, 0)))))%&gt;% \n  mutate(Education_Dummy = ifelse(EDUC_Grade %in% c(\"Elementary\", \"Middle\"), 0, 1)) %&gt;% \n  filter(INCTOT != 9999999 | INCTOT != 9999998) %&gt;% \n  mutate(Hispanic_Population = \n           if_else(COUNTYFIP == 37,47.7 , \n                   if_else(COUNTYFIP == 201, 11.1, \n                           if_else(COUNTYFIP == 59 | STATEFIP == 6, 33.7,\n                                   if_else(COUNTYFIP == 31, 24.0 ,\n                                           if_else(COUNTYFIP == 13 | STATEFIP == 4,29.6 , \n                                                   if_else(COUNTYFIP == 13 | STATEFIP == 6, 24.4,\n                                                           if_else(COUNTYFIP == 59 | STATEFIP == 36, 14.6 ,\n                                                                   if_else(COUNTYFIP == 31 | STATEFIP == 24, 17.0,\n                                                                           if_else(COUNTYFIP == 13 | STATEFIP == 34,20.3 , \n                                                                                   if_else(COUNTYFIP == 31 | STATEFIP == 32,22.2 ,0))))))))))) %&gt;% \n  filter(Hispanic_Population != 0)\n\n#Creating population variable \n\nCities_2010 %&gt;% \n  filter(COUNTYFIP != 0 ) %&gt;% \n  group_by(STATEFIP, COUNTYFIP) %&gt;% \n  summarise(n=n()) %&gt;% \n  arrange(-n)\n\n`summarise()` has grouped output by 'STATEFIP'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 23 × 3\n# Groups:   STATEFIP [16]\n   STATEFIP        COUNTYFIP     n\n   &lt;int+lbl&gt;       &lt;dbl+lbl&gt; &lt;int&gt;\n 1  6 [California]  37        1812\n 2 48 [Texas]      201         483\n 3  6 [California]  59         392\n 4 17 [Illinois]    31         311\n 5  4 [Arizona]     13         270\n 6  6 [California]  13          91\n 7 36 [New York]    59          52\n 8 24 [Maryland]    31          46\n 9 34 [New Jersey]  13          40\n10 32 [Nevada]      31          39\n# ℹ 13 more rows\n\n# 37 = Los Angeles, California, 47.7\n\n# 201 = Winnebago, Illinois, 10.9\n\n# 31 = Witefeld, Georgia, 31.6\n\n# 59 = Daviess County, Kentucky, 2.6 \n\n# 13 = Black Hawk Iowa, 3.7\n\nCities_2010 &lt;- mutate(Cities_2010, Hispanic_Population = \n                        if_else(COUNTYFIP == 37,47.7 , \n                                if_else(COUNTYFIP == 201, 11.1, \n                                        if_else(COUNTYFIP == 59 | STATEFIP == 6, 33.7,\n                                                if_else(COUNTYFIP == 31, 24.0 ,\n                                                        if_else(COUNTYFIP == 13 | STATEFIP == 4,29.6 , \n                                                                if_else(COUNTYFIP == 13 | STATEFIP == 6, 24.4,\n                                                                        if_else(COUNTYFIP == 59 | STATEFIP == 36, 14.6 ,\n                                                                                if_else(COUNTYFIP == 31 | STATEFIP == 24, 17.0,\n                                                                                        if_else(COUNTYFIP == 13 | STATEFIP == 34,20.3 , \n                                                                                                if_else(COUNTYFIP == 31 | STATEFIP == 32,22.2 ,0)))))))))))\n\n#OLS 2010\nOLS2010 &lt;- lm(LogWages ~ AGE + Gender  +   SPEAKENG_new  + Citizenship_Status + Experience_Squared, data = Cities_2010)\nsummary(OLS2010)\n\n\nCall:\nlm(formula = LogWages ~ AGE + Gender + SPEAKENG_new + Citizenship_Status + \n    Experience_Squared, data = Cities_2010)\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.1747 -0.3406  0.1192  0.5125  3.7177 \nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         8.564208   0.148643  57.616  &lt; 2e-16 ***\nAGE                 0.056858   0.004841  11.745  &lt; 2e-16 ***\nGender             -0.606501   0.028578 -21.222  &lt; 2e-16 ***\nSPEAKENG_new       -0.130377   0.015774  -8.265  &lt; 2e-16 ***\nCitizenship_Status  0.252150   0.030908   8.158 4.63e-16 ***\nExperience_Squared -0.095373   0.009965  -9.570  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nResidual standard error: 0.842 on 3640 degrees of freedom\nMultiple R-squared:  0.1911,    Adjusted R-squared:   0.19 \nF-statistic:   172 on 5 and 3640 DF,  p-value: &lt; 2.2e-16\n\n#OlS on SPEAKEng\nENG2010 &lt;- lm(SPEAKENG_new ~ AAA  + ELDCH + YNGCH + \n                Hispanic_Population + Years_Education \n              ,\n              data = Cities_2010)\nsummary(ENG2010)\n\n\nCall:\nlm(formula = SPEAKENG_new ~ AAA + ELDCH + YNGCH + Hispanic_Population + \n    Years_Education, data = Cities_2010)\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0044 -0.5890 -0.0560  0.5369  3.2370 \nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          3.414101   0.089348  38.211  &lt; 2e-16 ***\nAAA                  0.046987   0.003384  13.883  &lt; 2e-16 ***\nELDCH                0.013476   0.003180   4.238 2.31e-05 ***\nYNGCH               -0.008053   0.003299  -2.441   0.0147 *  \nHispanic_Population  0.002044   0.001082   1.888   0.0591 .  \nYears_Education     -0.110588   0.004817 -22.958  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nResidual standard error: 0.8581 on 3640 degrees of freedom\nMultiple R-squared:  0.2462,    Adjusted R-squared:  0.2452 \nF-statistic: 237.8 on 5 and 3640 DF,  p-value: &lt; 2.2e-16\n\n#IV 2010\niv2010 &lt;- ivreg(LogWages ~  SPEAKENG_new + Experience_Squared  + Gender |    Experience_Squared  + \n                  Gender + AAA + Hispanic_Population +\n                  ELDCH + YNGCH + Years_Education, data = Cities_2010)\nsummary(iv2010)\n\n\nCall:\nivreg(formula = LogWages ~ SPEAKENG_new + Experience_Squared + \n    Gender | Experience_Squared + Gender + AAA + Hispanic_Population + \n    ELDCH + YNGCH + Years_Education, data = Cities_2010)\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.0532 -0.3925  0.1141  0.5793  3.9114 \nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        11.235664   0.112564   99.82  &lt; 2e-16 ***\nSPEAKENG_new       -0.533560   0.044591  -11.97  &lt; 2e-16 ***\nExperience_Squared  0.054478   0.007402    7.36 2.26e-13 ***\nGender             -0.586243   0.031282  -18.74  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nResidual standard error: 0.9265 on 3642 degrees of freedom\nMultiple R-Squared: 0.02005,    Adjusted R-squared: 0.01925 \nWald test: 165.7 on 3 and 3642 DF,  p-value: &lt; 2.2e-16 \n\nview(head(Cities_1980, 10))\n\n\n\n\n\n# SPEAKENG\n\ndata$SPEAKENG_numeric &lt;- as.numeric(data$SPEAKENG)\ndata$SPEAKENG_new &lt;- dplyr::recode(data$SPEAKENG_numeric, `3` = 1, `4` = 2, `5` = 3, `6` = 4, `1` = 5)\n\n#EDUC\n\ndata$EDUC_Grade &lt;- case_when(data$EDUC %in% c(0:1) ~ \"Elementary\",\n                             data$EDUC %in% c(2) ~ \"Middle\",\n                             data$EDUC %in% c(3:6) ~ \"High_School\",\n                             data$EDUC %in% c(7:10) ~ \"College\",\n                             data$EDUC %in% c(11) ~ \"Graduate_Degree\")\n\n#Age At Arrival Variable \n\ndata &lt;- mutate(data, AAA = AGE - (YEAR - YRIMMIG))\n\n#Inflation Adjusted Income Variable \n\ndata &lt;- filter(data, INCTOT &gt; 0) \ndata &lt;- mutate(data, Inflation_Adjusted = INCTOT*CPI99)\n\n#Log wages\ndata &lt;- mutate(data, LogWages = log(Inflation_Adjusted))\n\n\n\n# 2019 --------------------------------------------------------------------\nlibrary(tidyverse)\n# SPEAKENG\n\ndata$SPEAKENG_numeric &lt;- as.numeric(data$SPEAKENG)\ndata$SPEAKENG_new &lt;- dplyr::recode(data$SPEAKENG_numeric, `3` = 1, `4` = 2, `5` = 3, `6` = 4, `1` = 5)\n\n#EDUC\n\ndata$EDUC_Grade &lt;- case_when(data$EDUC %in% c(0:1) ~ \"Elementary\",\n                             data$EDUC %in% c(2) ~ \"Middle\",\n                             data$EDUC %in% c(3:6) ~ \"High_School\",\n                             data$EDUC %in% c(7:10) ~ \"College\",\n                             data$EDUC %in% c(11) ~ \"Graduate_Degree\")\n\n#Age At Arrival Variable \n\ndata &lt;- mutate(data, AAA = AGE - (YEAR - YRIMMIG))\n\n#Inflation Adjusted Income Variable \n\ndata &lt;- filter(data, INCTOT &gt; 0) \ndata &lt;- mutate(data, Inflation_Adjusted = INCTOT*CPI99)\n\n#Log wages\ndata &lt;- mutate(data, LogWages = log(Inflation_Adjusted))\n\n\nlibrary(tidyverse)\nlibrary(AER)\n\nCities_2019 &lt;- data %&gt;% \n  filter(YEAR == 2019) %&gt;% \n  filter(AAA %in% c(0:18)) %&gt;% \n  filter(YRIMMIG %in% c(1989:2003)) %&gt;% \n  mutate(Hispanic_Origin = ifelse(HISPAN %in% c(1:4), 1, 0)) %&gt;% \n  mutate(Middle = ifelse(EDUC_Grade == \"Middle\", 1, 0)) %&gt;% \n  mutate(High_School = ifelse(EDUC_Grade == \"High_School\", 1, 0)) %&gt;% \n  mutate(College = ifelse(EDUC_Grade == \"College\", 1, 0)) %&gt;% \n  mutate(Graduate_Degree = ifelse(EDUC_Grade == \"Graduate_Degree\", 1, 0)) %&gt;% \n  mutate(Gender = ifelse(SEX == 2 , 1 , 0)) %&gt;% \n  filter(Hispanic_Origin == 1) %&gt;% \n  mutate(Citizenship_Status = ifelse(CITIZEN == 2, 1, 0)) %&gt;%\n  filter(Graduate_Degree != 1) %&gt;% \n  filter(ELDCH != 99 |\n           YNGCH != 99) %&gt;% \n  filter(COUNTYFIP %in% c(37, 31, 201, 113, 13)) %&gt;% \n  mutate(Years_In_Country = (AGE- AAA)) %&gt;% \n  filter(STATEFIP %in% c(6, 48, 17, 4, 24, 32, 34) |\n           COUNTYFIP %in% c(37, 201, 113, 31, 13)) %&gt;% \n  mutate(YSM = (AGE - AAA)) %&gt;% \n  filter(STATEFIP != 0 |\n           COUNTYFIP != 0) %&gt;% \n  mutate(Experience = ifelse(EDUC_Grade == \"Elementary\", AGE - 5 - 5, \n                             ifelse(EDUC_Grade == \"Middle\", AGE - 8 - 5, \n                                    ifelse(EDUC_Grade == \"High_School\", AGE - 12 - 5, \n                                           ifelse(EDUC_Grade == \"College\", AGE - 16 - 5, 0))))) %&gt;% \n  mutate(Experience_Squared = ifelse(EDUC_Grade == \"Elementary\", ((AGE - 5 - 5)^2)/100, \n                                     ifelse(EDUC_Grade == \"Middle\", ((AGE - 8 - 5)^2)/100, \n                                            ifelse(EDUC_Grade == \"High_School\", ((AGE - 12 - 5)^2)/100, \n                                                   ifelse(EDUC_Grade == \"College\", ((AGE - 16 - 5)^2)/100, 0))))) %&gt;% \n  mutate(Education_Dummy = ifelse(EDUC_Grade %in% c(\"Elementary\", \"Middle\"), 0, 1)) %&gt;% \n  mutate(Years_Education = ifelse(EDUC_Grade == \"Elementary\", 5, \n                                  ifelse(EDUC_Grade == \"Middle\", 8,\n                                         ifelse(EDUC_Grade == \"High_School\", 12,\n                                                ifelse(EDUC_Grade == \"College\", 16, 0))))) %&gt;% \n  filter(INCTOT != 9999999 | INCTOT != 9999998) %&gt;% \n  mutate(Hispanic_Population = \n           ifelse(COUNTYFIP == 37 | STATEFIP == 6, 48.0,\n                  ifelse(COUNTYFIP == 201 | STATEFIP == 48, 43.0,\n                         ifelse(COUNTYFIP == 113 | STATEFIP == 48, 40.5,\n                                ifelse(COUNTYFIP == 31 | STATEFIP == 17, 26.2,\n                                       ifelse(COUNTYFIP == 13 | STATEFIP == 4, 30.6,\n                                              ifelse(COUNTYFIP == 13 | STATEFIP == 6, 27.0,\n                                                     ifelse(COUNTYFIP == 31 | STATEFIP == 24, 20.5,\n                                                            ifelse(COUNTYFIP == 31 | STATEFIP == 34, 42.7, \n                                                                   ifelse(COUNTYFIP == 31 | STATEFIP == 32, 25.1,\n                                                                          ifelse(COUNTYFIP == 13 | STATEFIP == 34, 24.4, 0))))))))))) %&gt;% \n  filter(Hispanic_Population != 0 )\n\n\n#mutate(COUNTY_POPULATION = ifelse(STATEFIP %in% c(6,12, 36, 48, 36, 34), 1, 0))\n\nview(head(Cities_2019\n))\n\n# Creating Population Variable \nCities_2019 %&gt;% \n  group_by(STATEFIP, COUNTYFIP) %&gt;% \n  summarise(n=n()) %&gt;% \n  arrange(-n)\n\n`summarise()` has grouped output by 'STATEFIP'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 26 × 3\n# Groups:   STATEFIP [16]\n   STATEFIP        COUNTYFIP     n\n   &lt;int+lbl&gt;       &lt;dbl+lbl&gt; &lt;int&gt;\n 1  6 [California]  37        1146\n 2 48 [Texas]      201         378\n 3 48 [Texas]      113         305\n 4 17 [Illinois]    31         232\n 5  4 [Arizona]     13         230\n 6  6 [California]  13          69\n 7 24 [Maryland]    31          63\n 8 34 [New Jersey]  31          58\n 9 32 [Nevada]      31          41\n10 34 [New Jersey]  13          25\n# ℹ 16 more rows\n\nCities_2019 &lt;- mutate(Cities_2019, Hispanic_Population = \n                        ifelse(COUNTYFIP == 37 | STATEFIP == 6, 48.0,\n                               ifelse(COUNTYFIP == 201 | STATEFIP == 48, 43.0,\n                                      ifelse(COUNTYFIP == 113 | STATEFIP == 48, 40.5,\n                                             ifelse(COUNTYFIP == 31 | STATEFIP == 17, 26.2,\n                                                    ifelse(COUNTYFIP == 13 | STATEFIP == 4, 30.6,\n                                                           ifelse(COUNTYFIP == 13 | STATEFIP == 6, 27.0,\n                                                                  ifelse(COUNTYFIP == 31 | STATEFIP == 24, 20.5,\n                                                                         ifelse(COUNTYFIP == 31 | STATEFIP == 34, 42.7, \n                                                                                ifelse(COUNTYFIP == 31 | STATEFIP == 32, 25.1,\n                                                                                       ifelse(COUNTYFIP == 13 | STATEFIP == 34, 24.4, 0)))))))))))\n\n#OLS 2019\nOLS2019 &lt;- lm(LogWages ~ AGE + Gender   +SPEAKENG_new + Citizenship_Status + Experience_Squared  , data = Cities_2019)\nsummary(OLS2019)\n\n\nCall:\nlm(formula = LogWages ~ AGE + Gender + SPEAKENG_new + Citizenship_Status + \n    Experience_Squared, data = Cities_2019)\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.9358 -0.3293  0.1055  0.4698  3.6066 \nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         8.94440    0.17506  51.092  &lt; 2e-16 ***\nAGE                 0.04485    0.00576   7.786 9.86e-15 ***\nGender             -0.61124    0.03230 -18.923  &lt; 2e-16 ***\nSPEAKENG_new       -0.08795    0.01800  -4.887 1.09e-06 ***\nCitizenship_Status  0.21076    0.03541   5.952 3.00e-09 ***\nExperience_Squared -0.08482    0.01216  -6.974 3.88e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nResidual standard error: 0.8206 on 2649 degrees of freedom\nMultiple R-squared:  0.1576,    Adjusted R-squared:  0.156 \nF-statistic: 99.08 on 5 and 2649 DF,  p-value: &lt; 2.2e-16\n\n#OlS on SPEAKEng\nENG2019 &lt;- lm(SPEAKENG_new ~ AAA  + ELDCH + YNGCH + \n                Hispanic_Population + Years_Education \n              ,\n              data = Cities_2019)\nsummary(ENG2019)\n\n\nCall:\nlm(formula = SPEAKENG_new ~ AAA + ELDCH + YNGCH + Hispanic_Population + \n    Years_Education, data = Cities_2019)\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.7504 -0.5614 -0.0670  0.5146  3.0462 \nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          2.928088   0.127784  22.914  &lt; 2e-16 ***\nAAA                  0.052016   0.003846  13.523  &lt; 2e-16 ***\nELDCH                0.015652   0.003709   4.220 2.53e-05 ***\nYNGCH               -0.017050   0.003853  -4.425 1.00e-05 ***\nHispanic_Population  0.005954   0.002013   2.958  0.00313 ** \nYears_Education     -0.094606   0.005913 -16.000  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nResidual standard error: 0.861 on 2649 degrees of freedom\nMultiple R-squared:  0.2248,    Adjusted R-squared:  0.2233 \nF-statistic: 153.6 on 5 and 2649 DF,  p-value: &lt; 2.2e-16\n\n#IV 2019\niv2019 &lt;- ivreg(LogWages ~  SPEAKENG_new + Experience_Squared  + Gender|    Experience_Squared  + \n                  Gender + AAA + Hispanic_Population +\n                  ELDCH + YNGCH  + Years_Education, data = Cities_2019)\nsummary(iv2019)\n\n\nCall:\nivreg(formula = LogWages ~ SPEAKENG_new + Experience_Squared + \n    Gender | Experience_Squared + Gender + AAA + Hispanic_Population + \n    ELDCH + YNGCH + Years_Education, data = Cities_2019)\nResiduals:\n     Min       1Q   Median       3Q      Max \n-6.19642 -0.37649  0.09258  0.51456  3.37004 \nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        10.956875   0.128300  85.400  &lt; 2e-16 ***\nSPEAKENG_new       -0.378763   0.053393  -7.094 1.67e-12 ***\nExperience_Squared  0.029789   0.008468   3.518 0.000443 ***\nGender             -0.594880   0.034157 -17.416  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nResidual standard error: 0.8681 on 2651 degrees of freedom\nMultiple R-Squared: 0.05643,    Adjusted R-squared: 0.05537 \nWald test: 110.7 on 3 and 2651 DF,  p-value: &lt; 2.2e-16"
  }
]